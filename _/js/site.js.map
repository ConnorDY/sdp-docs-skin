{"version":3,"sources":["00-nav.js","03-page-versions.js","04-mobile-navbar.js","05-highlight.js","06-clipboard.js","07-on-this-page.js","08-lunr.js","09-search.js"],"names":["expandParents","element","panel","parentNode","matches","parentHeader","previousElementSibling","classList","remove","querySelector","add","document","mdc","topAppBar","MDCTopAppBar","attachTo","select","querySelectorAll","i","length","s","addEventListener","event","component","this","getAttribute","showSelector","options","selectedIndex","value","hideSelector","navShow","navHide","disabled","x","ripple","MDCRipple","item","target","parentElement","nextElementSibling","contains","navToggle","getElementById","iconButton","MDCIconButtonToggle","mainContainer","navContainer","toggle","selector","e","stopPropagation","documentElement","window","innerWidth","navbarToggles","Array","prototype","slice","call","forEach","el","dataset","hljs","initHighlighting","codeBlocks","copyIcon","createElement","innerText","icon","cloneNode","insertBefore","childNodes","ClipboardJS","text","lines","split","splice","join","on","trigger","_tippy","setContent","tippy","delegate","content","animation","theme","delay","placement","hideOnClick","onHidden","instance","headers","toc","header","li","tagName","setAttribute","h","id","scroll","top","offsetTop","left","behavior","setTimeout","appendChild","referenceElement","activeHeader","lastActiveHeader","referencePoint","getBoundingClientRect","activeId","global","step2list","step3list","v","C","re_mgr0","re_mgr1","re_meq1","re_s_v","re_1a","re2_1a","re_1b","re2_1b","re_1b_2","re2_1b_2","re3_1b_2","re4_1b_2","re_1c","re_2","re_3","re_4","re2_4","re_5","re_5_1","re3_5","root","factory","lunr","config","builder","Builder","pipeline","trimmer","stopWordFilter","stemmer","searchPipeline","build","porterStemmer","w","stem","suffix","firstch","re","re2","re3","re4","substr","toUpperCase","test","replace","fp","exec","toLowerCase","version","utils","warn","message","console","asString","obj","toString","clone","Object","create","keys","key","val","isArray","TypeError","FieldRef","docRef","fieldName","stringValue","_stringValue","joiner","fromString","n","indexOf","fieldRef","undefined","Set","elements","complete","intersect","other","union","empty","object","a","b","intersection","push","concat","idf","posting","documentCount","documentsWithTerm","Math","log","abs","Token","str","metadata","update","fn","tokenizer","map","t","trim","len","tokens","sliceEnd","sliceStart","sliceLength","charAt","match","separator","tokenMetadata","Pipeline","_stack","registeredFunctions","registerFunction","label","warnIfFunctionNotRegistered","load","serialised","fnName","Error","arguments","after","existingFn","newFn","pos","before","run","stackLength","memo","j","result","k","runString","token","reset","toJSON","Vector","_magnitude","positionForIndex","index","start","end","pivotPoint","floor","pivotIndex","insert","insertIdx","upsert","position","magnitude","sumOfSquares","elementsLength","sqrt","dot","otherVector","dotProduct","aLen","bLen","aVal","bVal","similarity","toArray","output","ational","tional","enci","anci","izer","bli","alli","entli","eli","ousli","ization","ation","ator","alism","iveness","fulness","ousness","aliti","iviti","biliti","logi","icate","ative","alize","iciti","ical","ful","ness","c","RegExp","generateStopWordFilter","stopWords","words","reduce","stopWord","TokenSet","final","edges","_nextId","fromArray","arr","finish","fromClause","clause","fromFuzzyString","term","editDistance","stack","node","editsRemaining","noEditNode","char","deletionNode","frame","pop","substitutionNode","insertionNode","transposeNode","charA","charB","next","prefix","edge","_str","labels","sort","qNode","qEdges","qLen","nEdges","nLen","q","qEdge","nEdge","previousWord","uncheckedNodes","minimizedNodes","word","commonPrefix","minimize","child","nextNode","parent","downTo","childKey","Index","attrs","invertedIndex","fieldVectors","tokenSet","fields","search","queryString","query","QueryParser","parse","Query","matchingFields","queryVectors","termFieldCache","requiredMatches","prohibitedMatches","clauses","terms","clauseMatches","usePipeline","m","termTokenSet","expandedTerms","presence","REQUIRED","field","expandedTerm","termIndex","_index","fieldPosting","matchingDocumentRefs","termField","matchingDocumentsSet","PROHIBITED","boost","l","fieldMatch","matchingDocumentRef","matchingFieldRef","MatchData","allRequiredMatches","allProhibitedMatches","matchingFieldRefs","results","isNegated","docMatch","fieldVector","score","matchData","combine","ref","serializedIndex","serializedVectors","serializedInvertedIndex","tokenSetBuilder","tuple","_ref","_fields","_documents","fieldTermFrequencies","fieldLengths","_b","_k1","metadataWhitelist","attributes","RangeError","number","k1","doc","extractor","fieldTerms","metadataKey","calculateAverageFieldLengths","fieldRefs","numberOfFields","accumulator","documentsWithField","averageFieldLength","createFieldVectors","fieldRefsLength","termIdfCache","fieldLength","termFrequencies","termsLength","fieldBoost","docBoost","scoreWithPrecision","tf","round","createTokenSet","use","args","unshift","apply","clonedMetadata","metadataKeys","otherMatchData","allFields","wildcard","String","NONE","LEADING","TRAILING","OPTIONAL","QueryParseError","name","QueryLexer","lexemes","escapeCharPositions","state","lexText","sliceString","subSlices","emit","type","escapeCharacter","EOS","width","ignore","backup","acceptDigitRun","charCode","charCodeAt","more","FIELD","TERM","EDIT_DISTANCE","BOOST","PRESENCE","lexField","lexer","lexTerm","lexEditDistance","lexBoost","lexEOS","termSeparator","currentClause","lexemeIdx","parseClause","peekLexeme","consumeLexeme","lexeme","nextClause","completedClause","parser","parsePresence","parseField","parseTerm","errorMessage","nextLexeme","possibleFields","f","parseEditDistance","parseBoost","parseInt","isNaN","define","amd","exports","module","searchButton","searchCancelButton","searchClearButton","clearSearch","searchInput","dispatchEvent","KeyboardEvent","enterOrSpacebarPressed","code","keyCode","which","toggleSearch","searchResult","toolbarContainer","regularTopBar","searchTopBar","antoraLunr","body","highlightText","hits","highlightSpan","textEnd","contextAfter","contextBefore","createTextNode","highlightTitle","hash","title","titles","filter","titleEnd","createSearchResult","store","searchResultDataset","url","includes","substring","positions","highlightHit","documentTitle","documentHit","documentHitLink","href","hit","searchResultItem","createSearchResultItem","searchIndex","firstChild","removeChild","createNoResult","init","data","func","wait","immediate","timeout","assign","context","callNow","clearTimeout"],"mappings":"CAAA,WACA,cAKA,SAAAA,EAAAC,GACA,IAAAC,EAAAD,EAAAE,WACA,IAAAD,EAAAE,QAAA,uBACA,OAEA,IAAAC,EAAAH,EAAAI,uBACAJ,EAAAK,UAAAC,OAAA,QACAH,EAAAI,cAAA,mBAAAF,UAAAG,IAAA,YACAV,EAAAK,GATAL,CADAW,SAAAF,cAAA,oBAAAN,YAcAS,IAAAC,UAAAC,aAAAC,SAAAJ,SAAAF,cAAA,qBAKA,IADA,IAAAO,EAAAL,SAAAM,iBAAA,mBACAC,EAAA,EAAAA,EAAAF,EAAAG,OAAAD,IAAA,CACA,IAAAE,EAAAJ,EAAAE,GACAE,EAAAC,iBAAA,SAAA,SAAAC,GACA,IAAAC,EAAAC,KAAAC,aAAA,kBAEAC,EAAA,sCAAAH,EAAA,oBADAC,KAAAG,QAAAH,KAAAI,eAAAC,MACA,KACAC,EAAA,sCAAAP,EAAA,gBACAQ,EAAApB,SAAAF,cAAAiB,GACAM,EAAArB,SAAAF,cAAAqB,GACAC,EAAAxB,UAAAC,OAAA,QACAwB,EAAAzB,UAAAG,IAAA,UAIA,IAAAU,EAAAO,QAAAR,SACAC,EAAAb,UAAAG,IAAA,kBACAU,EAAAa,UAAA,GAKA,IAAAC,EAAAvB,SAAAM,iBAAA,6BACA,IAAAC,EAAA,EAAAA,EAAAgB,EAAAf,OAAAD,IACAN,IAAAuB,OAAAC,UAAArB,SAAAmB,EAAAhB,IACAgB,EAAAhB,GAAAG,iBAAA,QAAA,SAAAC,GACA,IAAAe,EAAAf,EAAAgB,OACApC,EAAAmC,EAAAE,cAAAC,mBACAH,EAAA9B,UAAAkC,SAAA,aACAJ,EAAA9B,UAAAC,OAAA,YACAN,EAAAK,UAAAG,IAAA,UAEA2B,EAAA9B,UAAAG,IAAA,YACAR,EAAAK,UAAAC,OAAA,WAMA,IAAAkC,EAAA/B,SAAAgC,eAAA,sBACA/B,IAAAgC,WAAAC,oBAAA9B,SAAA2B,GACAA,EAAArB,iBAAA,QAAA,WAEA,IAAAyB,EAAAnC,SAAAF,cAAA,QACAsC,EAAApC,SAAAF,cAAA,qBACAsC,EAAAxC,UAAAkC,SAAA,SACAK,EAAAvC,UAAAG,IAAA,QACAqC,EAAAxC,UAAAC,OAAA,UAGAsC,EAAAvC,UAAAC,OAAA,QACAuC,EAAAxC,UAAAG,IAAA,WAzEA,GCAA,WACA,aAEA,IAAAsC,EAAArC,SAAAF,cAAA,uCACA,GAAAuC,EAAA,CAEA,IAAAC,EAAAtC,SAAAF,cAAA,kBAEAuC,EAAA3B,iBAAA,QAAA,SAAA6B,GACAD,EAAA1C,UAAAyC,OAAA,aAEAE,EAAAC,oBAGAxC,SAAAyC,gBAAA/B,iBAAA,QAAA,WACA4B,EAAA1C,UAAAC,OAAA,gBAfA,GCAAG,SAAAU,iBAAA,mBAAA,WAEAgC,OAAAC,YAAA,MACA3C,SAAAF,cAAA,qBACAF,UAAAG,IAAA,QAIA,IAAA6C,EAAAC,MAAAC,UAAAC,MAAAC,KAAAhD,SAAAM,iBAAA,kBAAA,GACA,IAAAsC,EAAApC,QACAoC,EAAAK,QAAA,SAAAC,GACAA,EAAAxC,iBAAA,QAAA,SAAA6B,GACAA,EAAAC,kBACAU,EAAAtD,UAAAyC,OAAA,aACArC,SAAAgC,eAAAkB,EAAAC,QAAAxB,QAAA/B,UAAAyC,OAAA,aACArC,SAAAyC,gBAAA7C,UAAAyC,OAAA,4BAMAK,OAAAhC,iBAAA,SAAA,WAGA,GADAV,SAAAF,cAAA,iDACAF,UAAAkC,SAAA,QAAA,CAEA,IAAAK,EAAAnC,SAAAF,cAAA,QACAqC,EAAAvC,UAAAkC,SAAA,SACAK,EAAAvC,UAAAC,OAAA,QAIA,IAAAuC,EAAApC,SAAAF,cAAA,qBACA,KAAA4C,OAAAC,YACAP,EAAAxC,UAAAkC,SAAA,SACAM,EAAAxC,UAAAC,OAAA,QAMA6C,OAAAC,WAAA,OACAP,EAAAxC,UAAAkC,SAAA,SACAM,EAAAxC,UAAAG,IAAA,WCzCAqD,KAAAC,mBCFA,WACA,aAEA,IAAAC,EAAAtD,SAAAM,iBAAA,2BACAiD,EAAAvD,SAAAwD,cAAA,KACAD,EAAA3D,UAAA,gCACA2D,EAAAE,UAAA,YACA,IAAA,IAAAlD,EAAA,EAAAA,EAAA+C,EAAA9C,OAAAD,IAAA,CACA,IAAAmD,EAAAH,EAAAI,WAAA,GACAL,EAAA/C,GAAAqD,aAAAF,EAAAJ,EAAA/C,GAAAsD,WAAA,IAIA,IAAAC,YAAA,iCAAA,CACAC,KAAA,SAAApC,GACA,IAAAqC,EAAArC,EAAAnC,WAAAiE,UAAAQ,MAAA,MAEA,OADAD,EAAAE,OAAA,EAAA,GACAF,EAAAG,KAAA,SAIAC,GAAA,UAAA,SAAA7B,GACAA,EAAA8B,QAAAC,OAAAC,WAAA,aAGAC,MAAAC,SAAA,0BAAA,CACA9C,OAAA,iCACA+C,QAAA,oBACAC,UAAA,aACAC,MAAA,YACAC,MAAA,CAAA,IAAA,GACAC,UAAA,SACAC,aAAA,EACAC,SAAA,SAAAC,GACAA,EAAAV,WAAA,wBAlCA,GCAA,WACA,aAKA,IAFA,IAAAW,EAAAlF,SAAAF,cAAA,kBAAAQ,iBAAA,0BACA6E,EAAAnF,SAAAgC,eAAA,OACAzB,EAAA,EAAAA,EAAA2E,EAAA1E,OAAAD,IAAA,CACA,IAAA6E,EAAAF,EAAA3E,GACA8E,EAAArF,SAAAwD,cAAA,MACA6B,EAAAzF,UAAAG,IAAA,YACAsF,EAAAzF,UAAAG,IAAAqF,EAAAE,SACAD,EAAA5B,UAAA2B,EAAA3B,UACA4B,EAAAE,aAAA,WAAAH,EAAAtE,aAAA,OACAuE,EAAA3E,iBAAA,QAAA,WACA,IAAA8E,EAAAC,EAAA5E,KAAAC,aAAA,YACA0E,EAAA,QAAAC,EAAAzF,SAAAgC,eAAAyD,GACAzF,SAAAF,cAAA,MACA4C,OAAAgD,OAAA,CACAC,IAAAH,EAAAI,UAAA,GACAC,KAAA,EACAC,SAAA,WAEAN,EAAA5F,UAAAG,IAAA,iBACAgG,WAAA,WAAAP,EAAA5F,UAAAC,OAAA,kBAAA,OAEAsF,EAAAa,YAAAX,GAIA,IAAAY,EAAAjG,SAAAF,cAAA,WAAAN,WACAkD,OAAAhC,iBAAA,SAAA,WAGA,IAFA,IAAAwF,EAAAC,EAAAnG,SAAAF,cAAA,oBACAsG,EAAAH,EAAAL,UAAA,MACArF,EAAA2E,EAAA1E,OAAA,EAAA,GAAAD,EAAAA,IAAA,CAEA,GADA2E,EAAA3E,GAAA8F,wBAAAV,IACAS,EAAA,CACA,IAAAE,EAAApB,EAAA3E,GAAAO,aAAA,MACAoF,EAAAlG,SAAAF,cAAA,uBAAAwG,EAAA,MACA,OAKAJ,GAAAA,IAAAC,IACAD,EAAAtG,UAAAG,IAAA,UACAoG,GACAA,EAAAvG,UAAAC,OAAA,aA9CA,GCMA,WAiCA,IAoCA0G,EAw2BAC,EAwBAC,EAWAC,EACAC,EAQAC,EACAC,EACAC,EACAC,EAEAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EAEAC,EACAC,EAEAC,EAEAC,EACAC,EAEAC,EACAC,EACAC,EA64EAC,EAAAC,EA71GAC,EAAA,SAAAC,GACA,IAAAC,EAAA,IAAAF,EAAAG,QAaA,OAXAD,EAAAE,SAAAvI,IACAmI,EAAAK,QACAL,EAAAM,eACAN,EAAAO,SAGAL,EAAAM,eAAA3I,IACAmI,EAAAO,SAGAN,EAAAnF,KAAAoF,EAAAA,GACAA,EAAAO,SAo8BA,SAAAC,EAAAC,GACA,IAAAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EAEA,GAAAP,EAAArI,OAAA,EAAA,OAAAqI,EAiBA,GAdA,MADAG,EAAAH,EAAAQ,OAAA,EAAA,MAEAR,EAAAG,EAAAM,cAAAT,EAAAQ,OAAA,IAKAH,EAAAjC,GADAgC,EAAAjC,GAGAuC,KAAAV,GAAAA,EAAAA,EAAAW,QAAAP,EAAA,QACAC,EAAAK,KAAAV,KAAAA,EAAAA,EAAAW,QAAAN,EAAA,SAIAA,EAAA/B,GADA8B,EAAA/B,GAEAqC,KAAAV,GAAA,CACA,IAAAY,EAAAR,EAAAS,KAAAb,IACAI,EAAArC,GACA2C,KAAAE,EAAA,MACAR,EAAA7B,EACAyB,EAAAA,EAAAW,QAAAP,EAAA,UAEA,GAAAC,EAAAK,KAAAV,GAAA,CAEAC,GADAW,EAAAP,EAAAQ,KAAAb,IACA,IACAK,EAAAnC,GACAwC,KAAAT,KAGAK,EAAA7B,EACA8B,EAAA7B,GAFA2B,EAAA7B,GAGAkC,KAJAV,EAAAC,GAIAD,GAAA,IACAM,EAAAI,KAAAV,IAAAI,EAAA7B,EAAAyB,EAAAA,EAAAW,QAAAP,EAAA,KACAG,EAAAG,KAAAV,KAAAA,GAAA,MAuCA,IAlCAI,EAAAzB,GACA+B,KAAAV,KAGAA,GADAC,GADAW,EAAAR,EAAAS,KAAAb,IACA,IACA,MAIAI,EAAAxB,GACA8B,KAAAV,KAEAC,GADAW,EAAAR,EAAAS,KAAAb,IACA,GACAE,EAAAU,EAAA,IACAR,EAAArC,GACA2C,KAAAT,KACAD,EAAAC,EAAAtC,EAAAuC,MAKAE,EAAAvB,GACA6B,KAAAV,KAEAC,GADAW,EAAAR,EAAAS,KAAAb,IACA,GACAE,EAAAU,EAAA,IACAR,EAAArC,GACA2C,KAAAT,KACAD,EAAAC,EAAArC,EAAAsC,KAMAG,EAAAtB,GADAqB,EAAAtB,GAEA4B,KAAAV,GAEAC,GADAW,EAAAR,EAAAS,KAAAb,IACA,IACAI,EAAApC,GACA0C,KAAAT,KACAD,EAAAC,QAEA,GAAAI,EAAAK,KAAAV,GAAA,CAEAC,GADAW,EAAAP,EAAAQ,KAAAb,IACA,GAAAY,EAAA,IACAP,EAAArC,GACA0C,KAAAT,KACAD,EAAAC,GA8BA,OAzBAG,EAAApB,GACA0B,KAAAV,KAEAC,GADAW,EAAAR,EAAAS,KAAAb,IACA,GAEAK,EAAApC,EACAqC,EAAApB,IAFAkB,EAAApC,GAGA0C,KAAAT,IAAAI,EAAAK,KAAAT,KAAAK,EAAAI,KAAAT,MACAD,EAAAC,IAKAI,EAAArC,GADAoC,EAAAnB,GAEAyB,KAAAV,IAAAK,EAAAK,KAAAV,KACAI,EAAA7B,EACAyB,EAAAA,EAAAW,QAAAP,EAAA,KAKA,KAAAD,IACAH,EAAAG,EAAAW,cAAAd,EAAAQ,OAAA,IAGAR,EA9jCAX,EAAA0B,QAAA,QAUA1B,EAAA2B,MAAA,GASA3B,EAAA2B,MAAAC,MAAAvD,EAQA1F,KANA,SAAAkJ,GACAxD,EAAAyD,SAAAA,QAAAF,MACAE,QAAAF,KAAAC,KAiBA7B,EAAA2B,MAAAI,SAAA,SAAAC,GACA,OAAAA,MAAAA,EACA,GAEAA,EAAAC,YAoBAjC,EAAA2B,MAAAO,MAAA,SAAAF,GACA,GAAAA,MAAAA,EACA,OAAAA,EAMA,IAHA,IAAAE,EAAAC,OAAAC,OAAA,MACAC,EAAAF,OAAAE,KAAAL,GAEA3J,EAAA,EAAAA,EAAAgK,EAAA/J,OAAAD,IAAA,CACA,IAAAiK,EAAAD,EAAAhK,GACAkK,EAAAP,EAAAM,GAEA,GAAA3H,MAAA6H,QAAAD,GACAL,EAAAI,GAAAC,EAAA1H,YADA,CAKA,GAAA,iBAAA0H,GACA,iBAAAA,GACA,kBAAAA,EAKA,MAAA,IAAAE,UAAA,yDAJAP,EAAAI,GAAAC,GAOA,OAAAL,GAEAlC,EAAA0C,SAAA,SAAAC,EAAAC,EAAAC,GACAlK,KAAAgK,OAAAA,EACAhK,KAAAiK,UAAAA,EACAjK,KAAAmK,aAAAD,GAGA7C,EAAA0C,SAAAK,OAAA,IAEA/C,EAAA0C,SAAAM,WAAA,SAAAzK,GACA,IAAA0K,EAAA1K,EAAA2K,QAAAlD,EAAA0C,SAAAK,QAEA,IAAA,IAAAE,EACA,KAAA,6BAGA,IAAAE,EAAA5K,EAAAsC,MAAA,EAAAoI,GACAN,EAAApK,EAAAsC,MAAAoI,EAAA,GAEA,OAAA,IAAAjD,EAAA0C,SAAAC,EAAAQ,EAAA5K,IAGAyH,EAAA0C,SAAA9H,UAAAqH,SAAA,WAKA,OAJAmB,MAAAzK,KAAAmK,eACAnK,KAAAmK,aAAAnK,KAAAiK,UAAA5C,EAAA0C,SAAAK,OAAApK,KAAAgK,QAGAhK,KAAAmK,cAYA9C,EAAAqD,IAAA,SAAAC,GAGA,GAFA3K,KAAA2K,SAAAnB,OAAAC,OAAA,MAEAkB,EAAA,CACA3K,KAAAL,OAAAgL,EAAAhL,OAEA,IAAA,IAAAD,EAAA,EAAAA,EAAAM,KAAAL,OAAAD,IACAM,KAAA2K,SAAAA,EAAAjL,KAAA,OAGAM,KAAAL,OAAA,GAWA0H,EAAAqD,IAAAE,SAAA,CACAC,UAAA,SAAAC,GACA,OAAAA,GAGAC,MAAA,SAAAD,GACA,OAAAA,GAGA7J,SAAA,WACA,OAAA,IAWAoG,EAAAqD,IAAAM,MAAA,CACAH,UAAA,WACA,OAAA7K,MAGA+K,MAAA,SAAAD,GACA,OAAAA,GAGA7J,SAAA,WACA,OAAA,IAUAoG,EAAAqD,IAAAzI,UAAAhB,SAAA,SAAAgK,GACA,QAAAjL,KAAA2K,SAAAM,IAWA5D,EAAAqD,IAAAzI,UAAA4I,UAAA,SAAAC,GACA,IAAAI,EAAAC,EAAAR,EAAAS,EAAA,GAEA,GAAAN,IAAAzD,EAAAqD,IAAAE,SACA,OAAA5K,KAGA,GAAA8K,IAAAzD,EAAAqD,IAAAM,MACA,OAAAF,EAKAK,EAFAnL,KAAAL,OAAAmL,EAAAnL,QACAuL,EAAAlL,KACA8K,IAEAI,EAAAJ,EACA9K,MAGA2K,EAAAnB,OAAAE,KAAAwB,EAAAP,UAEA,IAAA,IAAAjL,EAAA,EAAAA,EAAAiL,EAAAhL,OAAAD,IAAA,CACA,IAAAjB,EAAAkM,EAAAjL,GACAjB,KAAA0M,EAAAR,UACAS,EAAAC,KAAA5M,GAIA,OAAA,IAAA4I,EAAAqD,IAAAU,IAUA/D,EAAAqD,IAAAzI,UAAA8I,MAAA,SAAAD,GACA,OAAAA,IAAAzD,EAAAqD,IAAAE,SACAvD,EAAAqD,IAAAE,SAGAE,IAAAzD,EAAAqD,IAAAM,MACAhL,KAGA,IAAAqH,EAAAqD,IAAAlB,OAAAE,KAAA1J,KAAA2K,UAAAW,OAAA9B,OAAAE,KAAAoB,EAAAH,aAUAtD,EAAAkE,IAAA,SAAAC,EAAAC,GACA,IAAAC,EAAA,EAEA,IAAA,IAAAzB,KAAAuB,EACA,UAAAvB,IACAyB,GAAAlC,OAAAE,KAAA8B,EAAAvB,IAAAtK,QAGA,IAAAe,GAAA+K,EAAAC,EAAA,KAAAA,EAAA,IAEA,OAAAC,KAAAC,IAAA,EAAAD,KAAAE,IAAAnL,KAWA2G,EAAAyE,MAAA,SAAAC,EAAAC,GACAhM,KAAA+L,IAAAA,GAAA,GACA/L,KAAAgM,SAAAA,GAAA,IAQA3E,EAAAyE,MAAA7J,UAAAqH,SAAA,WACA,OAAAtJ,KAAA+L,KAuBA1E,EAAAyE,MAAA7J,UAAAgK,OAAA,SAAAC,GAEA,OADAlM,KAAA+L,IAAAG,EAAAlM,KAAA+L,IAAA/L,KAAAgM,UACAhM,MAUAqH,EAAAyE,MAAA7J,UAAAsH,MAAA,SAAA2C,GAEA,OADAA,EAAAA,GAAA,SAAAtM,GAAA,OAAAA,GACA,IAAAyH,EAAAyE,MAAAI,EAAAlM,KAAA+L,IAAA/L,KAAAgM,UAAAhM,KAAAgM,WAyBA3E,EAAA8E,UAAA,SAAA9C,EAAA2C,GACA,GAAA,MAAA3C,GAAAoB,MAAApB,EACA,MAAA,GAGA,GAAArH,MAAA6H,QAAAR,GACA,OAAAA,EAAA+C,IAAA,SAAAC,GACA,OAAA,IAAAhF,EAAAyE,MACAzE,EAAA2B,MAAAI,SAAAiD,GAAAvD,cACAzB,EAAA2B,MAAAO,MAAAyC,MASA,IAJA,IAAAD,EAAA1C,EAAAC,WAAAgD,OAAAxD,cACAyD,EAAAR,EAAApM,OACA6M,EAAA,GAEAC,EAAA,EAAAC,EAAA,EAAAD,GAAAF,EAAAE,IAAA,CACA,IACAE,EAAAF,EAAAC,EAEA,GAHAX,EAAAa,OAAAH,GAGAI,MAAAxF,EAAA8E,UAAAW,YAAAL,GAAAF,EAAA,CAEA,GAAA,EAAAI,EAAA,CACA,IAAAI,EAAA1F,EAAA2B,MAAAO,MAAAyC,IAAA,GACAe,EAAA,SAAA,CAAAL,EAAAC,GACAI,EAAA,MAAAP,EAAA7M,OAEA6M,EAAAnB,KACA,IAAAhE,EAAAyE,MACAC,EAAA7J,MAAAwK,EAAAD,GACAM,IAKAL,EAAAD,EAAA,GAKA,OAAAD,GAUAnF,EAAA8E,UAAAW,UAAA,UAmCAzF,EAAA2F,SAAA,WACAhN,KAAAiN,OAAA,IAGA5F,EAAA2F,SAAAE,oBAAA1D,OAAAC,OAAA,MAmCApC,EAAA2F,SAAAG,iBAAA,SAAAjB,EAAAkB,GACAA,KAAApN,KAAAkN,qBACA7F,EAAA2B,MAAAC,KAAA,6CAAAmE,GAGAlB,EAAAkB,MAAAA,EACA/F,EAAA2F,SAAAE,oBAAAhB,EAAAkB,OAAAlB,GASA7E,EAAA2F,SAAAK,4BAAA,SAAAnB,GACAA,EAAAkB,OAAAlB,EAAAkB,SAAApN,KAAAkN,qBAGA7F,EAAA2B,MAAAC,KAAA,kGAAAiD,IAcA7E,EAAA2F,SAAAM,KAAA,SAAAC,GACA,IAAA9F,EAAA,IAAAJ,EAAA2F,SAYA,OAVAO,EAAAnL,QAAA,SAAAoL,GACA,IAAAtB,EAAA7E,EAAA2F,SAAAE,oBAAAM,GAEA,IAAAtB,EAGA,MAAA,IAAAuB,MAAA,sCAAAD,GAFA/F,EAAAvI,IAAAgN,KAMAzE,GAUAJ,EAAA2F,SAAA/K,UAAA/C,IAAA,WACA8C,MAAAC,UAAAC,MAAAC,KAAAuL,WAEAtL,QAAA,SAAA8J,GACA7E,EAAA2F,SAAAK,4BAAAnB,GACAlM,KAAAiN,OAAA5B,KAAAa,IACAlM,OAYAqH,EAAA2F,SAAA/K,UAAA0L,MAAA,SAAAC,EAAAC,GACAxG,EAAA2F,SAAAK,4BAAAQ,GAEA,IAAAC,EAAA9N,KAAAiN,OAAA1C,QAAAqD,GACA,IAAA,GAAAE,EACA,MAAA,IAAAL,MAAA,0BAGAK,GAAA,EACA9N,KAAAiN,OAAA5J,OAAAyK,EAAA,EAAAD,IAYAxG,EAAA2F,SAAA/K,UAAA8L,OAAA,SAAAH,EAAAC,GACAxG,EAAA2F,SAAAK,4BAAAQ,GAEA,IAAAC,EAAA9N,KAAAiN,OAAA1C,QAAAqD,GACA,IAAA,GAAAE,EACA,MAAA,IAAAL,MAAA,0BAGAzN,KAAAiN,OAAA5J,OAAAyK,EAAA,EAAAD,IAQAxG,EAAA2F,SAAA/K,UAAAjD,OAAA,SAAAkN,GACA,IAAA4B,EAAA9N,KAAAiN,OAAA1C,QAAA2B,IACA,GAAA4B,GAIA9N,KAAAiN,OAAA5J,OAAAyK,EAAA,IAUAzG,EAAA2F,SAAA/K,UAAA+L,IAAA,SAAAxB,GAGA,IAFA,IAAAyB,EAAAjO,KAAAiN,OAAAtN,OAEAD,EAAA,EAAAA,EAAAuO,EAAAvO,IAAA,CAIA,IAHA,IAAAwM,EAAAlM,KAAAiN,OAAAvN,GACAwO,EAAA,GAEAC,EAAA,EAAAA,EAAA3B,EAAA7M,OAAAwO,IAAA,CACA,IAAAC,EAAAlC,EAAAM,EAAA2B,GAAAA,EAAA3B,GAEA,QAAA,IAAA4B,GAAA,KAAAA,EAEA,GAAAA,aAAApM,MACA,IAAA,IAAAqM,EAAA,EAAAA,EAAAD,EAAAzO,OAAA0O,IACAH,EAAA7C,KAAA+C,EAAAC,SAGAH,EAAA7C,KAAA+C,GAIA5B,EAAA0B,EAGA,OAAA1B,GAaAnF,EAAA2F,SAAA/K,UAAAqM,UAAA,SAAAvC,EAAAC,GACA,IAAAuC,EAAA,IAAAlH,EAAAyE,MAAAC,EAAAC,GAEA,OAAAhM,KAAAgO,IAAA,CAAAO,IAAAnC,IAAA,SAAAC,GACA,OAAAA,EAAA/C,cAQAjC,EAAA2F,SAAA/K,UAAAuM,MAAA,WACAxO,KAAAiN,OAAA,IAUA5F,EAAA2F,SAAA/K,UAAAwM,OAAA,WACA,OAAAzO,KAAAiN,OAAAb,IAAA,SAAAF,GAGA,OAFA7E,EAAA2F,SAAAK,4BAAAnB,GAEAA,EAAAkB,SAwBA/F,EAAAqH,OAAA,SAAA/D,GACA3K,KAAA2O,WAAA,EACA3O,KAAA2K,SAAAA,GAAA,IAcAtD,EAAAqH,OAAAzM,UAAA2M,iBAAA,SAAAC,GAEA,GAAA,GAAA7O,KAAA2K,SAAAhL,OACA,OAAA,EASA,IANA,IAAAmP,EAAA,EACAC,EAAA/O,KAAA2K,SAAAhL,OAAA,EACAgN,EAAAoC,EAAAD,EACAE,EAAArD,KAAAsD,MAAAtC,EAAA,GACAuC,EAAAlP,KAAA2K,SAAA,EAAAqE,GAEA,EAAArC,IACAuC,EAAAL,IACAC,EAAAE,GAGAH,EAAAK,IACAH,EAAAC,GAGAE,GAAAL,IAIAlC,EAAAoC,EAAAD,EACAE,EAAAF,EAAAnD,KAAAsD,MAAAtC,EAAA,GACAuC,EAAAlP,KAAA2K,SAAA,EAAAqE,GAGA,OAAAE,GAAAL,GAIAA,EAAAK,EAHA,EAAAF,EAOAE,EAAAL,EACA,GAAAG,EAAA,QADA,GAcA3H,EAAAqH,OAAAzM,UAAAkN,OAAA,SAAAC,EAAAxF,GACA5J,KAAAqP,OAAAD,EAAAxF,EAAA,WACA,KAAA,qBAYAvC,EAAAqH,OAAAzM,UAAAoN,OAAA,SAAAD,EAAAxF,EAAAsC,GACAlM,KAAA2O,WAAA,EACA,IAAAW,EAAAtP,KAAA4O,iBAAAQ,GAEApP,KAAA2K,SAAA2E,IAAAF,EACApP,KAAA2K,SAAA2E,EAAA,GAAApD,EAAAlM,KAAA2K,SAAA2E,EAAA,GAAA1F,GAEA5J,KAAA2K,SAAAtH,OAAAiM,EAAA,EAAAF,EAAAxF,IASAvC,EAAAqH,OAAAzM,UAAAsN,UAAA,WACA,GAAAvP,KAAA2O,WAAA,OAAA3O,KAAA2O,WAKA,IAHA,IAAAa,EAAA,EACAC,EAAAzP,KAAA2K,SAAAhL,OAEAD,EAAA,EAAAA,EAAA+P,EAAA/P,GAAA,EAAA,CACA,IAAAkK,EAAA5J,KAAA2K,SAAAjL,GACA8P,GAAA5F,EAAAA,EAGA,OAAA5J,KAAA2O,WAAAhD,KAAA+D,KAAAF,IASAnI,EAAAqH,OAAAzM,UAAA0N,IAAA,SAAAC,GAOA,IANA,IAAAC,EAAA,EACA3E,EAAAlL,KAAA2K,SAAAQ,EAAAyE,EAAAjF,SACAmF,EAAA5E,EAAAvL,OAAAoQ,EAAA5E,EAAAxL,OACAqQ,EAAA,EAAAC,EAAA,EACAvQ,EAAA,EAAAyO,EAAA,EAEAzO,EAAAoQ,GAAA3B,EAAA4B,IACAC,EAAA9E,EAAAxL,KAAAuQ,EAAA9E,EAAAgD,IAEAzO,GAAA,EACAuQ,EAAAD,EACA7B,GAAA,EACA6B,GAAAC,IACAJ,GAAA3E,EAAAxL,EAAA,GAAAyL,EAAAgD,EAAA,GACAzO,GAAA,EACAyO,GAAA,GAIA,OAAA0B,GAUAxI,EAAAqH,OAAAzM,UAAAiO,WAAA,SAAAN,GACA,OAAA5P,KAAA2P,IAAAC,GAAA5P,KAAAuP,aAAA,GAQAlI,EAAAqH,OAAAzM,UAAAkO,QAAA,WAGA,IAFA,IAAAC,EAAA,IAAApO,MAAAhC,KAAA2K,SAAAhL,OAAA,GAEAD,EAAA,EAAAyO,EAAA,EAAAzO,EAAAM,KAAA2K,SAAAhL,OAAAD,GAAA,EAAAyO,IACAiC,EAAAjC,GAAAnO,KAAA2K,SAAAjL,GAGA,OAAA0Q,GAQA/I,EAAAqH,OAAAzM,UAAAwM,OAAA,WACA,OAAAzO,KAAA2K,UAoBAtD,EAAAO,SACAjC,EAAA,CACA0K,QAAA,MACAC,OAAA,OACAC,KAAA,OACAC,KAAA,OACAC,KAAA,MACAC,IAAA,MACAC,KAAA,KACAC,MAAA,MACAC,IAAA,IACAC,MAAA,MACAC,QAAA,MACAC,MAAA,MACAC,KAAA,MACAC,MAAA,KACAC,QAAA,MACAC,QAAA,MACAC,QAAA,MACAC,MAAA,KACAC,MAAA,MACAC,OAAA,MACAC,KAAA,OAGA7L,EAAA,CACA8L,MAAA,KACAC,MAAA,GACAC,MAAA,KACAC,MAAA,KACAC,KAAA,KACAC,IAAA,GACAC,KAAA,IAIAnM,EAAA,WACAC,EAAAmM,qBAQAlM,EAAA,IAAAmM,OALA,4DAMAlM,EAAA,IAAAkM,OAJA,8FAKAjM,EAAA,IAAAiM,OANA,gFAOAhM,EAAA,IAAAgM,OALA,kCAOA/L,EAAA,kBACAC,EAAA,iBACAC,EAAA,aACAC,EAAA,kBACAC,EAAA,KACAC,EAAA,cACAC,EAAA,IAAAyL,OAAA,sBACAxL,EAAA,IAAAwL,OAAA,IAAApM,EAAAD,EAAA,gBAEAc,EAAA,mBACAC,EAAA,2IAEAC,EAAA,iDAEAC,EAAA,sFACAC,EAAA,oBAEAC,EAAA,WACAC,EAAA,MACAC,EAAA,IAAAgL,OAAA,IAAApM,EAAAD,EAAA,gBAkIA,SAAA0I,GACA,OAAAA,EAAAtC,OAAAlE,KAIAV,EAAA2F,SAAAG,iBAAA9F,EAAAO,QAAA,WAmBAP,EAAA8K,uBAAA,SAAAC,GACA,IAAAC,EAAAD,EAAAE,OAAA,SAAApE,EAAAqE,GAEA,OADArE,EAAAqE,GAAAA,EACArE,GACA,IAEA,OAAA,SAAAK,GACA,GAAAA,GAAA8D,EAAA9D,EAAAjF,cAAAiF,EAAAjF,WAAA,OAAAiF,IAiBAlH,EAAAM,eAAAN,EAAA8K,uBAAA,CACA,IACA,OACA,QACA,SACA,QACA,MACA,SACA,OACA,KACA,QACA,KACA,MACA,MACA,MACA,KACA,KACA,KACA,UACA,OACA,MACA,KACA,MACA,SACA,QACA,OACA,MACA,KACA,OACA,SACA,OACA,OACA,QACA,MACA,OACA,MACA,MACA,MACA,MACA,OACA,KACA,MACA,OACA,MACA,MACA,MACA,UACA,IACA,KACA,KACA,OACA,KACA,KACA,MACA,OACA,QACA,MACA,OACA,SACA,MACA,KACA,QACA,OACA,OACA,KACA,UACA,KACA,MACA,MACA,KACA,MACA,QACA,KACA,OACA,KACA,QACA,MACA,MACA,SACA,OACA,MACA,OACA,MACA,SACA,QACA,KACA,OACA,OACA,OACA,MACA,QACA,OACA,OACA,QACA,QACA,OACA,OACA,MACA,KACA,MACA,OACA,KACA,QACA,MACA,KACA,OACA,OACA,OACA,QACA,QACA,QACA,MACA,OACA,MACA,OACA,OACA,QACA,MACA,MACA,SAGA9K,EAAA2F,SAAAG,iBAAA9F,EAAAM,eAAA,kBAqBAN,EAAAK,QAAA,SAAA6G,GACA,OAAAA,EAAAtC,OAAA,SAAArM,GACA,OAAAA,EAAA+I,QAAA,OAAA,IAAAA,QAAA,OAAA,OAIAtB,EAAA2F,SAAAG,iBAAA9F,EAAAK,QAAA,WA2BAL,EAAAmL,SAAA,WACAxS,KAAAyS,OAAA,EACAzS,KAAA0S,MAAA,GACA1S,KAAA4E,GAAAyC,EAAAmL,SAAAG,QACAtL,EAAAmL,SAAAG,SAAA,GAWAtL,EAAAmL,SAAAG,QAAA,EASAtL,EAAAmL,SAAAI,UAAA,SAAAC,GAGA,IAFA,IAAAtL,EAAA,IAAAF,EAAAmL,SAAAhL,QAEA9H,EAAA,EAAA6M,EAAAsG,EAAAlT,OAAAD,EAAA6M,EAAA7M,IACA6H,EAAA4H,OAAA0D,EAAAnT,IAIA,OADA6H,EAAAuL,SACAvL,EAAAJ,MAYAE,EAAAmL,SAAAO,WAAA,SAAAC,GACA,MAAA,iBAAAA,EACA3L,EAAAmL,SAAAS,gBAAAD,EAAAE,KAAAF,EAAAG,cAEA9L,EAAAmL,SAAAnI,WAAA2I,EAAAE,OAmBA7L,EAAAmL,SAAAS,gBAAA,SAAAlH,EAAAoH,GASA,IARA,IAAAhM,EAAA,IAAAE,EAAAmL,SAEAY,EAAA,CAAA,CACAC,KAAAlM,EACAmM,eAAAH,EACApH,IAAAA,IAGAqH,EAAAzT,QAAA,CACA,IAKA4T,EAwBAC,EACAC,EA9BAC,EAAAN,EAAAO,MAGA,GAAA,EAAAD,EAAA3H,IAAApM,QACA6T,EAAAE,EAAA3H,IAAAa,OAAA,MAGA8G,EAAAL,KAAAX,MACAa,EAAAG,EAAAL,KAAAX,MAAAc,IAEAD,EAAA,IAAAlM,EAAAmL,SACAkB,EAAAL,KAAAX,MAAAc,GAAAD,GAGA,GAAAG,EAAA3H,IAAApM,OACA4T,EAAAd,OAAA,EAEAW,EAAA/H,KAAA,CACAgI,KAAAE,EACAD,eAAAI,EAAAJ,eACAvH,IAAA2H,EAAA3H,IAAA7J,MAAA,KAQA,GAAA,EAAAwR,EAAAJ,gBAAA,EAAAI,EAAA3H,IAAApM,QACA6T,EAAAE,EAAA3H,IAAAa,OAAA,MAGA8G,EAAAL,KAAAX,MACAe,EAAAC,EAAAL,KAAAX,MAAAc,IAEAC,EAAA,IAAApM,EAAAmL,SACAkB,EAAAL,KAAAX,MAAAc,GAAAC,GAGAC,EAAA3H,IAAApM,QAAA,EACA8T,EAAAhB,OAAA,EAEAW,EAAA/H,KAAA,CACAgI,KAAAI,EACAH,eAAAI,EAAAJ,eAAA,EACAvH,IAAA2H,EAAA3H,IAAA7J,MAAA,KAcA,GAPA,EAAAwR,EAAAJ,gBAAA,GAAAI,EAAA3H,IAAApM,SACA+T,EAAAL,KAAAZ,OAAA,GAMA,EAAAiB,EAAAJ,gBAAA,GAAAI,EAAA3H,IAAApM,OAAA,CACA,GAAA,MAAA+T,EAAAL,KAAAX,MACA,IAAAkB,EAAAF,EAAAL,KAAAX,MAAA,SACA,CACAkB,EAAA,IAAAvM,EAAAmL,SACAkB,EAAAL,KAAAX,MAAA,KAAAkB,EAGA,GAAAF,EAAA3H,IAAApM,OACAiU,EAAAnB,OAAA,EAEAW,EAAA/H,KAAA,CACAgI,KAAAO,EACAN,eAAAI,EAAAJ,eAAA,EACAvH,IAAA2H,EAAA3H,IAAA7J,MAAA,KAOA,GAAA,EAAAwR,EAAAJ,eAAA,CACA,GAAA,MAAAI,EAAAL,KAAAX,MACA,IAAAmB,EAAAH,EAAAL,KAAAX,MAAA,SACA,CACAmB,EAAA,IAAAxM,EAAAmL,SACAkB,EAAAL,KAAAX,MAAA,KAAAmB,EAGA,GAAAH,EAAA3H,IAAApM,OACAkU,EAAApB,OAAA,EAEAW,EAAA/H,KAAA,CACAgI,KAAAQ,EACAP,eAAAI,EAAAJ,eAAA,EACAvH,IAAA2H,EAAA3H,MAQA,GAAA,EAAA2H,EAAAJ,gBAAA,EAAAI,EAAA3H,IAAApM,OAAA,CACA,IAEAmU,EAFAC,EAAAL,EAAA3H,IAAAa,OAAA,GACAoH,EAAAN,EAAA3H,IAAAa,OAAA,GAGAoH,KAAAN,EAAAL,KAAAX,MACAoB,EAAAJ,EAAAL,KAAAX,MAAAsB,IAEAF,EAAA,IAAAzM,EAAAmL,SACAkB,EAAAL,KAAAX,MAAAsB,GAAAF,GAGA,GAAAJ,EAAA3H,IAAApM,OACAmU,EAAArB,OAAA,EAEAW,EAAA/H,KAAA,CACAgI,KAAAS,EACAR,eAAAI,EAAAJ,eAAA,EACAvH,IAAAgI,EAAAL,EAAA3H,IAAA7J,MAAA,MAMA,OAAAiF,GAaAE,EAAAmL,SAAAnI,WAAA,SAAA0B,GAYA,IAXA,IAAAsH,EAAA,IAAAhM,EAAAmL,SACArL,EAAAkM,EAUA3T,EAAA,EAAA6M,EAAAR,EAAApM,OAAAD,EAAA6M,EAAA7M,IAAA,CACA,IAAA8T,EAAAzH,EAAArM,GACA+S,EAAA/S,GAAA6M,EAAA,EAEA,GAAA,KAAAiH,GACAH,EAAAX,MAAAc,GAAAH,GACAZ,MAAAA,MAEA,CACA,IAAAwB,EAAA,IAAA5M,EAAAmL,SACAyB,EAAAxB,MAAAA,EAEAY,EAAAX,MAAAc,GAAAS,EACAZ,EAAAY,GAIA,OAAA9M,GASAE,EAAAmL,SAAAvQ,UAAAkO,QAAA,WAQA,IAPA,IAAAkC,EAAA,GAEAe,EAAA,CAAA,CACAc,OAAA,GACAb,KAAArT,OAGAoT,EAAAzT,QAAA,CACA,IAAA+T,EAAAN,EAAAO,MACAjB,EAAAlJ,OAAAE,KAAAgK,EAAAL,KAAAX,OACAnG,EAAAmG,EAAA/S,OAEA+T,EAAAL,KAAAZ,QAKAiB,EAAAQ,OAAAtH,OAAA,GACAyF,EAAAhH,KAAAqI,EAAAQ,SAGA,IAAA,IAAAxU,EAAA,EAAAA,EAAA6M,EAAA7M,IAAA,CACA,IAAAyU,EAAAzB,EAAAhT,GAEA0T,EAAA/H,KAAA,CACA6I,OAAAR,EAAAQ,OAAA5I,OAAA6I,GACAd,KAAAK,EAAAL,KAAAX,MAAAyB,MAKA,OAAA9B,GAaAhL,EAAAmL,SAAAvQ,UAAAqH,SAAA,WASA,GAAAtJ,KAAAoU,KACA,OAAApU,KAAAoU,KAOA,IAJA,IAAArI,EAAA/L,KAAAyS,MAAA,IAAA,IACA4B,EAAA7K,OAAAE,KAAA1J,KAAA0S,OAAA4B,OACA/H,EAAA8H,EAAA1U,OAEAD,EAAA,EAAAA,EAAA6M,EAAA7M,IAAA,CACA,IAAA0N,EAAAiH,EAAA3U,GAGAqM,EAAAA,EAAAqB,EAFApN,KAAA0S,MAAAtF,GAEAxI,GAGA,OAAAmH,GAaA1E,EAAAmL,SAAAvQ,UAAA4I,UAAA,SAAAM,GAUA,IATA,IAAAiF,EAAA,IAAA/I,EAAAmL,SACAkB,OAAAjJ,EAEA2I,EAAA,CAAA,CACAmB,MAAApJ,EACAiF,OAAAA,EACAiD,KAAArT,OAGAoT,EAAAzT,QAAA,CACA+T,EAAAN,EAAAO,MAWA,IALA,IAAAa,EAAAhL,OAAAE,KAAAgK,EAAAa,MAAA7B,OACA+B,EAAAD,EAAA7U,OACA+U,EAAAlL,OAAAE,KAAAgK,EAAAL,KAAAX,OACAiC,EAAAD,EAAA/U,OAEAiV,EAAA,EAAAA,EAAAH,EAAAG,IAGA,IAFA,IAAAC,EAAAL,EAAAI,GAEAtK,EAAA,EAAAA,EAAAqK,EAAArK,IAAA,CACA,IAAAwK,EAAAJ,EAAApK,GAEA,GAAAwK,GAAAD,GAAA,KAAAA,EAAA,CACA,IAAAxB,EAAAK,EAAAL,KAAAX,MAAAoC,GACAP,EAAAb,EAAAa,MAAA7B,MAAAmC,GACApC,EAAAY,EAAAZ,OAAA8B,EAAA9B,MACAwB,OAAAxJ,EAEAqK,KAAApB,EAAAtD,OAAAsC,OAIAuB,EAAAP,EAAAtD,OAAAsC,MAAAoC,IACArC,MAAAwB,EAAAxB,OAAAA,IAMAwB,EAAA,IAAA5M,EAAAmL,UACAC,MAAAA,EACAiB,EAAAtD,OAAAsC,MAAAoC,GAAAb,GAGAb,EAAA/H,KAAA,CACAkJ,MAAAA,EACAnE,OAAA6D,EACAZ,KAAAA,MAOA,OAAAjD,GAEA/I,EAAAmL,SAAAhL,QAAA,WACAxH,KAAA+U,aAAA,GACA/U,KAAAmH,KAAA,IAAAE,EAAAmL,SACAxS,KAAAgV,eAAA,GACAhV,KAAAiV,eAAA,IAGA5N,EAAAmL,SAAAhL,QAAAvF,UAAAkN,OAAA,SAAA+F,GACA,IAAA7B,EACA8B,EAAA,EAEA,GAAAD,EAAAlV,KAAA+U,aACA,MAAA,IAAAtH,MAAA,+BAGA,IAAA,IAAA/N,EAAA,EAAAA,EAAAwV,EAAAvV,QAAAD,EAAAM,KAAA+U,aAAApV,QACAuV,EAAAxV,IAAAM,KAAA+U,aAAArV,GADAA,IAEAyV,IAGAnV,KAAAoV,SAAAD,GAGA9B,EADA,GAAArT,KAAAgV,eAAArV,OACAK,KAAAmH,KAEAnH,KAAAgV,eAAAhV,KAAAgV,eAAArV,OAAA,GAAA0V,MAGA,IAAA3V,EAAAyV,EAAAzV,EAAAwV,EAAAvV,OAAAD,IAAA,CACA,IAAA4V,EAAA,IAAAjO,EAAAmL,SACAgB,EAAA0B,EAAAxV,GAEA2T,EAAAX,MAAAc,GAAA8B,EAEAtV,KAAAgV,eAAA3J,KAAA,CACAkK,OAAAlC,EACAG,KAAAA,EACA6B,MAAAC,IAGAjC,EAAAiC,EAGAjC,EAAAZ,OAAA,EACAzS,KAAA+U,aAAAG,GAGA7N,EAAAmL,SAAAhL,QAAAvF,UAAA6Q,OAAA,WACA9S,KAAAoV,SAAA,IAGA/N,EAAAmL,SAAAhL,QAAAvF,UAAAmT,SAAA,SAAAI,GACA,IAAA,IAAA9V,EAAAM,KAAAgV,eAAArV,OAAA,EAAA6V,GAAA9V,EAAAA,IAAA,CACA,IAAA2T,EAAArT,KAAAgV,eAAAtV,GACA+V,EAAApC,EAAAgC,MAAA/L,WAEAmM,KAAAzV,KAAAiV,eACA5B,EAAAkC,OAAA7C,MAAAW,EAAAG,MAAAxT,KAAAiV,eAAAQ,IAIApC,EAAAgC,MAAAjB,KAAAqB,EAEAzV,KAAAiV,eAAAQ,GAAApC,EAAAgC,OAGArV,KAAAgV,eAAArB,QAwBAtM,EAAAqO,MAAA,SAAAC,GACA3V,KAAA4V,cAAAD,EAAAC,cACA5V,KAAA6V,aAAAF,EAAAE,aACA7V,KAAA8V,SAAAH,EAAAG,SACA9V,KAAA+V,OAAAJ,EAAAI,OACA/V,KAAAyH,SAAAkO,EAAAlO,UA0EAJ,EAAAqO,MAAAzT,UAAA+T,OAAA,SAAAC,GACA,OAAAjW,KAAAkW,MAAA,SAAAA,GACA,IAAA7O,EAAA8O,YAAAF,EAAAC,GACAE,WA6BA/O,EAAAqO,MAAAzT,UAAAiU,MAAA,SAAAhK,GAoBA,IAZA,IAAAgK,EAAA,IAAA7O,EAAAgP,MAAArW,KAAA+V,QACAO,EAAA9M,OAAAC,OAAA,MACA8M,EAAA/M,OAAAC,OAAA,MACA+M,EAAAhN,OAAAC,OAAA,MACAgN,EAAAjN,OAAAC,OAAA,MACAiN,EAAAlN,OAAAC,OAAA,MAOA/J,EAAA,EAAAA,EAAAM,KAAA+V,OAAApW,OAAAD,IACA6W,EAAAvW,KAAA+V,OAAArW,IAAA,IAAA2H,EAAAqH,OAGAxC,EAAA/J,KAAA+T,EAAAA,GAEA,IAAAxW,EAAA,EAAAA,EAAAwW,EAAAS,QAAAhX,OAAAD,IAAA,CASA,IAAAsT,EAAAkD,EAAAS,QAAAjX,GACAkX,EAAA,KACAC,EAAAxP,EAAAqD,IAAAE,SAGAgM,EADA5D,EAAA8D,YACA9W,KAAAyH,SAAA6G,UAAA0E,EAAAE,KAAA,CACA6C,OAAA/C,EAAA+C,SAGA,CAAA/C,EAAAE,MAGA,IAAA,IAAA6D,EAAA,EAAAA,EAAAH,EAAAjX,OAAAoX,IAAA,CACA,IAAA7D,EAAA0D,EAAAG,GAQA/D,EAAAE,KAAAA,EAOA,IAAA8D,EAAA3P,EAAAmL,SAAAO,WAAAC,GACAiE,EAAAjX,KAAA8V,SAAAjL,UAAAmM,GAAA7G,UAQA,GAAA,IAAA8G,EAAAtX,QAAAqT,EAAAkE,WAAA7P,EAAAgP,MAAAa,SAAAC,SAAA,CACA,IAAA,IAAA9I,EAAA,EAAAA,EAAA2E,EAAA+C,OAAApW,OAAA0O,IAAA,CAEAoI,EADAW,EAAApE,EAAA+C,OAAA1H,IACAhH,EAAAqD,IAAAM,MAGA,MAGA,IAAA,IAAAmD,EAAA,EAAAA,EAAA8I,EAAAtX,OAAAwO,IAKA,CAAA,IAAAkJ,EAAAJ,EAAA9I,GACA3C,EAAAxL,KAAA4V,cAAAyB,GACAC,EAAA9L,EAAA+L,OAEA,IAAAlJ,EAAA,EAAAA,EAAA2E,EAAA+C,OAAApW,OAAA0O,IAAA,CASA,IACAmJ,EAAAhM,EADA4L,EAAApE,EAAA+C,OAAA1H,IAEAoJ,EAAAjO,OAAAE,KAAA8N,GACAE,EAAAL,EAAA,IAAAD,EACAO,EAAA,IAAAtQ,EAAAqD,IAAA+M,GAoBA,GAbAzE,EAAAkE,UAAA7P,EAAAgP,MAAAa,SAAAC,WACAN,EAAAA,EAAA9L,MAAA4M,QAEAlN,IAAAgM,EAAAW,KACAX,EAAAW,GAAA/P,EAAAqD,IAAAE,WASAoI,EAAAkE,UAAA7P,EAAAgP,MAAAa,SAAAU,YA4BA,GANArB,EAAAa,GAAA/H,OAAAiI,EAAAtE,EAAA6E,MAAA,SAAA3M,EAAAC,GAAA,OAAAD,EAAAC,KAMAqL,EAAAkB,GAAA,CAIA,IAAA,IAAAI,EAAA,EAAAA,EAAAL,EAAA9X,OAAAmY,IAAA,CAOA,IAGAC,EAHAC,EAAAP,EAAAK,GACAG,EAAA,IAAA5Q,EAAA0C,SAAAiO,EAAAZ,GACApL,EAAAwL,EAAAQ,QAGAvN,KAAAsN,EAAAzB,EAAA2B,IACA3B,EAAA2B,GAAA,IAAA5Q,EAAA6Q,UAAAb,EAAAD,EAAApL,GAEA+L,EAAA7Y,IAAAmY,EAAAD,EAAApL,GAKAwK,EAAAkB,IAAA,aAnDAjN,IAAAiM,EAAAU,KACAV,EAAAU,GAAA/P,EAAAqD,IAAAM,OAGA0L,EAAAU,GAAAV,EAAAU,GAAArM,MAAA4M,KA0DA,GAAA3E,EAAAkE,WAAA7P,EAAAgP,MAAAa,SAAAC,SACA,IAAA9I,EAAA,EAAAA,EAAA2E,EAAA+C,OAAApW,OAAA0O,IAAA,CAEAoI,EADAW,EAAApE,EAAA+C,OAAA1H,IACAoI,EAAAW,GAAAvM,UAAAgM,IAUA,IAAAsB,EAAA9Q,EAAAqD,IAAAE,SACAwN,EAAA/Q,EAAAqD,IAAAM,MAEA,IAAAtL,EAAA,EAAAA,EAAAM,KAAA+V,OAAApW,OAAAD,IAAA,CACA,IAAA0X,EAEAX,EAFAW,EAAApX,KAAA+V,OAAArW,MAGAyY,EAAAA,EAAAtN,UAAA4L,EAAAW,KAGAV,EAAAU,KACAgB,EAAAA,EAAArN,MAAA2L,EAAAU,KAIA,IAAAiB,EAAA7O,OAAAE,KAAA4M,GACAgC,EAAA,GACA1Z,EAAA4K,OAAAC,OAAA,MAYA,GAAAyM,EAAAqC,YAAA,CACAF,EAAA7O,OAAAE,KAAA1J,KAAA6V,cAEA,IAAAnW,EAAA,EAAAA,EAAA2Y,EAAA1Y,OAAAD,IAAA,CACAuY,EAAAI,EAAA3Y,GAAA,IACA8K,EAAAnD,EAAA0C,SAAAM,WAAA4N,GACA3B,EAAA2B,GAAA,IAAA5Q,EAAA6Q,WAIA,IAAAxY,EAAA,EAAAA,EAAA2Y,EAAA1Y,OAAAD,IAAA,CASA,IACAsK,GADAQ,EAAAnD,EAAA0C,SAAAM,WAAAgO,EAAA3Y,KACAsK,OAEA,GAAAmO,EAAAlX,SAAA+I,KAIAoO,EAAAnX,SAAA+I,GAAA,CAIA,IAEAwO,EAFAC,EAAAzY,KAAA6V,aAAArL,GACAkO,EAAAnC,EAAA/L,EAAAP,WAAAiG,WAAAuI,GAGA,QAAAhO,KAAA+N,EAAA5Z,EAAAoL,IACAwO,EAAAE,OAAAA,EACAF,EAAAG,UAAAC,QAAAtC,EAAA9L,QACA,CACA,IAAAqC,EAAA,CACAgM,IAAA7O,EACA0O,MAAAA,EACAC,UAAArC,EAAA9L,IAEA5L,EAAAoL,GAAA6C,EACAyL,EAAAjN,KAAAwB,KAOA,OAAAyL,EAAAhE,KAAA,SAAApJ,EAAAC,GACA,OAAAA,EAAAuN,MAAAxN,EAAAwN,SAYArR,EAAAqO,MAAAzT,UAAAwM,OAAA,WACA,IAAAmH,EAAApM,OAAAE,KAAA1J,KAAA4V,eACAtB,OACAlI,IAAA,SAAA8G,GACA,MAAA,CAAAA,EAAAlT,KAAA4V,cAAA1C,KACAlT,MAEA6V,EAAArM,OAAAE,KAAA1J,KAAA6V,cACAzJ,IAAA,SAAAyM,GACA,MAAA,CAAAA,EAAA7Y,KAAA6V,aAAAgD,GAAApK,WACAzO,MAEA,MAAA,CACA+I,QAAA1B,EAAA0B,QACAgN,OAAA/V,KAAA+V,OACAF,aAAAA,EACAD,cAAAA,EACAnO,SAAAzH,KAAAyH,SAAAgH,WAUApH,EAAAqO,MAAApI,KAAA,SAAAwL,GACA,IAAAnD,EAAA,GACAE,EAAA,GACAkD,EAAAD,EAAAjD,aACAD,EAAA,GACAoD,EAAAF,EAAAlD,cACAqD,EAAA,IAAA5R,EAAAmL,SAAAhL,QACAC,EAAAJ,EAAA2F,SAAAM,KAAAwL,EAAArR,UAEAqR,EAAA/P,SAAA1B,EAAA0B,SACA1B,EAAA2B,MAAAC,KAAA,4EAAA5B,EAAA0B,QAAA,sCAAA+P,EAAA/P,QAAA,KAGA,IAAA,IAAArJ,EAAA,EAAAA,EAAAqZ,EAAApZ,OAAAD,IAAA,CACA,IACAmZ,GADAK,EAAAH,EAAArZ,IACA,GACAiL,EAAAuO,EAAA,GAEArD,EAAAgD,GAAA,IAAAxR,EAAAqH,OAAA/D,GAGA,IAAAjL,EAAA,EAAAA,EAAAsZ,EAAArZ,OAAAD,IAAA,CACA,IAAAwZ,EACAhG,GADAgG,EAAAF,EAAAtZ,IACA,GACA8L,EAAA0N,EAAA,GAEAD,EAAA9J,OAAA+D,GACA0C,EAAA1C,GAAA1H,EAYA,OATAyN,EAAAnG,SAEA6C,EAAAI,OAAA+C,EAAA/C,OAEAJ,EAAAE,aAAAA,EACAF,EAAAC,cAAAA,EACAD,EAAAG,SAAAmD,EAAA9R,KACAwO,EAAAlO,SAAAA,EAEA,IAAAJ,EAAAqO,MAAAC,IA+BAtO,EAAAG,QAAA,WACAxH,KAAAmZ,KAAA,KACAnZ,KAAAoZ,QAAA5P,OAAAC,OAAA,MACAzJ,KAAAqZ,WAAA7P,OAAAC,OAAA,MACAzJ,KAAA4V,cAAApM,OAAAC,OAAA,MACAzJ,KAAAsZ,qBAAA,GACAtZ,KAAAuZ,aAAA,GACAvZ,KAAAmM,UAAA9E,EAAA8E,UACAnM,KAAAyH,SAAA,IAAAJ,EAAA2F,SACAhN,KAAA6H,eAAA,IAAAR,EAAA2F,SACAhN,KAAAyL,cAAA,EACAzL,KAAAwZ,GAAA,IACAxZ,KAAAyZ,IAAA,IACAzZ,KAAAsX,UAAA,EACAtX,KAAA0Z,kBAAA,IAeArS,EAAAG,QAAAvF,UAAA4W,IAAA,SAAAA,GACA7Y,KAAAmZ,KAAAN,GAmCAxR,EAAAG,QAAAvF,UAAAmV,MAAA,SAAAnN,EAAA0P,GACA,GAAA,KAAAjR,KAAAuB,GACA,MAAA,IAAA2P,WAAA,UAAA3P,EAAA,oCAGAjK,KAAAoZ,QAAAnP,GAAA0P,GAAA,IAWAtS,EAAAG,QAAAvF,UAAAkJ,EAAA,SAAA0O,GAEA7Z,KAAAwZ,GADAK,EAAA,EACA,EACA,EAAAA,EACA,EAEAA,GAWAxS,EAAAG,QAAAvF,UAAA6X,GAAA,SAAAD,GACA7Z,KAAAyZ,IAAAI,GAoBAxS,EAAAG,QAAAvF,UAAA/C,IAAA,SAAA6a,EAAAJ,GACA,IAAA3P,EAAA+P,EAAA/Z,KAAAmZ,MACApD,EAAAvM,OAAAE,KAAA1J,KAAAoZ,SAEApZ,KAAAqZ,WAAArP,GAAA2P,GAAA,GACA3Z,KAAAyL,eAAA,EAEA,IAAA,IAAA/L,EAAA,EAAAA,EAAAqW,EAAApW,OAAAD,IAAA,CACA,IAAAuK,EAAA8L,EAAArW,GACAsa,EAAAha,KAAAoZ,QAAAnP,GAAA+P,UACA5C,EAAA4C,EAAAA,EAAAD,GAAAA,EAAA9P,GACAuC,EAAAxM,KAAAmM,UAAAiL,EAAA,CACArB,OAAA,CAAA9L,KAEA2M,EAAA5W,KAAAyH,SAAAuG,IAAAxB,GACAhC,EAAA,IAAAnD,EAAA0C,SAAAC,EAAAC,GACAgQ,EAAAzQ,OAAAC,OAAA,MAEAzJ,KAAAsZ,qBAAA9O,GAAAyP,EACAja,KAAAuZ,aAAA/O,GAAA,EAGAxK,KAAAuZ,aAAA/O,IAAAoM,EAAAjX,OAGA,IAAA,IAAAwO,EAAA,EAAAA,EAAAyI,EAAAjX,OAAAwO,IAAA,CACA,IAAA+E,EAAA0D,EAAAzI,GAUA,GARA1D,MAAAwP,EAAA/G,KACA+G,EAAA/G,GAAA,GAGA+G,EAAA/G,IAAA,EAIAzI,MAAAzK,KAAA4V,cAAA1C,GAAA,CACA,IAAA1H,EAAAhC,OAAAC,OAAA,MACA+B,EAAA,OAAAxL,KAAAsX,UACAtX,KAAAsX,WAAA,EAEA,IAAA,IAAAjJ,EAAA,EAAAA,EAAA0H,EAAApW,OAAA0O,IACA7C,EAAAuK,EAAA1H,IAAA7E,OAAAC,OAAA,MAGAzJ,KAAA4V,cAAA1C,GAAA1H,EAIAf,MAAAzK,KAAA4V,cAAA1C,GAAAjJ,GAAAD,KACAhK,KAAA4V,cAAA1C,GAAAjJ,GAAAD,GAAAR,OAAAC,OAAA,OAKA,IAAA,IAAAqO,EAAA,EAAAA,EAAA9X,KAAA0Z,kBAAA/Z,OAAAmY,IAAA,CACA,IAAAoC,EAAAla,KAAA0Z,kBAAA5B,GACA9L,EAAAkH,EAAAlH,SAAAkO,GAEAzP,MAAAzK,KAAA4V,cAAA1C,GAAAjJ,GAAAD,GAAAkQ,KACAla,KAAA4V,cAAA1C,GAAAjJ,GAAAD,GAAAkQ,GAAA,IAGAla,KAAA4V,cAAA1C,GAAAjJ,GAAAD,GAAAkQ,GAAA7O,KAAAW,OAYA3E,EAAAG,QAAAvF,UAAAkY,6BAAA,WAOA,IALA,IAAAC,EAAA5Q,OAAAE,KAAA1J,KAAAuZ,cACAc,EAAAD,EAAAza,OACA2a,EAAA,GACAC,EAAA,GAEA7a,EAAA,EAAAA,EAAA2a,EAAA3a,IAAA,CACA,IAAA8K,EAAAnD,EAAA0C,SAAAM,WAAA+P,EAAA1a,IACA0X,EAAA5M,EAAAP,UAEAsQ,EAAAnD,KAAAmD,EAAAnD,GAAA,GACAmD,EAAAnD,IAAA,EAEAkD,EAAAlD,KAAAkD,EAAAlD,GAAA,GACAkD,EAAAlD,IAAApX,KAAAuZ,aAAA/O,GAGA,IAAAuL,EAAAvM,OAAAE,KAAA1J,KAAAoZ,SAEA,IAAA1Z,EAAA,EAAAA,EAAAqW,EAAApW,OAAAD,IAAA,CACA,IAAAuK,EAAA8L,EAAArW,GACA4a,EAAArQ,GAAAqQ,EAAArQ,GAAAsQ,EAAAtQ,GAGAjK,KAAAwa,mBAAAF,GAQAjT,EAAAG,QAAAvF,UAAAwY,mBAAA,WAMA,IALA,IAAA5E,EAAA,GACAuE,EAAA5Q,OAAAE,KAAA1J,KAAAsZ,sBACAoB,EAAAN,EAAAza,OACAgb,EAAAnR,OAAAC,OAAA,MAEA/J,EAAA,EAAAA,EAAAgb,EAAAhb,IAAA,CAaA,IAZA,IAAA8K,EAAAnD,EAAA0C,SAAAM,WAAA+P,EAAA1a,IACAuK,EAAAO,EAAAP,UACA2Q,EAAA5a,KAAAuZ,aAAA/O,GACAiO,EAAA,IAAApR,EAAAqH,OACAmM,EAAA7a,KAAAsZ,qBAAA9O,GACAoM,EAAApN,OAAAE,KAAAmR,GACAC,EAAAlE,EAAAjX,OAGAob,EAAA/a,KAAAoZ,QAAAnP,GAAA4N,OAAA,EACAmD,EAAAhb,KAAAqZ,WAAA7O,EAAAR,QAAA6N,OAAA,EAEA1J,EAAA,EAAAA,EAAA2M,EAAA3M,IAAA,CACA,IAGA5C,EAAAmN,EAAAuC,EAHA/H,EAAA0D,EAAAzI,GACA+M,EAAAL,EAAA3H,GACAoE,EAAAtX,KAAA4V,cAAA1C,GAAAqE,YAGA9M,IAAAkQ,EAAAzH,IACA3H,EAAAlE,EAAAkE,IAAAvL,KAAA4V,cAAA1C,GAAAlT,KAAAyL,eACAkP,EAAAzH,GAAA3H,GAEAA,EAAAoP,EAAAzH,GAGAwF,EAAAnN,IAAAvL,KAAAyZ,IAAA,GAAAyB,IAAAlb,KAAAyZ,KAAA,EAAAzZ,KAAAwZ,GAAAxZ,KAAAwZ,IAAAoB,EAAA5a,KAAAwa,mBAAAvQ,KAAAiR,GACAxC,GAAAqC,EACArC,GAAAsC,EACAC,EAAAtP,KAAAwP,MAAA,IAAAzC,GAAA,IAQAD,EAAAtJ,OAAAmI,EAAA2D,GAGApF,EAAArL,GAAAiO,EAGAzY,KAAA6V,aAAAA,GAQAxO,EAAAG,QAAAvF,UAAAmZ,eAAA,WACApb,KAAA8V,SAAAzO,EAAAmL,SAAAI,UACApJ,OAAAE,KAAA1J,KAAA4V,eAAAtB,SAYAjN,EAAAG,QAAAvF,UAAA6F,MAAA,WAKA,OAJA9H,KAAAma,+BACAna,KAAAya,qBACAza,KAAAob,iBAEA,IAAA/T,EAAAqO,MAAA,CACAE,cAAA5V,KAAA4V,cACAC,aAAA7V,KAAA6V,aACAC,SAAA9V,KAAA8V,SACAC,OAAAvM,OAAAE,KAAA1J,KAAAoZ,SACA3R,SAAAzH,KAAA6H,kBAkBAR,EAAAG,QAAAvF,UAAAoZ,IAAA,SAAAnP,GACA,IAAAoP,EAAAtZ,MAAAC,UAAAC,MAAAC,KAAAuL,UAAA,GACA4N,EAAAC,QAAAvb,MACAkM,EAAAsP,MAAAxb,KAAAsb,IAcAjU,EAAA6Q,UAAA,SAAAhF,EAAAkE,EAAApL,GASA,IARA,IAAAyP,EAAAjS,OAAAC,OAAA,MACAiS,EAAAlS,OAAAE,KAAAsC,GAAA,IAOAtM,EAAA,EAAAA,EAAAgc,EAAA/b,OAAAD,IAAA,CACA,IAAAiK,EAAA+R,EAAAhc,GACA+b,EAAA9R,GAAAqC,EAAArC,GAAAzH,QAGAlC,KAAAgM,SAAAxC,OAAAC,OAAA,WAEAgB,IAAAyI,IACAlT,KAAAgM,SAAAkH,GAAA1J,OAAAC,OAAA,MACAzJ,KAAAgM,SAAAkH,GAAAkE,GAAAqE,IAaApU,EAAA6Q,UAAAjW,UAAA2W,QAAA,SAAA+C,GAGA,IAFA,IAAA/E,EAAApN,OAAAE,KAAAiS,EAAA3P,UAEAtM,EAAA,EAAAA,EAAAkX,EAAAjX,OAAAD,IAAA,CACA,IAAAwT,EAAA0D,EAAAlX,GACAqW,EAAAvM,OAAAE,KAAAiS,EAAA3P,SAAAkH,IAEAzI,MAAAzK,KAAAgM,SAAAkH,KACAlT,KAAAgM,SAAAkH,GAAA1J,OAAAC,OAAA,OAGA,IAAA,IAAA0E,EAAA,EAAAA,EAAA4H,EAAApW,OAAAwO,IAAA,CACA,IAAAiJ,EAAArB,EAAA5H,GACAzE,EAAAF,OAAAE,KAAAiS,EAAA3P,SAAAkH,GAAAkE,IAEA3M,MAAAzK,KAAAgM,SAAAkH,GAAAkE,KACApX,KAAAgM,SAAAkH,GAAAkE,GAAA5N,OAAAC,OAAA,OAGA,IAAA,IAAA4E,EAAA,EAAAA,EAAA3E,EAAA/J,OAAA0O,IAAA,CACA,IAAA1E,EAAAD,EAAA2E,GAEA5D,MAAAzK,KAAAgM,SAAAkH,GAAAkE,GAAAzN,GACA3J,KAAAgM,SAAAkH,GAAAkE,GAAAzN,GAAAgS,EAAA3P,SAAAkH,GAAAkE,GAAAzN,GAEA3J,KAAAgM,SAAAkH,GAAAkE,GAAAzN,GAAA3J,KAAAgM,SAAAkH,GAAAkE,GAAAzN,GAAA2B,OAAAqQ,EAAA3P,SAAAkH,GAAAkE,GAAAzN,QAeAtC,EAAA6Q,UAAAjW,UAAA/C,IAAA,SAAAgU,EAAAkE,EAAApL,GACA,KAAAkH,KAAAlT,KAAAgM,UAGA,OAFAhM,KAAAgM,SAAAkH,GAAA1J,OAAAC,OAAA,WACAzJ,KAAAgM,SAAAkH,GAAAkE,GAAApL,GAIA,GAAAoL,KAAApX,KAAAgM,SAAAkH,GAOA,IAFA,IAAAwI,EAAAlS,OAAAE,KAAAsC,GAEAtM,EAAA,EAAAA,EAAAgc,EAAA/b,OAAAD,IAAA,CACA,IAAAiK,EAAA+R,EAAAhc,GAEAiK,KAAA3J,KAAAgM,SAAAkH,GAAAkE,GACApX,KAAAgM,SAAAkH,GAAAkE,GAAAzN,GAAA3J,KAAAgM,SAAAkH,GAAAkE,GAAAzN,GAAA2B,OAAAU,EAAArC,IAEA3J,KAAAgM,SAAAkH,GAAAkE,GAAAzN,GAAAqC,EAAArC,QAZA3J,KAAAgM,SAAAkH,GAAAkE,GAAApL,GA2BA3E,EAAAgP,MAAA,SAAAuF,GACA5b,KAAA2W,QAAA,GACA3W,KAAA4b,UAAAA,GA2BAvU,EAAAgP,MAAAwF,SAAA,IAAAC,OAAA,KACAzU,EAAAgP,MAAAwF,SAAAE,KAAA,EACA1U,EAAAgP,MAAAwF,SAAAG,QAAA,EACA3U,EAAAgP,MAAAwF,SAAAI,SAAA,EAaA5U,EAAAgP,MAAAa,SAAA,CAIAgF,SAAA,EAMA/E,SAAA,EAMAS,WAAA,GA0BAvQ,EAAAgP,MAAApU,UAAA+Q,OAAA,SAAAA,GA+BA,MA9BA,WAAAA,IACAA,EAAA+C,OAAA/V,KAAA4b,WAGA,UAAA5I,IACAA,EAAA6E,MAAA,GAGA,gBAAA7E,IACAA,EAAA8D,aAAA,GAGA,aAAA9D,IACAA,EAAA6I,SAAAxU,EAAAgP,MAAAwF,SAAAE,MAGA/I,EAAA6I,SAAAxU,EAAAgP,MAAAwF,SAAAG,SAAAhJ,EAAAE,KAAAtG,OAAA,IAAAvF,EAAAgP,MAAAwF,WACA7I,EAAAE,KAAA,IAAAF,EAAAE,MAGAF,EAAA6I,SAAAxU,EAAAgP,MAAAwF,SAAAI,UAAAjJ,EAAAE,KAAAhR,OAAA,IAAAmF,EAAAgP,MAAAwF,WACA7I,EAAAE,KAAAF,EAAAE,KAAA,KAGA,aAAAF,IACAA,EAAAkE,SAAA7P,EAAAgP,MAAAa,SAAAgF,UAGAlc,KAAA2W,QAAAtL,KAAA2H,GAEAhT,MAUAqH,EAAAgP,MAAApU,UAAAsW,UAAA,WACA,IAAA,IAAA7Y,EAAA,EAAAA,EAAAM,KAAA2W,QAAAhX,OAAAD,IACA,GAAAM,KAAA2W,QAAAjX,GAAAwX,UAAA7P,EAAAgP,MAAAa,SAAAU,WACA,OAAA,EAIA,OAAA,GA6BAvQ,EAAAgP,MAAApU,UAAAiR,KAAA,SAAAA,EAAA/S,GACA,GAAA6B,MAAA6H,QAAAqJ,GAEA,OADAA,EAAA9Q,QAAA,SAAAiK,GAAArM,KAAAkT,KAAA7G,EAAAhF,EAAA2B,MAAAO,MAAApJ,KAAAH,MACAA,KAGA,IAAAgT,EAAA7S,GAAA,GAKA,OAJA6S,EAAAE,KAAAA,EAAA5J,WAEAtJ,KAAAgT,OAAAA,GAEAhT,MAEAqH,EAAA8U,gBAAA,SAAAjT,EAAA4F,EAAAC,GACA/O,KAAAoc,KAAA,kBACApc,KAAAkJ,QAAAA,EACAlJ,KAAA8O,MAAAA,EACA9O,KAAA+O,IAAAA,GAGA1H,EAAA8U,gBAAAla,UAAA,IAAAwL,MACApG,EAAAgV,WAAA,SAAAtQ,GACA/L,KAAAsc,QAAA,GACAtc,KAAA+L,IAAAA,EACA/L,KAAAL,OAAAoM,EAAApM,OACAK,KAAA8N,IAAA,EACA9N,KAAA8O,MAAA,EACA9O,KAAAuc,oBAAA,IAGAlV,EAAAgV,WAAApa,UAAA+L,IAAA,WAGA,IAFA,IAAAwO,EAAAnV,EAAAgV,WAAAI,QAEAD,GACAA,EAAAA,EAAAxc,OAIAqH,EAAAgV,WAAApa,UAAAya,YAAA,WAKA,IAJA,IAAAC,EAAA,GACAjQ,EAAA1M,KAAA8O,MACArC,EAAAzM,KAAA8N,IAEApO,EAAA,EAAAA,EAAAM,KAAAuc,oBAAA5c,OAAAD,IACA+M,EAAAzM,KAAAuc,oBAAA7c,GACAid,EAAAtR,KAAArL,KAAA+L,IAAA7J,MAAAwK,EAAAD,IACAC,EAAAD,EAAA,EAMA,OAHAkQ,EAAAtR,KAAArL,KAAA+L,IAAA7J,MAAAwK,EAAA1M,KAAA8N,MACA9N,KAAAuc,oBAAA5c,OAAA,EAEAgd,EAAArZ,KAAA,KAGA+D,EAAAgV,WAAApa,UAAA2a,KAAA,SAAAC,GACA7c,KAAAsc,QAAAjR,KAAA,CACAwR,KAAAA,EACA9Q,IAAA/L,KAAA0c,cACA5N,MAAA9O,KAAA8O,MACAC,IAAA/O,KAAA8N,MAGA9N,KAAA8O,MAAA9O,KAAA8N,KAGAzG,EAAAgV,WAAApa,UAAA6a,gBAAA,WACA9c,KAAAuc,oBAAAlR,KAAArL,KAAA8N,IAAA,GACA9N,KAAA8N,KAAA,GAGAzG,EAAAgV,WAAApa,UAAAgS,KAAA,WACA,GAAAjU,KAAA8N,KAAA9N,KAAAL,OACA,OAAA0H,EAAAgV,WAAAU,IAGA,IAAAvJ,EAAAxT,KAAA+L,IAAAa,OAAA5M,KAAA8N,KAEA,OADA9N,KAAA8N,KAAA,EACA0F,GAGAnM,EAAAgV,WAAApa,UAAA+a,MAAA,WACA,OAAAhd,KAAA8N,IAAA9N,KAAA8O,OAGAzH,EAAAgV,WAAApa,UAAAgb,OAAA,WACAjd,KAAA8O,OAAA9O,KAAA8N,MACA9N,KAAA8N,KAAA,GAGA9N,KAAA8O,MAAA9O,KAAA8N,KAGAzG,EAAAgV,WAAApa,UAAAib,OAAA,aACAld,KAAA8N,KAGAzG,EAAAgV,WAAApa,UAAAkb,eAAA,WAGA,IAFA,IAAA3J,EAAA4J,EAKA,IADAA,GADA5J,EAAAxT,KAAAiU,QACAoJ,WAAA,KACAD,EAAA,KAEA5J,GAAAnM,EAAAgV,WAAAU,KACA/c,KAAAkd,UAIA7V,EAAAgV,WAAApa,UAAAqb,KAAA,WACA,OAAAtd,KAAA8N,IAAA9N,KAAAL,QAGA0H,EAAAgV,WAAAU,IAAA,MACA1V,EAAAgV,WAAAkB,MAAA,QACAlW,EAAAgV,WAAAmB,KAAA,OACAnW,EAAAgV,WAAAoB,cAAA,gBACApW,EAAAgV,WAAAqB,MAAA,QACArW,EAAAgV,WAAAsB,SAAA,WAEAtW,EAAAgV,WAAAuB,SAAA,SAAAC,GAIA,OAHAA,EAAAX,SACAW,EAAAjB,KAAAvV,EAAAgV,WAAAkB,OACAM,EAAAZ,SACA5V,EAAAgV,WAAAI,SAGApV,EAAAgV,WAAAyB,QAAA,SAAAD,GAQA,GAPA,EAAAA,EAAAb,UACAa,EAAAX,SACAW,EAAAjB,KAAAvV,EAAAgV,WAAAmB,OAGAK,EAAAZ,SAEAY,EAAAP,OACA,OAAAjW,EAAAgV,WAAAI,SAIApV,EAAAgV,WAAA0B,gBAAA,SAAAF,GAIA,OAHAA,EAAAZ,SACAY,EAAAV,iBACAU,EAAAjB,KAAAvV,EAAAgV,WAAAoB,eACApW,EAAAgV,WAAAI,SAGApV,EAAAgV,WAAA2B,SAAA,SAAAH,GAIA,OAHAA,EAAAZ,SACAY,EAAAV,iBACAU,EAAAjB,KAAAvV,EAAAgV,WAAAqB,OACArW,EAAAgV,WAAAI,SAGApV,EAAAgV,WAAA4B,OAAA,SAAAJ,GACA,EAAAA,EAAAb,SACAa,EAAAjB,KAAAvV,EAAAgV,WAAAmB,OAeAnW,EAAAgV,WAAA6B,cAAA7W,EAAA8E,UAAAW,UAEAzF,EAAAgV,WAAAI,QAAA,SAAAoB,GACA,OAAA,CACA,IAAArK,EAAAqK,EAAA5J,OAEA,GAAAT,GAAAnM,EAAAgV,WAAAU,IACA,OAAA1V,EAAAgV,WAAA4B,OAIA,GAAA,IAAAzK,EAAA6J,WAAA,GAAA,CAKA,GAAA,KAAA7J,EACA,OAAAnM,EAAAgV,WAAAuB,SAGA,GAAA,KAAApK,EAKA,OAJAqK,EAAAX,SACA,EAAAW,EAAAb,SACAa,EAAAjB,KAAAvV,EAAAgV,WAAAmB,MAEAnW,EAAAgV,WAAA0B,gBAGA,GAAA,KAAAvK,EAKA,OAJAqK,EAAAX,SACA,EAAAW,EAAAb,SACAa,EAAAjB,KAAAvV,EAAAgV,WAAAmB,MAEAnW,EAAAgV,WAAA2B,SAMA,GAAA,KAAAxK,GAAA,IAAAqK,EAAAb,QAEA,OADAa,EAAAjB,KAAAvV,EAAAgV,WAAAsB,UACAtW,EAAAgV,WAAAI,QAMA,GAAA,KAAAjJ,GAAA,IAAAqK,EAAAb,QAEA,OADAa,EAAAjB,KAAAvV,EAAAgV,WAAAsB,UACAtW,EAAAgV,WAAAI,QAGA,GAAAjJ,EAAA3G,MAAAxF,EAAAgV,WAAA6B,eACA,OAAA7W,EAAAgV,WAAAyB,aAzCAD,EAAAf,oBA8CAzV,EAAA8O,YAAA,SAAApK,EAAAmK,GACAlW,KAAA6d,MAAA,IAAAxW,EAAAgV,WAAAtQ,GACA/L,KAAAkW,MAAAA,EACAlW,KAAAme,cAAA,GACAne,KAAAoe,UAAA,GAGA/W,EAAA8O,YAAAlU,UAAAmU,MAAA,WACApW,KAAA6d,MAAA7P,MACAhO,KAAAsc,QAAAtc,KAAA6d,MAAAvB,QAIA,IAFA,IAAAE,EAAAnV,EAAA8O,YAAAkI,YAEA7B,GACAA,EAAAA,EAAAxc,MAGA,OAAAA,KAAAkW,OAGA7O,EAAA8O,YAAAlU,UAAAqc,WAAA,WACA,OAAAte,KAAAsc,QAAAtc,KAAAoe,YAGA/W,EAAA8O,YAAAlU,UAAAsc,cAAA,WACA,IAAAC,EAAAxe,KAAAse,aAEA,OADAte,KAAAoe,WAAA,EACAI,GAGAnX,EAAA8O,YAAAlU,UAAAwc,WAAA,WACA,IAAAC,EAAA1e,KAAAme,cACAne,KAAAkW,MAAAlD,OAAA0L,GACA1e,KAAAme,cAAA,IAGA9W,EAAA8O,YAAAkI,YAAA,SAAAM,GACA,IAAAH,EAAAG,EAAAL,aAEA,GAAA7T,MAAA+T,EAIA,OAAAA,EAAA3B,MACA,KAAAxV,EAAAgV,WAAAsB,SACA,OAAAtW,EAAA8O,YAAAyI,cACA,KAAAvX,EAAAgV,WAAAkB,MACA,OAAAlW,EAAA8O,YAAA0I,WACA,KAAAxX,EAAAgV,WAAAmB,KACA,OAAAnW,EAAA8O,YAAA2I,UACA,QACA,IAAAC,EAAA,4CAAAP,EAAA3B,KAMA,MAJA,GAAA2B,EAAAzS,IAAApM,SACAof,GAAA,gBAAAP,EAAAzS,IAAA,KAGA,IAAA1E,EAAA8U,gBAAA4C,EAAAP,EAAA1P,MAAA0P,EAAAzP,OAIA1H,EAAA8O,YAAAyI,cAAA,SAAAD,GACA,IAAAH,EAAAG,EAAAJ,gBAEA,GAAA9T,MAAA+T,EAAA,CAIA,OAAAA,EAAAzS,KACA,IAAA,IACA4S,EAAAR,cAAAjH,SAAA7P,EAAAgP,MAAAa,SAAAU,WACA,MACA,IAAA,IACA+G,EAAAR,cAAAjH,SAAA7P,EAAAgP,MAAAa,SAAAC,SACA,MACA,QACA,IAAA4H,EAAA,kCAAAP,EAAAzS,IAAA,IACA,MAAA,IAAA1E,EAAA8U,gBAAA4C,EAAAP,EAAA1P,MAAA0P,EAAAzP,KAGA,IAAAiQ,EAAAL,EAAAL,aAEA,GAAA7T,MAAAuU,EAAA,CACAD,EAAA,yCACA,MAAA,IAAA1X,EAAA8U,gBAAA4C,EAAAP,EAAA1P,MAAA0P,EAAAzP,KAGA,OAAAiQ,EAAAnC,MACA,KAAAxV,EAAAgV,WAAAkB,MACA,OAAAlW,EAAA8O,YAAA0I,WACA,KAAAxX,EAAAgV,WAAAmB,KACA,OAAAnW,EAAA8O,YAAA2I,UACA,QACAC,EAAA,mCAAAC,EAAAnC,KAAA,IACA,MAAA,IAAAxV,EAAA8U,gBAAA4C,EAAAC,EAAAlQ,MAAAkQ,EAAAjQ,QAIA1H,EAAA8O,YAAA0I,WAAA,SAAAF,GACA,IAAAH,EAAAG,EAAAJ,gBAEA,GAAA9T,MAAA+T,EAAA,CAIA,IAAA,GAAAG,EAAAzI,MAAA0F,UAAArR,QAAAiU,EAAAzS,KAAA,CACA,IAAAkT,EAAAN,EAAAzI,MAAA0F,UAAAxP,IAAA,SAAA8S,GAAA,MAAA,IAAAA,EAAA,MAAA5b,KAAA,MACAyb,EAAA,uBAAAP,EAAAzS,IAAA,uBAAAkT,EAEA,MAAA,IAAA5X,EAAA8U,gBAAA4C,EAAAP,EAAA1P,MAAA0P,EAAAzP,KAGA4P,EAAAR,cAAApI,OAAA,CAAAyI,EAAAzS,KAEA,IAAAiT,EAAAL,EAAAL,aAEA,GAAA7T,MAAAuU,EAAA,CACAD,EAAA,gCACA,MAAA,IAAA1X,EAAA8U,gBAAA4C,EAAAP,EAAA1P,MAAA0P,EAAAzP,KAGA,OAAAiQ,EAAAnC,MACA,KAAAxV,EAAAgV,WAAAmB,KACA,OAAAnW,EAAA8O,YAAA2I,UACA,QACAC,EAAA,0BAAAC,EAAAnC,KAAA,IACA,MAAA,IAAAxV,EAAA8U,gBAAA4C,EAAAC,EAAAlQ,MAAAkQ,EAAAjQ,QAIA1H,EAAA8O,YAAA2I,UAAA,SAAAH,GACA,IAAAH,EAAAG,EAAAJ,gBAEA,GAAA9T,MAAA+T,EAAA,CAIAG,EAAAR,cAAAjL,KAAAsL,EAAAzS,IAAAjD,eAEA,GAAA0V,EAAAzS,IAAAxB,QAAA,OACAoU,EAAAR,cAAArH,aAAA,GAGA,IAAAkI,EAAAL,EAAAL,aAEA,GAAA7T,MAAAuU,EAKA,OAAAA,EAAAnC,MACA,KAAAxV,EAAAgV,WAAAmB,KAEA,OADAmB,EAAAF,aACApX,EAAA8O,YAAA2I,UACA,KAAAzX,EAAAgV,WAAAkB,MAEA,OADAoB,EAAAF,aACApX,EAAA8O,YAAA0I,WACA,KAAAxX,EAAAgV,WAAAoB,cACA,OAAApW,EAAA8O,YAAAgJ,kBACA,KAAA9X,EAAAgV,WAAAqB,MACA,OAAArW,EAAA8O,YAAAiJ,WACA,KAAA/X,EAAAgV,WAAAsB,SAEA,OADAgB,EAAAF,aACApX,EAAA8O,YAAAyI,cACA,QACA,IAAAG,EAAA,2BAAAC,EAAAnC,KAAA,IACA,MAAA,IAAAxV,EAAA8U,gBAAA4C,EAAAC,EAAAlQ,MAAAkQ,EAAAjQ,UApBA4P,EAAAF,eAwBApX,EAAA8O,YAAAgJ,kBAAA,SAAAR,GACA,IAAAH,EAAAG,EAAAJ,gBAEA,GAAA9T,MAAA+T,EAAA,CAIA,IAAArL,EAAAkM,SAAAb,EAAAzS,IAAA,IAEA,GAAAuT,MAAAnM,GAAA,CACA,IAAA4L,EAAA,gCACA,MAAA,IAAA1X,EAAA8U,gBAAA4C,EAAAP,EAAA1P,MAAA0P,EAAAzP,KAGA4P,EAAAR,cAAAhL,aAAAA,EAEA,IAAA6L,EAAAL,EAAAL,aAEA,GAAA7T,MAAAuU,EAKA,OAAAA,EAAAnC,MACA,KAAAxV,EAAAgV,WAAAmB,KAEA,OADAmB,EAAAF,aACApX,EAAA8O,YAAA2I,UACA,KAAAzX,EAAAgV,WAAAkB,MAEA,OADAoB,EAAAF,aACApX,EAAA8O,YAAA0I,WACA,KAAAxX,EAAAgV,WAAAoB,cACA,OAAApW,EAAA8O,YAAAgJ,kBACA,KAAA9X,EAAAgV,WAAAqB,MACA,OAAArW,EAAA8O,YAAAiJ,WACA,KAAA/X,EAAAgV,WAAAsB,SAEA,OADAgB,EAAAF,aACApX,EAAA8O,YAAAyI,cACA,QACAG,EAAA,2BAAAC,EAAAnC,KAAA,IACA,MAAA,IAAAxV,EAAA8U,gBAAA4C,EAAAC,EAAAlQ,MAAAkQ,EAAAjQ,UApBA4P,EAAAF,eAwBApX,EAAA8O,YAAAiJ,WAAA,SAAAT,GACA,IAAAH,EAAAG,EAAAJ,gBAEA,GAAA9T,MAAA+T,EAAA,CAIA,IAAA3G,EAAAwH,SAAAb,EAAAzS,IAAA,IAEA,GAAAuT,MAAAzH,GAAA,CACA,IAAAkH,EAAA,wBACA,MAAA,IAAA1X,EAAA8U,gBAAA4C,EAAAP,EAAA1P,MAAA0P,EAAAzP,KAGA4P,EAAAR,cAAAtG,MAAAA,EAEA,IAAAmH,EAAAL,EAAAL,aAEA,GAAA7T,MAAAuU,EAKA,OAAAA,EAAAnC,MACA,KAAAxV,EAAAgV,WAAAmB,KAEA,OADAmB,EAAAF,aACApX,EAAA8O,YAAA2I,UACA,KAAAzX,EAAAgV,WAAAkB,MAEA,OADAoB,EAAAF,aACApX,EAAA8O,YAAA0I,WACA,KAAAxX,EAAAgV,WAAAoB,cACA,OAAApW,EAAA8O,YAAAgJ,kBACA,KAAA9X,EAAAgV,WAAAqB,MACA,OAAArW,EAAA8O,YAAAiJ,WACA,KAAA/X,EAAAgV,WAAAsB,SAEA,OADAgB,EAAAF,aACApX,EAAA8O,YAAAyI,cACA,QACAG,EAAA,2BAAAC,EAAAnC,KAAA,IACA,MAAA,IAAAxV,EAAA8U,gBAAA4C,EAAAC,EAAAlQ,MAAAkQ,EAAAjQ,UApBA4P,EAAAF,eA4BAtX,EAeAnH,KAfAoH,EAeA,WAMA,OAAAC,GApBA,mBAAAkY,QAAAA,OAAAC,IAEAD,OAAAnY,GACA,iBAAAqY,QAMAC,OAAAD,QAAArY,IAGAD,EAAAE,KAAAD,IA34GA,GCNAjI,SAAAU,iBAAA,mBAAA,WACA,IAAA8f,EAAAxgB,SAAAF,cAAA,2BACA2gB,EAAAzgB,SAAAF,cAAA,sCACA4gB,EAAA1gB,SAAAF,cAAA,uCAEA,SAAA6gB,IACA,IAAAC,EAAA5gB,SAAAgC,eAAA,gBACA4e,EAAA1f,MAAA,GACA0f,EAAAC,cAAA,IAAAC,cAAA,YAGA,SAAAC,EAAAxe,GAEA,IAAAye,EAAAze,EAAA0e,SAAA1e,EAAA2e,MAIA,OAHA,KAGAF,GAFA,KAEAA,EAGA,SAAAG,IACA,IAAAhf,EAAAnC,SAAAF,cAAA,QACAsC,EAAApC,SAAAF,cAAA,qBACAshB,EAAAphB,SAAAF,cAAA,gCACAuhB,EAAArhB,SAAAF,cAAA,YACAshB,EAAAxhB,UAAAkC,SAAA,SACAsf,EAAAxhB,UAAAC,OAAA,QACAsC,EAAAvC,UAAAG,IAAA,QACAqC,EAAAxC,UAAAG,IAAA,QACAshB,EAAAzhB,UAAAG,IAAA,UAEAqhB,EAAAxhB,UAAAG,IAAA,QACAoC,EAAAvC,UAAAC,OAAA,QAGA,KAAA6C,OAAAC,WACAP,EAAAxC,UAAAC,OAAA,QAEAwhB,EAAAzhB,UAAAC,OAAA,SAIA,IAAAyhB,EAAAthB,SAAAF,cAAA,yBACAyhB,EAAAvhB,SAAAF,cAAA,iDACAwhB,EAAA1hB,UAAAkC,SAAA,SACAwf,EAAA1hB,UAAAC,OAAA,QACA0hB,EAAA3hB,UAAAG,IAAA,UAEAuhB,EAAA1hB,UAAAG,IAAA,QACAwhB,EAAA3hB,UAAAC,OAAA,SAKA2gB,EAAA9f,iBAAA,QAAAygB,GACAX,EAAA9f,iBAAA,WAAA,SAAA6B,GACAwe,EAAAxe,IACA4e,MAGA,iBAAAze,QACA8d,EAAA9f,iBAAA,eAAAygB,GAIAV,EAAA/f,iBAAA,QAAAygB,GACAV,EAAA/f,iBAAA,WAAA,SAAA6B,GACAwe,EAAAxe,IACA4e,MAGA,iBAAAze,QACA+d,EAAA/f,iBAAA,eAAAygB,GAIAT,EAAAhgB,iBAAA,QAAAigB,GACAD,EAAAhgB,iBAAA,WAAA,SAAA6B,GACAwe,EAAAxe,IACAoe,MAGA,iBAAAje,QACAge,EAAAhgB,iBAAA,eAAAigB,KAMAje,OAAA8e,WAAA,SAAAtZ,GACA,IAAA0Y,EAAA5gB,SAAAgC,eAAA,gBACAof,EAAAphB,SAAAwD,cAAA,OACAie,EAAAzhB,SAAAF,cAAA,SAKA,SAAA4hB,EAAA9G,EAAAzK,GACA,IAAAwR,EAAA,GACAhS,EAAAQ,EAAA,GACA3P,EAAA2P,EAAA,GAEApM,EAAA6W,EAAA7W,KACA6d,EAAA5hB,SAAAwD,cAAA,QACAoe,EAAAhiB,UAAAG,IAAA,2BACA6hB,EAAAne,UAAAM,EAAAsF,OAAAsG,EAAAnP,GAEA,IAAAoP,EAAAD,EAAAnP,EACAqhB,EAAA9d,EAAAvD,OAAA,EAEAshB,EAAAD,EAAAjS,EADA,GACAiS,EAAAjS,EADA,GAEAmS,EAAApS,EAFA,GAEA,EAAA,EAAAA,EAFA,GAgBA,OAbA,IAAAA,GAAAC,IAAAiS,EACAF,EAAAzV,KAAA0V,GACA,IAAAjS,GACAgS,EAAAzV,KAAA0V,GACAD,EAAAzV,KAAAlM,SAAAgiB,eAAAje,EAAAsF,OAAAuG,EAAAkS,MACAlS,IAAAiS,GACAF,EAAAzV,KAAAlM,SAAAgiB,eAAAje,EAAAsF,OAAA,EAAAsG,KACAgS,EAAAzV,KAAA0V,KAEAD,EAAAzV,KAAAlM,SAAAgiB,eAAA,MAAAje,EAAAsF,OAAA0Y,EAAApS,EAAAoS,KACAJ,EAAAzV,KAAA0V,GACAD,EAAAzV,KAAAlM,SAAAgiB,eAAAje,EAAAsF,OAAAuG,EAAAkS,EAAAlS,GAAA,SAEA+R,EAGA,SAAAM,EAAAC,EAAAtH,EAAAzK,GACA,IAMAgS,EANAR,EAAA,GACAhS,EAAAQ,EAAA,GACA3P,EAAA2P,EAAA,GAEAyR,EAAA5hB,SAAAwD,cAAA,QACAoe,EAAAhiB,UAAAG,IAAA,2BAGAoiB,EADAD,EACAtH,EAAAwH,OAAAC,OAAA,SAAA3gB,GACA,OAAAA,EAAA+D,KAAAyc,IACA,GAAAne,KAEA6W,EAAAuH,MAEAP,EAAAne,UAAA0e,EAAA9Y,OAAAsG,EAAAnP,GAEA,IAAAoP,EAAAD,EAAAnP,EACA8hB,EAAAH,EAAA3hB,OAAA,EAcA,OAbA,IAAAmP,GAAAC,IAAA0S,EACAX,EAAAzV,KAAA0V,GACA,IAAAjS,GACAgS,EAAAzV,KAAA0V,GACAD,EAAAzV,KAAAlM,SAAAgiB,eAAAG,EAAA9Y,OAAA7I,EAAA8hB,MACA1S,IAAA0S,GACAX,EAAAzV,KAAAlM,SAAAgiB,eAAAG,EAAA9Y,OAAA,EAAAsG,KACAgS,EAAAzV,KAAA0V,KAEAD,EAAAzV,KAAAlM,SAAAgiB,eAAAG,EAAA9Y,OAAA,EAAAsG,KACAgS,EAAAzV,KAAA0V,GACAD,EAAAzV,KAAAlM,SAAAgiB,eAAAG,EAAA9Y,OAAAuG,EAAA0S,MAEAX,EAsBA,SAAAY,EAAAtT,EAAAuT,EAAAC,GACAxT,EAAAhM,QAAA,SAAAvB,GACA,IACAwgB,EADAQ,EAAAhhB,EAAAgY,IAEAgJ,EAAAC,SAAA,OACAT,EAAAQ,EAAAE,UAAAF,EAAAtX,QAAA,KAAA,GACAsX,EAAAA,EAAAlZ,QAAA,IAAA0Y,EAAA,KAEA,IAAAtH,EAAA4H,EAAAE,GAEAf,EA7BA,SAAA9U,EAAAqV,EAAAtH,GACA,IAAA+G,EAAA,GACA,IAAA,IAAAvS,KAAAvC,EAAA,CACA,IAAA+J,EAAA/J,EAAAuC,GACA,IAAA,IAAA6I,KAAArB,EAAA,CACA,IAAAiM,EAAAjM,EAAAqB,GACA,GAAA4K,EAAA1S,SAAA,CACA,IAAAA,EAAA0S,EAAA1S,SAAA,GACA,UAAA8H,EACA0J,EAAAM,EAAAC,EAAAtH,EAAAzK,GACA,SAAA8H,IACA0J,EAAAD,EAAA9G,EAAAzK,MAKA,OAAAwR,EAaAmB,CADAphB,EAAA8X,UAAA3M,SACAqV,EAAAtH,GACA6H,EAAAzc,YAIA,SAAA4U,EAAAlZ,EAAAigB,GACA,IAAAoB,EAAA/iB,SAAAwD,cAAA,OACAuf,EAAAnjB,UAAAG,IAAA,gCACAgjB,EAAAtf,UAAAmX,EAAAuH,MACA,IAAAa,EAAAhjB,SAAAwD,cAAA,OACAwf,EAAApjB,UAAAG,IAAA,8BACA,IAAAkjB,EAAAjjB,SAAAwD,cAAA,KACAyf,EAAAC,KAAAxhB,EAAAgY,IACAsJ,EAAAhd,YAAAid,GACAtB,EAAA1e,QAAA,SAAAkgB,GACAF,EAAAjd,YAAAmd,KAEA,IAAAC,EAAApjB,SAAAwD,cAAA,OAIA,OAHA4f,EAAAxjB,UAAAG,IAAA,sBACAqjB,EAAApd,YAAA+c,GACAK,EAAApd,YAAAgd,GACAI,EApBAC,CAAAzI,EAAAlZ,EAAAigB,MAmDA,SAAA2B,EAAA5T,EAAA8S,EAAAze,GAEA,KAAAqd,EAAAmC,YACAnC,EAAAoC,YAAApC,EAAAmC,YAEA,GAAA,KAAAxf,EAAAoJ,OAAA,CAGA,IAxBAuC,EAAA3L,EAEAkL,EAsBAA,GAxBAlL,EAwBAA,EArBA,GADAkL,GAFAS,EAwBAA,GAtBAmH,OAAA9S,IACAvD,QAKA,GADAyO,EAAAS,EAAAmH,OAAA9S,EAAA,MACAvD,OAJAyO,EAQAA,EAAAS,EAAAmH,OAAA,IAAA9S,EAAA,MAaA0e,EAAAziB,SAAAwD,cAAA,OACAif,EAAA7iB,UAAAG,IAAA,yBACAqhB,EAAApb,YAAAyc,GACA,EAAAxT,EAAAzO,OACA+hB,EAAAtT,EAAAuT,EAAAC,GAEAA,EAAAzc,YA3CA,SAAAjC,GACA,IAAAqf,EAAApjB,SAAAwD,cAAA,OACA4f,EAAAxjB,UAAAG,IAAA,sBACA,IAAAijB,EAAAhjB,SAAAwD,cAAA,OACAwf,EAAApjB,UAAAG,IAAA,8BACA,IAAAgK,EAAA/J,SAAAwD,cAAA,UAIA,OAHAuG,EAAAtG,UAAA,+BAAAM,EAAA,IACAif,EAAAhd,YAAA+D,GACAqZ,EAAApd,YAAAgd,GACAI,EAkCAK,CAAA1f,KA6BA,OAnMAqd,EAAAxhB,UAAAG,IAAA,+BACAqhB,EAAAxhB,UAAAG,IAAA,QACA0hB,EAAA7d,aAAAwd,EAAAK,EAAA8B,YAiMA,CACAG,KAVA,SAAAC,GACA,IAjBAC,EAAAC,EAAAC,EACAC,EAgBArU,EAAArF,OAAA2Z,OAAA,CAAAtU,MAAAxH,EAAAqO,MAAApI,KAAAwV,EAAAjU,OAAA8S,MAAAmB,EAAAnB,QACA3L,GAlBA+M,EAkBA,WACAN,EAAA5T,EAAAA,MAAAA,EAAA8S,MAAA5B,EAAA1f,QAnBA2iB,EAoBA,IAlBA,WACA,IAAAI,EAAApjB,KACAsb,EAAA5N,UAKA2V,EAAAJ,IAAAC,EACAI,aAAAJ,GACAA,EAAAhe,WANA,WACAge,EAAA,KACAD,GAAAF,EAAAvH,MAAA4H,EAAA9H,IAIA0H,GACAK,GAAAN,EAAAvH,MAAA4H,EAAA9H,KAUAyE,EAAAlgB,iBAAA,UAAAmW,KApMA,CA0MAnU,OAAAwF","file":"site.js","sourcesContent":[";(function () {\n  'use strict'\n\n  // expand current page nav \n  var currentPageItem = document.querySelector('.is-current-page').parentNode\n  expandParents(currentPageItem)\n  function expandParents(element) {\n    var panel = element.parentNode\n    if(!panel.matches(\".nav-children-panel\")){\n        return \n    }\n    var parentHeader = panel.previousElementSibling\n    panel.classList.remove('hide')\n    parentHeader.querySelector('.material-icons').classList.add('expanded')\n    expandParents(parentHeader)\n  }  \n\n  //header \n  mdc.topAppBar.MDCTopAppBar.attachTo(document.querySelector('.mdc-top-app-bar'))\n\n  // nav \n  /// version select box\n  var select = document.querySelectorAll('.select-version')\n  for(var i = 0; i < select.length; i++){\n    var s = select[i]\n    s.addEventListener('change', function(event){\n      var component = this.getAttribute('data-component')\n      var version = this.options[this.selectedIndex].value\n      var showSelector = '.nav-container div[data-component=\"' + component + '\"][data-version=\"' + version + '\"]'\n      var hideSelector = '.nav-container div[data-component=\"' + component + '\"]:not(.hide)'\n      var navShow = document.querySelector(showSelector)\n      var navHide = document.querySelector(hideSelector)\n      navShow.classList.remove('hide')\n      navHide.classList.add('hide')\n    })\n\n    // Disable select if there is only one version\n    if (s.options.length === 1) {\n      s.classList.add('single-version');\n      s.disabled = true;\n    }\n  }\n\n  /// nav-tree\n  var x = document.querySelectorAll('.nav-item .material-icons'); \n  for(var i = 0; i < x.length; i++){\n    mdc.ripple.MDCRipple.attachTo(x[i])\n    x[i].addEventListener('click', function(event){\n      var item = event.target\n      var panel = item.parentElement.nextElementSibling\n      if(item.classList.contains('expanded')){\n        item.classList.remove('expanded')\n        panel.classList.add('hide')\n      } else{\n        item.classList.add('expanded')\n        panel.classList.remove('hide')\n      }\n    })\n  }\n\n  // toolbar menu button \n  var navToggle = document.getElementById('toolbar-nav-toggle')\n  mdc.iconButton.MDCIconButtonToggle.attachTo(navToggle)\n  navToggle.addEventListener('click', function(){\n    // Toggle navigation and main content together\n    var mainContainer = document.querySelector('main')\n    var navContainer = document.querySelector('div.nav-container')\n    if(navContainer.classList.contains('hide')){\n      mainContainer.classList.add('hide')\n      navContainer.classList.remove('hide')\n\n    } else{\n      mainContainer.classList.remove('hide')\n      navContainer.classList.add('hide')\n    }\n  })\n\n})()\n",";(function () {\n  'use strict'\n\n  var toggle = document.querySelector('.page-versions .version-menu-toggle')\n  if (!toggle) return\n\n  var selector = document.querySelector('.page-versions')\n\n  toggle.addEventListener('click', function (e) {\n    selector.classList.toggle('is-active')\n    // don't let this event get smothered\n    e.stopPropagation()\n  })\n\n  document.documentElement.addEventListener('click', function () {\n    selector.classList.remove('is-active')\n  })\n})()\n","document.addEventListener('DOMContentLoaded', function () {\n  // Hide navbar on mobile\n  if (window.innerWidth <= 1024) {\n    var navContainer = document.querySelector('div.nav-container')\n    navContainer.classList.add('hide')\n  }\n\n  // Add event listeners to hamburger icons\n  var navbarToggles = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0)\n  if (navbarToggles.length === 0) return\n  navbarToggles.forEach(function (el) {\n    el.addEventListener('click', function (e) {\n      e.stopPropagation()\n      el.classList.toggle('is-active')\n      document.getElementById(el.dataset.target).classList.toggle('is-active')\n      document.documentElement.classList.toggle('is-clipped--navbar')\n    })\n  })\n})\n\n\nwindow.addEventListener('resize', function () {\n  // Only expand/unhide elements on resize if search isn't open\n  var searchTopBar = document.querySelector('.mdc-top-app-bar__row.sdp-top-app-bar__search')\n  if (searchTopBar.classList.contains('hide')) {\n    // Unhide main content\n    var mainContainer = document.querySelector('main')\n    if (mainContainer.classList.contains('hide')) {\n      mainContainer.classList.remove('hide')\n    }\n\n    // Expand navbar if window is resized from mobile to desktop\n    var navContainer = document.querySelector('div.nav-container')\n    if (window.innerWidth > 1024) {\n      if (navContainer.classList.contains('hide')) {\n        navContainer.classList.remove('hide')\n      }\n    }\n  }\n\n  // Hide navbar if window is resized from desktop to mobile\n  if (window.innerWidth < 1025) {\n    if (!navContainer.classList.contains('hide')) {\n      navContainer.classList.add('hide')\n    }\n  }\n});\n",";(function () {\n  \n  hljs.initHighlighting()\n\n})()\n  ",";(function () {\n    'use strict'\n    // <i class=\"material-icons mdc-icon-button search\" tabindex=\"0\" role=\"button\">search</i>\n    var codeBlocks = document.querySelectorAll('.doc .listingblock code')\n    var copyIcon = document.createElement('i')\n    copyIcon.classList = 'material-icons codeCopyButton'\n    copyIcon.innerText = 'file_copy'\n    for (var i = 0; i < codeBlocks.length; i++) {\n      var icon = copyIcon.cloneNode(true)\n      codeBlocks[i].insertBefore(icon, codeBlocks[i].childNodes[0])\n    }\n \n    /*global ClipboardJS*/\n    var clipboard = new ClipboardJS('.material-icons.codeCopyButton', {\n      text: function (target) { \n        var lines = target.parentNode.innerText.split(\"\\n\")\n        lines.splice(0,1)\n        return lines.join(\"\\n\") \n      },\n    })\n  \n    clipboard.on('success', function (e) {\n      e.trigger._tippy.setContent('copied!')\n    })\n  \n    tippy.delegate('.doc .listingblock code', {\n      target: '.material-icons.codeCopyButton',\n      content: 'copy to clipboard',\n      animation: 'shift-away',\n      theme: 'clipboard',\n      delay: [500, 0],\n      placement: 'bottom',\n      hideOnClick: false,\n      onHidden: function (instance) {\n        instance.setContent('copy to clipboard')\n      },\n    })\n  })()\n  ",";(function () {\n  'use strict'\n\n  // create table of contents \n  var headers = document.querySelector('.doc .contents').querySelectorAll('h1, h2, h3, h4, h5, h6')\n  var toc = document.getElementById('toc')\n  for(var i = 0; i < headers.length; i++){\n      var header = headers[i]        \n      var li = document.createElement('li')\n      li.classList.add('toc-item')\n      li.classList.add(header.tagName)\n      li.innerText = header.innerText \n      li.setAttribute('headerId', header.getAttribute('id'))\n      li.addEventListener('click', function() {\n          var h, id = this.getAttribute('headerId')\n          if(id != 'null'){ h = document.getElementById(id) } \n          else { h = document.querySelector('h1') }\n          window.scroll({\n              top: h.offsetTop + 30, // 30 to account for fixed header height \n              left: 0,\n              behavior: 'smooth'\n          })\n          h.classList.add('toc-highlight')\n          setTimeout(function(){ h.classList.remove('toc-highlight') }, 2000)\n      })\n      toc.appendChild(li)\n  }\n\n  // enable highlighted toc\n  var referenceElement = document.querySelector('article').parentNode\n  window.addEventListener('scroll', function () {\n    var activeHeader, lastActiveHeader = document.querySelector('.toc-item.active')\n    var referencePoint = referenceElement.offsetTop + 108.1 \n    for (var i = (headers.length - 1); i >= 0; i--) {\n      var hTop = headers[i].getBoundingClientRect().top\n      if (hTop < referencePoint) {\n        var activeId = headers[i].getAttribute('id')\n        activeHeader = document.querySelector('.toc-item[headerId=\"' + activeId + '\"]')\n        break\n      }\n    }\n\n    // there is currently an active header and it has changed\n    if(activeHeader && activeHeader !== lastActiveHeader){\n      activeHeader.classList.add('active')\n      if(lastActiveHeader){\n        lastActiveHeader.classList.remove('active')\n      }\n    }\n  })\n\n})()\n  ","/**\n * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.3\n * Copyright (C) 2018 Oliver Nightingale\n * @license MIT\n */\n\n;(function(){\n\n  /**\n   * A convenience function for configuring and constructing\n   * a new lunr Index.\n   *\n   * A lunr.Builder instance is created and the pipeline setup\n   * with a trimmer, stop word filter and stemmer.\n   *\n   * This builder object is yielded to the configuration function\n   * that is passed as a parameter, allowing the list of fields\n   * and other builder parameters to be customised.\n   *\n   * All documents _must_ be added within the passed config function.\n   *\n   * @example\n   * var idx = lunr(function () {\n   *   this.field('title')\n   *   this.field('body')\n   *   this.ref('id')\n   *\n   *   documents.forEach(function (doc) {\n   *     this.add(doc)\n   *   }, this)\n   * })\n   *\n   * @see {@link lunr.Builder}\n   * @see {@link lunr.Pipeline}\n   * @see {@link lunr.trimmer}\n   * @see {@link lunr.stopWordFilter}\n   * @see {@link lunr.stemmer}\n   * @namespace {function} lunr\n   */\n  var lunr = function (config) {\n    var builder = new lunr.Builder\n  \n    builder.pipeline.add(\n      lunr.trimmer,\n      lunr.stopWordFilter,\n      lunr.stemmer\n    )\n  \n    builder.searchPipeline.add(\n      lunr.stemmer\n    )\n  \n    config.call(builder, builder)\n    return builder.build()\n  }\n  \n  lunr.version = \"2.3.3\"\n  /*!\n   * lunr.utils\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * A namespace containing utils for the rest of the lunr library\n   * @namespace lunr.utils\n   */\n  lunr.utils = {}\n  \n  /**\n   * Print a warning message to the console.\n   *\n   * @param {String} message The message to be printed.\n   * @memberOf lunr.utils\n   * @function\n   */\n  lunr.utils.warn = (function (global) {\n    /* eslint-disable no-console */\n    return function (message) {\n      if (global.console && console.warn) {\n        console.warn(message)\n      }\n    }\n    /* eslint-enable no-console */\n  })(this)\n  \n  /**\n   * Convert an object to a string.\n   *\n   * In the case of `null` and `undefined` the function returns\n   * the empty string, in all other cases the result of calling\n   * `toString` on the passed object is returned.\n   *\n   * @param {Any} obj The object to convert to a string.\n   * @return {String} string representation of the passed object.\n   * @memberOf lunr.utils\n   */\n  lunr.utils.asString = function (obj) {\n    if (obj === void 0 || obj === null) {\n      return \"\"\n    } else {\n      return obj.toString()\n    }\n  }\n  \n  /**\n   * Clones an object.\n   *\n   * Will create a copy of an existing object such that any mutations\n   * on the copy cannot affect the original.\n   *\n   * Only shallow objects are supported, passing a nested object to this\n   * function will cause a TypeError.\n   *\n   * Objects with primitives, and arrays of primitives are supported.\n   *\n   * @param {Object} obj The object to clone.\n   * @return {Object} a clone of the passed object.\n   * @throws {TypeError} when a nested object is passed.\n   * @memberOf Utils\n   */\n  lunr.utils.clone = function (obj) {\n    if (obj === null || obj === undefined) {\n      return obj\n    }\n  \n    var clone = Object.create(null),\n        keys = Object.keys(obj)\n  \n    for (var i = 0; i < keys.length; i++) {\n      var key = keys[i],\n          val = obj[key]\n  \n      if (Array.isArray(val)) {\n        clone[key] = val.slice()\n        continue\n      }\n  \n      if (typeof val === 'string' ||\n          typeof val === 'number' ||\n          typeof val === 'boolean') {\n        clone[key] = val\n        continue\n      }\n  \n      throw new TypeError(\"clone is not deep and does not support nested objects\")\n    }\n  \n    return clone\n  }\n  lunr.FieldRef = function (docRef, fieldName, stringValue) {\n    this.docRef = docRef\n    this.fieldName = fieldName\n    this._stringValue = stringValue\n  }\n  \n  lunr.FieldRef.joiner = \"/\"\n  \n  lunr.FieldRef.fromString = function (s) {\n    var n = s.indexOf(lunr.FieldRef.joiner)\n  \n    if (n === -1) {\n      throw \"malformed field ref string\"\n    }\n  \n    var fieldRef = s.slice(0, n),\n        docRef = s.slice(n + 1)\n  \n    return new lunr.FieldRef (docRef, fieldRef, s)\n  }\n  \n  lunr.FieldRef.prototype.toString = function () {\n    if (this._stringValue == undefined) {\n      this._stringValue = this.fieldName + lunr.FieldRef.joiner + this.docRef\n    }\n  \n    return this._stringValue\n  }\n  /*!\n   * lunr.Set\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * A lunr set.\n   *\n   * @constructor\n   */\n  lunr.Set = function (elements) {\n    this.elements = Object.create(null)\n  \n    if (elements) {\n      this.length = elements.length\n  \n      for (var i = 0; i < this.length; i++) {\n        this.elements[elements[i]] = true\n      }\n    } else {\n      this.length = 0\n    }\n  }\n  \n  /**\n   * A complete set that contains all elements.\n   *\n   * @static\n   * @readonly\n   * @type {lunr.Set}\n   */\n  lunr.Set.complete = {\n    intersect: function (other) {\n      return other\n    },\n  \n    union: function (other) {\n      return other\n    },\n  \n    contains: function () {\n      return true\n    }\n  }\n  \n  /**\n   * An empty set that contains no elements.\n   *\n   * @static\n   * @readonly\n   * @type {lunr.Set}\n   */\n  lunr.Set.empty = {\n    intersect: function () {\n      return this\n    },\n  \n    union: function (other) {\n      return other\n    },\n  \n    contains: function () {\n      return false\n    }\n  }\n  \n  /**\n   * Returns true if this set contains the specified object.\n   *\n   * @param {object} object - Object whose presence in this set is to be tested.\n   * @returns {boolean} - True if this set contains the specified object.\n   */\n  lunr.Set.prototype.contains = function (object) {\n    return !!this.elements[object]\n  }\n  \n  /**\n   * Returns a new set containing only the elements that are present in both\n   * this set and the specified set.\n   *\n   * @param {lunr.Set} other - set to intersect with this set.\n   * @returns {lunr.Set} a new set that is the intersection of this and the specified set.\n   */\n  \n  lunr.Set.prototype.intersect = function (other) {\n    var a, b, elements, intersection = []\n  \n    if (other === lunr.Set.complete) {\n      return this\n    }\n  \n    if (other === lunr.Set.empty) {\n      return other\n    }\n  \n    if (this.length < other.length) {\n      a = this\n      b = other\n    } else {\n      a = other\n      b = this\n    }\n  \n    elements = Object.keys(a.elements)\n  \n    for (var i = 0; i < elements.length; i++) {\n      var element = elements[i]\n      if (element in b.elements) {\n        intersection.push(element)\n      }\n    }\n  \n    return new lunr.Set (intersection)\n  }\n  \n  /**\n   * Returns a new set combining the elements of this and the specified set.\n   *\n   * @param {lunr.Set} other - set to union with this set.\n   * @return {lunr.Set} a new set that is the union of this and the specified set.\n   */\n  \n  lunr.Set.prototype.union = function (other) {\n    if (other === lunr.Set.complete) {\n      return lunr.Set.complete\n    }\n  \n    if (other === lunr.Set.empty) {\n      return this\n    }\n  \n    return new lunr.Set(Object.keys(this.elements).concat(Object.keys(other.elements)))\n  }\n  /**\n   * A function to calculate the inverse document frequency for\n   * a posting. This is shared between the builder and the index\n   *\n   * @private\n   * @param {object} posting - The posting for a given term\n   * @param {number} documentCount - The total number of documents.\n   */\n  lunr.idf = function (posting, documentCount) {\n    var documentsWithTerm = 0\n  \n    for (var fieldName in posting) {\n      if (fieldName == '_index') continue // Ignore the term index, its not a field\n      documentsWithTerm += Object.keys(posting[fieldName]).length\n    }\n  \n    var x = (documentCount - documentsWithTerm + 0.5) / (documentsWithTerm + 0.5)\n  \n    return Math.log(1 + Math.abs(x))\n  }\n  \n  /**\n   * A token wraps a string representation of a token\n   * as it is passed through the text processing pipeline.\n   *\n   * @constructor\n   * @param {string} [str=''] - The string token being wrapped.\n   * @param {object} [metadata={}] - Metadata associated with this token.\n   */\n  lunr.Token = function (str, metadata) {\n    this.str = str || \"\"\n    this.metadata = metadata || {}\n  }\n  \n  /**\n   * Returns the token string that is being wrapped by this object.\n   *\n   * @returns {string}\n   */\n  lunr.Token.prototype.toString = function () {\n    return this.str\n  }\n  \n  /**\n   * A token update function is used when updating or optionally\n   * when cloning a token.\n   *\n   * @callback lunr.Token~updateFunction\n   * @param {string} str - The string representation of the token.\n   * @param {Object} metadata - All metadata associated with this token.\n   */\n  \n  /**\n   * Applies the given function to the wrapped string token.\n   *\n   * @example\n   * token.update(function (str, metadata) {\n   *   return str.toUpperCase()\n   * })\n   *\n   * @param {lunr.Token~updateFunction} fn - A function to apply to the token string.\n   * @returns {lunr.Token}\n   */\n  lunr.Token.prototype.update = function (fn) {\n    this.str = fn(this.str, this.metadata)\n    return this\n  }\n  \n  /**\n   * Creates a clone of this token. Optionally a function can be\n   * applied to the cloned token.\n   *\n   * @param {lunr.Token~updateFunction} [fn] - An optional function to apply to the cloned token.\n   * @returns {lunr.Token}\n   */\n  lunr.Token.prototype.clone = function (fn) {\n    fn = fn || function (s) { return s }\n    return new lunr.Token (fn(this.str, this.metadata), this.metadata)\n  }\n  /*!\n   * lunr.tokenizer\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * A function for splitting a string into tokens ready to be inserted into\n   * the search index. Uses `lunr.tokenizer.separator` to split strings, change\n   * the value of this property to change how strings are split into tokens.\n   *\n   * This tokenizer will convert its parameter to a string by calling `toString` and\n   * then will split this string on the character in `lunr.tokenizer.separator`.\n   * Arrays will have their elements converted to strings and wrapped in a lunr.Token.\n   *\n   * Optional metadata can be passed to the tokenizer, this metadata will be cloned and\n   * added as metadata to every token that is created from the object to be tokenized.\n   *\n   * @static\n   * @param {?(string|object|object[])} obj - The object to convert into tokens\n   * @param {?object} metadata - Optional metadata to associate with every token\n   * @returns {lunr.Token[]}\n   * @see {@link lunr.Pipeline}\n   */\n  lunr.tokenizer = function (obj, metadata) {\n    if (obj == null || obj == undefined) {\n      return []\n    }\n  \n    if (Array.isArray(obj)) {\n      return obj.map(function (t) {\n        return new lunr.Token(\n          lunr.utils.asString(t).toLowerCase(),\n          lunr.utils.clone(metadata)\n        )\n      })\n    }\n  \n    var str = obj.toString().trim().toLowerCase(),\n        len = str.length,\n        tokens = []\n  \n    for (var sliceEnd = 0, sliceStart = 0; sliceEnd <= len; sliceEnd++) {\n      var char = str.charAt(sliceEnd),\n          sliceLength = sliceEnd - sliceStart\n  \n      if ((char.match(lunr.tokenizer.separator) || sliceEnd == len)) {\n  \n        if (sliceLength > 0) {\n          var tokenMetadata = lunr.utils.clone(metadata) || {}\n          tokenMetadata[\"position\"] = [sliceStart, sliceLength]\n          tokenMetadata[\"index\"] = tokens.length\n  \n          tokens.push(\n            new lunr.Token (\n              str.slice(sliceStart, sliceEnd),\n              tokenMetadata\n            )\n          )\n        }\n  \n        sliceStart = sliceEnd + 1\n      }\n  \n    }\n  \n    return tokens\n  }\n  \n  /**\n   * The separator used to split a string into tokens. Override this property to change the behaviour of\n   * `lunr.tokenizer` behaviour when tokenizing strings. By default this splits on whitespace and hyphens.\n   *\n   * @static\n   * @see lunr.tokenizer\n   */\n  lunr.tokenizer.separator = /[\\s\\-]+/\n  /*!\n   * lunr.Pipeline\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * lunr.Pipelines maintain an ordered list of functions to be applied to all\n   * tokens in documents entering the search index and queries being ran against\n   * the index.\n   *\n   * An instance of lunr.Index created with the lunr shortcut will contain a\n   * pipeline with a stop word filter and an English language stemmer. Extra\n   * functions can be added before or after either of these functions or these\n   * default functions can be removed.\n   *\n   * When run the pipeline will call each function in turn, passing a token, the\n   * index of that token in the original list of all tokens and finally a list of\n   * all the original tokens.\n   *\n   * The output of functions in the pipeline will be passed to the next function\n   * in the pipeline. To exclude a token from entering the index the function\n   * should return undefined, the rest of the pipeline will not be called with\n   * this token.\n   *\n   * For serialisation of pipelines to work, all functions used in an instance of\n   * a pipeline should be registered with lunr.Pipeline. Registered functions can\n   * then be loaded. If trying to load a serialised pipeline that uses functions\n   * that are not registered an error will be thrown.\n   *\n   * If not planning on serialising the pipeline then registering pipeline functions\n   * is not necessary.\n   *\n   * @constructor\n   */\n  lunr.Pipeline = function () {\n    this._stack = []\n  }\n  \n  lunr.Pipeline.registeredFunctions = Object.create(null)\n  \n  /**\n   * A pipeline function maps lunr.Token to lunr.Token. A lunr.Token contains the token\n   * string as well as all known metadata. A pipeline function can mutate the token string\n   * or mutate (or add) metadata for a given token.\n   *\n   * A pipeline function can indicate that the passed token should be discarded by returning\n   * null. This token will not be passed to any downstream pipeline functions and will not be\n   * added to the index.\n   *\n   * Multiple tokens can be returned by returning an array of tokens. Each token will be passed\n   * to any downstream pipeline functions and all will returned tokens will be added to the index.\n   *\n   * Any number of pipeline functions may be chained together using a lunr.Pipeline.\n   *\n   * @interface lunr.PipelineFunction\n   * @param {lunr.Token} token - A token from the document being processed.\n   * @param {number} i - The index of this token in the complete list of tokens for this document/field.\n   * @param {lunr.Token[]} tokens - All tokens for this document/field.\n   * @returns {(?lunr.Token|lunr.Token[])}\n   */\n  \n  /**\n   * Register a function with the pipeline.\n   *\n   * Functions that are used in the pipeline should be registered if the pipeline\n   * needs to be serialised, or a serialised pipeline needs to be loaded.\n   *\n   * Registering a function does not add it to a pipeline, functions must still be\n   * added to instances of the pipeline for them to be used when running a pipeline.\n   *\n   * @param {lunr.PipelineFunction} fn - The function to check for.\n   * @param {String} label - The label to register this function with\n   */\n  lunr.Pipeline.registerFunction = function (fn, label) {\n    if (label in this.registeredFunctions) {\n      lunr.utils.warn('Overwriting existing registered function: ' + label)\n    }\n  \n    fn.label = label\n    lunr.Pipeline.registeredFunctions[fn.label] = fn\n  }\n  \n  /**\n   * Warns if the function is not registered as a Pipeline function.\n   *\n   * @param {lunr.PipelineFunction} fn - The function to check for.\n   * @private\n   */\n  lunr.Pipeline.warnIfFunctionNotRegistered = function (fn) {\n    var isRegistered = fn.label && (fn.label in this.registeredFunctions)\n  \n    if (!isRegistered) {\n      lunr.utils.warn('Function is not registered with pipeline. This may cause problems when serialising the index.\\n', fn)\n    }\n  }\n  \n  /**\n   * Loads a previously serialised pipeline.\n   *\n   * All functions to be loaded must already be registered with lunr.Pipeline.\n   * If any function from the serialised data has not been registered then an\n   * error will be thrown.\n   *\n   * @param {Object} serialised - The serialised pipeline to load.\n   * @returns {lunr.Pipeline}\n   */\n  lunr.Pipeline.load = function (serialised) {\n    var pipeline = new lunr.Pipeline\n  \n    serialised.forEach(function (fnName) {\n      var fn = lunr.Pipeline.registeredFunctions[fnName]\n  \n      if (fn) {\n        pipeline.add(fn)\n      } else {\n        throw new Error('Cannot load unregistered function: ' + fnName)\n      }\n    })\n  \n    return pipeline\n  }\n  \n  /**\n   * Adds new functions to the end of the pipeline.\n   *\n   * Logs a warning if the function has not been registered.\n   *\n   * @param {lunr.PipelineFunction[]} functions - Any number of functions to add to the pipeline.\n   */\n  lunr.Pipeline.prototype.add = function () {\n    var fns = Array.prototype.slice.call(arguments)\n  \n    fns.forEach(function (fn) {\n      lunr.Pipeline.warnIfFunctionNotRegistered(fn)\n      this._stack.push(fn)\n    }, this)\n  }\n  \n  /**\n   * Adds a single function after a function that already exists in the\n   * pipeline.\n   *\n   * Logs a warning if the function has not been registered.\n   *\n   * @param {lunr.PipelineFunction} existingFn - A function that already exists in the pipeline.\n   * @param {lunr.PipelineFunction} newFn - The new function to add to the pipeline.\n   */\n  lunr.Pipeline.prototype.after = function (existingFn, newFn) {\n    lunr.Pipeline.warnIfFunctionNotRegistered(newFn)\n  \n    var pos = this._stack.indexOf(existingFn)\n    if (pos == -1) {\n      throw new Error('Cannot find existingFn')\n    }\n  \n    pos = pos + 1\n    this._stack.splice(pos, 0, newFn)\n  }\n  \n  /**\n   * Adds a single function before a function that already exists in the\n   * pipeline.\n   *\n   * Logs a warning if the function has not been registered.\n   *\n   * @param {lunr.PipelineFunction} existingFn - A function that already exists in the pipeline.\n   * @param {lunr.PipelineFunction} newFn - The new function to add to the pipeline.\n   */\n  lunr.Pipeline.prototype.before = function (existingFn, newFn) {\n    lunr.Pipeline.warnIfFunctionNotRegistered(newFn)\n  \n    var pos = this._stack.indexOf(existingFn)\n    if (pos == -1) {\n      throw new Error('Cannot find existingFn')\n    }\n  \n    this._stack.splice(pos, 0, newFn)\n  }\n  \n  /**\n   * Removes a function from the pipeline.\n   *\n   * @param {lunr.PipelineFunction} fn The function to remove from the pipeline.\n   */\n  lunr.Pipeline.prototype.remove = function (fn) {\n    var pos = this._stack.indexOf(fn)\n    if (pos == -1) {\n      return\n    }\n  \n    this._stack.splice(pos, 1)\n  }\n  \n  /**\n   * Runs the current list of functions that make up the pipeline against the\n   * passed tokens.\n   *\n   * @param {Array} tokens The tokens to run through the pipeline.\n   * @returns {Array}\n   */\n  lunr.Pipeline.prototype.run = function (tokens) {\n    var stackLength = this._stack.length\n  \n    for (var i = 0; i < stackLength; i++) {\n      var fn = this._stack[i]\n      var memo = []\n  \n      for (var j = 0; j < tokens.length; j++) {\n        var result = fn(tokens[j], j, tokens)\n  \n        if (result === void 0 || result === '') continue\n  \n        if (result instanceof Array) {\n          for (var k = 0; k < result.length; k++) {\n            memo.push(result[k])\n          }\n        } else {\n          memo.push(result)\n        }\n      }\n  \n      tokens = memo\n    }\n  \n    return tokens\n  }\n  \n  /**\n   * Convenience method for passing a string through a pipeline and getting\n   * strings out. This method takes care of wrapping the passed string in a\n   * token and mapping the resulting tokens back to strings.\n   *\n   * @param {string} str - The string to pass through the pipeline.\n   * @param {?object} metadata - Optional metadata to associate with the token\n   * passed to the pipeline.\n   * @returns {string[]}\n   */\n  lunr.Pipeline.prototype.runString = function (str, metadata) {\n    var token = new lunr.Token (str, metadata)\n  \n    return this.run([token]).map(function (t) {\n      return t.toString()\n    })\n  }\n  \n  /**\n   * Resets the pipeline by removing any existing processors.\n   *\n   */\n  lunr.Pipeline.prototype.reset = function () {\n    this._stack = []\n  }\n  \n  /**\n   * Returns a representation of the pipeline ready for serialisation.\n   *\n   * Logs a warning if the function has not been registered.\n   *\n   * @returns {Array}\n   */\n  lunr.Pipeline.prototype.toJSON = function () {\n    return this._stack.map(function (fn) {\n      lunr.Pipeline.warnIfFunctionNotRegistered(fn)\n  \n      return fn.label\n    })\n  }\n  /*!\n   * lunr.Vector\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * A vector is used to construct the vector space of documents and queries. These\n   * vectors support operations to determine the similarity between two documents or\n   * a document and a query.\n   *\n   * Normally no parameters are required for initializing a vector, but in the case of\n   * loading a previously dumped vector the raw elements can be provided to the constructor.\n   *\n   * For performance reasons vectors are implemented with a flat array, where an elements\n   * index is immediately followed by its value. E.g. [index, value, index, value]. This\n   * allows the underlying array to be as sparse as possible and still offer decent\n   * performance when being used for vector calculations.\n   *\n   * @constructor\n   * @param {Number[]} [elements] - The flat list of element index and element value pairs.\n   */\n  lunr.Vector = function (elements) {\n    this._magnitude = 0\n    this.elements = elements || []\n  }\n  \n  \n  /**\n   * Calculates the position within the vector to insert a given index.\n   *\n   * This is used internally by insert and upsert. If there are duplicate indexes then\n   * the position is returned as if the value for that index were to be updated, but it\n   * is the callers responsibility to check whether there is a duplicate at that index\n   *\n   * @param {Number} insertIdx - The index at which the element should be inserted.\n   * @returns {Number}\n   */\n  lunr.Vector.prototype.positionForIndex = function (index) {\n    // For an empty vector the tuple can be inserted at the beginning\n    if (this.elements.length == 0) {\n      return 0\n    }\n  \n    var start = 0,\n        end = this.elements.length / 2,\n        sliceLength = end - start,\n        pivotPoint = Math.floor(sliceLength / 2),\n        pivotIndex = this.elements[pivotPoint * 2]\n  \n    while (sliceLength > 1) {\n      if (pivotIndex < index) {\n        start = pivotPoint\n      }\n  \n      if (pivotIndex > index) {\n        end = pivotPoint\n      }\n  \n      if (pivotIndex == index) {\n        break\n      }\n  \n      sliceLength = end - start\n      pivotPoint = start + Math.floor(sliceLength / 2)\n      pivotIndex = this.elements[pivotPoint * 2]\n    }\n  \n    if (pivotIndex == index) {\n      return pivotPoint * 2\n    }\n  \n    if (pivotIndex > index) {\n      return pivotPoint * 2\n    }\n  \n    if (pivotIndex < index) {\n      return (pivotPoint + 1) * 2\n    }\n  }\n  \n  /**\n   * Inserts an element at an index within the vector.\n   *\n   * Does not allow duplicates, will throw an error if there is already an entry\n   * for this index.\n   *\n   * @param {Number} insertIdx - The index at which the element should be inserted.\n   * @param {Number} val - The value to be inserted into the vector.\n   */\n  lunr.Vector.prototype.insert = function (insertIdx, val) {\n    this.upsert(insertIdx, val, function () {\n      throw \"duplicate index\"\n    })\n  }\n  \n  /**\n   * Inserts or updates an existing index within the vector.\n   *\n   * @param {Number} insertIdx - The index at which the element should be inserted.\n   * @param {Number} val - The value to be inserted into the vector.\n   * @param {function} fn - A function that is called for updates, the existing value and the\n   * requested value are passed as arguments\n   */\n  lunr.Vector.prototype.upsert = function (insertIdx, val, fn) {\n    this._magnitude = 0\n    var position = this.positionForIndex(insertIdx)\n  \n    if (this.elements[position] == insertIdx) {\n      this.elements[position + 1] = fn(this.elements[position + 1], val)\n    } else {\n      this.elements.splice(position, 0, insertIdx, val)\n    }\n  }\n  \n  /**\n   * Calculates the magnitude of this vector.\n   *\n   * @returns {Number}\n   */\n  lunr.Vector.prototype.magnitude = function () {\n    if (this._magnitude) return this._magnitude\n  \n    var sumOfSquares = 0,\n        elementsLength = this.elements.length\n  \n    for (var i = 1; i < elementsLength; i += 2) {\n      var val = this.elements[i]\n      sumOfSquares += val * val\n    }\n  \n    return this._magnitude = Math.sqrt(sumOfSquares)\n  }\n  \n  /**\n   * Calculates the dot product of this vector and another vector.\n   *\n   * @param {lunr.Vector} otherVector - The vector to compute the dot product with.\n   * @returns {Number}\n   */\n  lunr.Vector.prototype.dot = function (otherVector) {\n    var dotProduct = 0,\n        a = this.elements, b = otherVector.elements,\n        aLen = a.length, bLen = b.length,\n        aVal = 0, bVal = 0,\n        i = 0, j = 0\n  \n    while (i < aLen && j < bLen) {\n      aVal = a[i], bVal = b[j]\n      if (aVal < bVal) {\n        i += 2\n      } else if (aVal > bVal) {\n        j += 2\n      } else if (aVal == bVal) {\n        dotProduct += a[i + 1] * b[j + 1]\n        i += 2\n        j += 2\n      }\n    }\n  \n    return dotProduct\n  }\n  \n  /**\n   * Calculates the similarity between this vector and another vector.\n   *\n   * @param {lunr.Vector} otherVector - The other vector to calculate the\n   * similarity with.\n   * @returns {Number}\n   */\n  lunr.Vector.prototype.similarity = function (otherVector) {\n    return this.dot(otherVector) / this.magnitude() || 0\n  }\n  \n  /**\n   * Converts the vector to an array of the elements within the vector.\n   *\n   * @returns {Number[]}\n   */\n  lunr.Vector.prototype.toArray = function () {\n    var output = new Array (this.elements.length / 2)\n  \n    for (var i = 1, j = 0; i < this.elements.length; i += 2, j++) {\n      output[j] = this.elements[i]\n    }\n  \n    return output\n  }\n  \n  /**\n   * A JSON serializable representation of the vector.\n   *\n   * @returns {Number[]}\n   */\n  lunr.Vector.prototype.toJSON = function () {\n    return this.elements\n  }\n  /* eslint-disable */\n  /*!\n   * lunr.stemmer\n   * Copyright (C) 2018 Oliver Nightingale\n   * Includes code from - http://tartarus.org/~martin/PorterStemmer/js.txt\n   */\n  \n  /**\n   * lunr.stemmer is an english language stemmer, this is a JavaScript\n   * implementation of the PorterStemmer taken from http://tartarus.org/~martin\n   *\n   * @static\n   * @implements {lunr.PipelineFunction}\n   * @param {lunr.Token} token - The string to stem\n   * @returns {lunr.Token}\n   * @see {@link lunr.Pipeline}\n   * @function\n   */\n  lunr.stemmer = (function(){\n    var step2list = {\n        \"ational\" : \"ate\",\n        \"tional\" : \"tion\",\n        \"enci\" : \"ence\",\n        \"anci\" : \"ance\",\n        \"izer\" : \"ize\",\n        \"bli\" : \"ble\",\n        \"alli\" : \"al\",\n        \"entli\" : \"ent\",\n        \"eli\" : \"e\",\n        \"ousli\" : \"ous\",\n        \"ization\" : \"ize\",\n        \"ation\" : \"ate\",\n        \"ator\" : \"ate\",\n        \"alism\" : \"al\",\n        \"iveness\" : \"ive\",\n        \"fulness\" : \"ful\",\n        \"ousness\" : \"ous\",\n        \"aliti\" : \"al\",\n        \"iviti\" : \"ive\",\n        \"biliti\" : \"ble\",\n        \"logi\" : \"log\"\n      },\n  \n      step3list = {\n        \"icate\" : \"ic\",\n        \"ative\" : \"\",\n        \"alize\" : \"al\",\n        \"iciti\" : \"ic\",\n        \"ical\" : \"ic\",\n        \"ful\" : \"\",\n        \"ness\" : \"\"\n      },\n  \n      c = \"[^aeiou]\",          // consonant\n      v = \"[aeiouy]\",          // vowel\n      C = c + \"[^aeiouy]*\",    // consonant sequence\n      V = v + \"[aeiou]*\",      // vowel sequence\n  \n      mgr0 = \"^(\" + C + \")?\" + V + C,               // [C]VC... is m>0\n      meq1 = \"^(\" + C + \")?\" + V + C + \"(\" + V + \")?$\",  // [C]VC[V] is m=1\n      mgr1 = \"^(\" + C + \")?\" + V + C + V + C,       // [C]VCVC... is m>1\n      s_v = \"^(\" + C + \")?\" + v;                   // vowel in stem\n  \n    var re_mgr0 = new RegExp(mgr0);\n    var re_mgr1 = new RegExp(mgr1);\n    var re_meq1 = new RegExp(meq1);\n    var re_s_v = new RegExp(s_v);\n  \n    var re_1a = /^(.+?)(ss|i)es$/;\n    var re2_1a = /^(.+?)([^s])s$/;\n    var re_1b = /^(.+?)eed$/;\n    var re2_1b = /^(.+?)(ed|ing)$/;\n    var re_1b_2 = /.$/;\n    var re2_1b_2 = /(at|bl|iz)$/;\n    var re3_1b_2 = new RegExp(\"([^aeiouylsz])\\\\1$\");\n    var re4_1b_2 = new RegExp(\"^\" + C + v + \"[^aeiouwxy]$\");\n  \n    var re_1c = /^(.+?[^aeiou])y$/;\n    var re_2 = /^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/;\n  \n    var re_3 = /^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/;\n  \n    var re_4 = /^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/;\n    var re2_4 = /^(.+?)(s|t)(ion)$/;\n  \n    var re_5 = /^(.+?)e$/;\n    var re_5_1 = /ll$/;\n    var re3_5 = new RegExp(\"^\" + C + v + \"[^aeiouwxy]$\");\n  \n    var porterStemmer = function porterStemmer(w) {\n      var stem,\n        suffix,\n        firstch,\n        re,\n        re2,\n        re3,\n        re4;\n  \n      if (w.length < 3) { return w; }\n  \n      firstch = w.substr(0,1);\n      if (firstch == \"y\") {\n        w = firstch.toUpperCase() + w.substr(1);\n      }\n  \n      // Step 1a\n      re = re_1a\n      re2 = re2_1a;\n  \n      if (re.test(w)) { w = w.replace(re,\"$1$2\"); }\n      else if (re2.test(w)) { w = w.replace(re2,\"$1$2\"); }\n  \n      // Step 1b\n      re = re_1b;\n      re2 = re2_1b;\n      if (re.test(w)) {\n        var fp = re.exec(w);\n        re = re_mgr0;\n        if (re.test(fp[1])) {\n          re = re_1b_2;\n          w = w.replace(re,\"\");\n        }\n      } else if (re2.test(w)) {\n        var fp = re2.exec(w);\n        stem = fp[1];\n        re2 = re_s_v;\n        if (re2.test(stem)) {\n          w = stem;\n          re2 = re2_1b_2;\n          re3 = re3_1b_2;\n          re4 = re4_1b_2;\n          if (re2.test(w)) { w = w + \"e\"; }\n          else if (re3.test(w)) { re = re_1b_2; w = w.replace(re,\"\"); }\n          else if (re4.test(w)) { w = w + \"e\"; }\n        }\n      }\n  \n      // Step 1c - replace suffix y or Y by i if preceded by a non-vowel which is not the first letter of the word (so cry -> cri, by -> by, say -> say)\n      re = re_1c;\n      if (re.test(w)) {\n        var fp = re.exec(w);\n        stem = fp[1];\n        w = stem + \"i\";\n      }\n  \n      // Step 2\n      re = re_2;\n      if (re.test(w)) {\n        var fp = re.exec(w);\n        stem = fp[1];\n        suffix = fp[2];\n        re = re_mgr0;\n        if (re.test(stem)) {\n          w = stem + step2list[suffix];\n        }\n      }\n  \n      // Step 3\n      re = re_3;\n      if (re.test(w)) {\n        var fp = re.exec(w);\n        stem = fp[1];\n        suffix = fp[2];\n        re = re_mgr0;\n        if (re.test(stem)) {\n          w = stem + step3list[suffix];\n        }\n      }\n  \n      // Step 4\n      re = re_4;\n      re2 = re2_4;\n      if (re.test(w)) {\n        var fp = re.exec(w);\n        stem = fp[1];\n        re = re_mgr1;\n        if (re.test(stem)) {\n          w = stem;\n        }\n      } else if (re2.test(w)) {\n        var fp = re2.exec(w);\n        stem = fp[1] + fp[2];\n        re2 = re_mgr1;\n        if (re2.test(stem)) {\n          w = stem;\n        }\n      }\n  \n      // Step 5\n      re = re_5;\n      if (re.test(w)) {\n        var fp = re.exec(w);\n        stem = fp[1];\n        re = re_mgr1;\n        re2 = re_meq1;\n        re3 = re3_5;\n        if (re.test(stem) || (re2.test(stem) && !(re3.test(stem)))) {\n          w = stem;\n        }\n      }\n  \n      re = re_5_1;\n      re2 = re_mgr1;\n      if (re.test(w) && re2.test(w)) {\n        re = re_1b_2;\n        w = w.replace(re,\"\");\n      }\n  \n      // and turn initial Y back to y\n  \n      if (firstch == \"y\") {\n        w = firstch.toLowerCase() + w.substr(1);\n      }\n  \n      return w;\n    };\n  \n    return function (token) {\n      return token.update(porterStemmer);\n    }\n  })();\n  \n  lunr.Pipeline.registerFunction(lunr.stemmer, 'stemmer')\n  /*!\n   * lunr.stopWordFilter\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * lunr.generateStopWordFilter builds a stopWordFilter function from the provided\n   * list of stop words.\n   *\n   * The built in lunr.stopWordFilter is built using this generator and can be used\n   * to generate custom stopWordFilters for applications or non English languages.\n   *\n   * @function\n   * @param {Array} token The token to pass through the filter\n   * @returns {lunr.PipelineFunction}\n   * @see lunr.Pipeline\n   * @see lunr.stopWordFilter\n   */\n  lunr.generateStopWordFilter = function (stopWords) {\n    var words = stopWords.reduce(function (memo, stopWord) {\n      memo[stopWord] = stopWord\n      return memo\n    }, {})\n  \n    return function (token) {\n      if (token && words[token.toString()] !== token.toString()) return token\n    }\n  }\n  \n  /**\n   * lunr.stopWordFilter is an English language stop word list filter, any words\n   * contained in the list will not be passed through the filter.\n   *\n   * This is intended to be used in the Pipeline. If the token does not pass the\n   * filter then undefined will be returned.\n   *\n   * @function\n   * @implements {lunr.PipelineFunction}\n   * @params {lunr.Token} token - A token to check for being a stop word.\n   * @returns {lunr.Token}\n   * @see {@link lunr.Pipeline}\n   */\n  lunr.stopWordFilter = lunr.generateStopWordFilter([\n    'a',\n    'able',\n    'about',\n    'across',\n    'after',\n    'all',\n    'almost',\n    'also',\n    'am',\n    'among',\n    'an',\n    'and',\n    'any',\n    'are',\n    'as',\n    'at',\n    'be',\n    'because',\n    'been',\n    'but',\n    'by',\n    'can',\n    'cannot',\n    'could',\n    'dear',\n    'did',\n    'do',\n    'does',\n    'either',\n    'else',\n    'ever',\n    'every',\n    'for',\n    'from',\n    'get',\n    'got',\n    'had',\n    'has',\n    'have',\n    'he',\n    'her',\n    'hers',\n    'him',\n    'his',\n    'how',\n    'however',\n    'i',\n    'if',\n    'in',\n    'into',\n    'is',\n    'it',\n    'its',\n    'just',\n    'least',\n    'let',\n    'like',\n    'likely',\n    'may',\n    'me',\n    'might',\n    'most',\n    'must',\n    'my',\n    'neither',\n    'no',\n    'nor',\n    'not',\n    'of',\n    'off',\n    'often',\n    'on',\n    'only',\n    'or',\n    'other',\n    'our',\n    'own',\n    'rather',\n    'said',\n    'say',\n    'says',\n    'she',\n    'should',\n    'since',\n    'so',\n    'some',\n    'than',\n    'that',\n    'the',\n    'their',\n    'them',\n    'then',\n    'there',\n    'these',\n    'they',\n    'this',\n    'tis',\n    'to',\n    'too',\n    'twas',\n    'us',\n    'wants',\n    'was',\n    'we',\n    'were',\n    'what',\n    'when',\n    'where',\n    'which',\n    'while',\n    'who',\n    'whom',\n    'why',\n    'will',\n    'with',\n    'would',\n    'yet',\n    'you',\n    'your'\n  ])\n  \n  lunr.Pipeline.registerFunction(lunr.stopWordFilter, 'stopWordFilter')\n  /*!\n   * lunr.trimmer\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * lunr.trimmer is a pipeline function for trimming non word\n   * characters from the beginning and end of tokens before they\n   * enter the index.\n   *\n   * This implementation may not work correctly for non latin\n   * characters and should either be removed or adapted for use\n   * with languages with non-latin characters.\n   *\n   * @static\n   * @implements {lunr.PipelineFunction}\n   * @param {lunr.Token} token The token to pass through the filter\n   * @returns {lunr.Token}\n   * @see lunr.Pipeline\n   */\n  lunr.trimmer = function (token) {\n    return token.update(function (s) {\n      return s.replace(/^\\W+/, '').replace(/\\W+$/, '')\n    })\n  }\n  \n  lunr.Pipeline.registerFunction(lunr.trimmer, 'trimmer')\n  /*!\n   * lunr.TokenSet\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * A token set is used to store the unique list of all tokens\n   * within an index. Token sets are also used to represent an\n   * incoming query to the index, this query token set and index\n   * token set are then intersected to find which tokens to look\n   * up in the inverted index.\n   *\n   * A token set can hold multiple tokens, as in the case of the\n   * index token set, or it can hold a single token as in the\n   * case of a simple query token set.\n   *\n   * Additionally token sets are used to perform wildcard matching.\n   * Leading, contained and trailing wildcards are supported, and\n   * from this edit distance matching can also be provided.\n   *\n   * Token sets are implemented as a minimal finite state automata,\n   * where both common prefixes and suffixes are shared between tokens.\n   * This helps to reduce the space used for storing the token set.\n   *\n   * @constructor\n   */\n  lunr.TokenSet = function () {\n    this.final = false\n    this.edges = {}\n    this.id = lunr.TokenSet._nextId\n    lunr.TokenSet._nextId += 1\n  }\n  \n  /**\n   * Keeps track of the next, auto increment, identifier to assign\n   * to a new tokenSet.\n   *\n   * TokenSets require a unique identifier to be correctly minimised.\n   *\n   * @private\n   */\n  lunr.TokenSet._nextId = 1\n  \n  /**\n   * Creates a TokenSet instance from the given sorted array of words.\n   *\n   * @param {String[]} arr - A sorted array of strings to create the set from.\n   * @returns {lunr.TokenSet}\n   * @throws Will throw an error if the input array is not sorted.\n   */\n  lunr.TokenSet.fromArray = function (arr) {\n    var builder = new lunr.TokenSet.Builder\n  \n    for (var i = 0, len = arr.length; i < len; i++) {\n      builder.insert(arr[i])\n    }\n  \n    builder.finish()\n    return builder.root\n  }\n  \n  /**\n   * Creates a token set from a query clause.\n   *\n   * @private\n   * @param {Object} clause - A single clause from lunr.Query.\n   * @param {string} clause.term - The query clause term.\n   * @param {number} [clause.editDistance] - The optional edit distance for the term.\n   * @returns {lunr.TokenSet}\n   */\n  lunr.TokenSet.fromClause = function (clause) {\n    if ('editDistance' in clause) {\n      return lunr.TokenSet.fromFuzzyString(clause.term, clause.editDistance)\n    } else {\n      return lunr.TokenSet.fromString(clause.term)\n    }\n  }\n  \n  /**\n   * Creates a token set representing a single string with a specified\n   * edit distance.\n   *\n   * Insertions, deletions, substitutions and transpositions are each\n   * treated as an edit distance of 1.\n   *\n   * Increasing the allowed edit distance will have a dramatic impact\n   * on the performance of both creating and intersecting these TokenSets.\n   * It is advised to keep the edit distance less than 3.\n   *\n   * @param {string} str - The string to create the token set from.\n   * @param {number} editDistance - The allowed edit distance to match.\n   * @returns {lunr.Vector}\n   */\n  lunr.TokenSet.fromFuzzyString = function (str, editDistance) {\n    var root = new lunr.TokenSet\n  \n    var stack = [{\n      node: root,\n      editsRemaining: editDistance,\n      str: str\n    }]\n  \n    while (stack.length) {\n      var frame = stack.pop()\n  \n      // no edit\n      if (frame.str.length > 0) {\n        var char = frame.str.charAt(0),\n            noEditNode\n  \n        if (char in frame.node.edges) {\n          noEditNode = frame.node.edges[char]\n        } else {\n          noEditNode = new lunr.TokenSet\n          frame.node.edges[char] = noEditNode\n        }\n  \n        if (frame.str.length == 1) {\n          noEditNode.final = true\n        } else {\n          stack.push({\n            node: noEditNode,\n            editsRemaining: frame.editsRemaining,\n            str: frame.str.slice(1)\n          })\n        }\n      }\n  \n      // deletion\n      // can only do a deletion if we have enough edits remaining\n      // and if there are characters left to delete in the string\n      if (frame.editsRemaining > 0 && frame.str.length > 1) {\n        var char = frame.str.charAt(1),\n            deletionNode\n  \n        if (char in frame.node.edges) {\n          deletionNode = frame.node.edges[char]\n        } else {\n          deletionNode = new lunr.TokenSet\n          frame.node.edges[char] = deletionNode\n        }\n  \n        if (frame.str.length <= 2) {\n          deletionNode.final = true\n        } else {\n          stack.push({\n            node: deletionNode,\n            editsRemaining: frame.editsRemaining - 1,\n            str: frame.str.slice(2)\n          })\n        }\n      }\n  \n      // deletion\n      // just removing the last character from the str\n      if (frame.editsRemaining > 0 && frame.str.length == 1) {\n        frame.node.final = true\n      }\n  \n      // substitution\n      // can only do a substitution if we have enough edits remaining\n      // and if there are characters left to substitute\n      if (frame.editsRemaining > 0 && frame.str.length >= 1) {\n        if (\"*\" in frame.node.edges) {\n          var substitutionNode = frame.node.edges[\"*\"]\n        } else {\n          var substitutionNode = new lunr.TokenSet\n          frame.node.edges[\"*\"] = substitutionNode\n        }\n  \n        if (frame.str.length == 1) {\n          substitutionNode.final = true\n        } else {\n          stack.push({\n            node: substitutionNode,\n            editsRemaining: frame.editsRemaining - 1,\n            str: frame.str.slice(1)\n          })\n        }\n      }\n  \n      // insertion\n      // can only do insertion if there are edits remaining\n      if (frame.editsRemaining > 0) {\n        if (\"*\" in frame.node.edges) {\n          var insertionNode = frame.node.edges[\"*\"]\n        } else {\n          var insertionNode = new lunr.TokenSet\n          frame.node.edges[\"*\"] = insertionNode\n        }\n  \n        if (frame.str.length == 0) {\n          insertionNode.final = true\n        } else {\n          stack.push({\n            node: insertionNode,\n            editsRemaining: frame.editsRemaining - 1,\n            str: frame.str\n          })\n        }\n      }\n  \n      // transposition\n      // can only do a transposition if there are edits remaining\n      // and there are enough characters to transpose\n      if (frame.editsRemaining > 0 && frame.str.length > 1) {\n        var charA = frame.str.charAt(0),\n            charB = frame.str.charAt(1),\n            transposeNode\n  \n        if (charB in frame.node.edges) {\n          transposeNode = frame.node.edges[charB]\n        } else {\n          transposeNode = new lunr.TokenSet\n          frame.node.edges[charB] = transposeNode\n        }\n  \n        if (frame.str.length == 1) {\n          transposeNode.final = true\n        } else {\n          stack.push({\n            node: transposeNode,\n            editsRemaining: frame.editsRemaining - 1,\n            str: charA + frame.str.slice(2)\n          })\n        }\n      }\n    }\n  \n    return root\n  }\n  \n  /**\n   * Creates a TokenSet from a string.\n   *\n   * The string may contain one or more wildcard characters (*)\n   * that will allow wildcard matching when intersecting with\n   * another TokenSet.\n   *\n   * @param {string} str - The string to create a TokenSet from.\n   * @returns {lunr.TokenSet}\n   */\n  lunr.TokenSet.fromString = function (str) {\n    var node = new lunr.TokenSet,\n        root = node\n  \n    /*\n     * Iterates through all characters within the passed string\n     * appending a node for each character.\n     *\n     * When a wildcard character is found then a self\n     * referencing edge is introduced to continually match\n     * any number of any characters.\n     */\n    for (var i = 0, len = str.length; i < len; i++) {\n      var char = str[i],\n          final = (i == len - 1)\n  \n      if (char == \"*\") {\n        node.edges[char] = node\n        node.final = final\n  \n      } else {\n        var next = new lunr.TokenSet\n        next.final = final\n  \n        node.edges[char] = next\n        node = next\n      }\n    }\n  \n    return root\n  }\n  \n  /**\n   * Converts this TokenSet into an array of strings\n   * contained within the TokenSet.\n   *\n   * @returns {string[]}\n   */\n  lunr.TokenSet.prototype.toArray = function () {\n    var words = []\n  \n    var stack = [{\n      prefix: \"\",\n      node: this\n    }]\n  \n    while (stack.length) {\n      var frame = stack.pop(),\n          edges = Object.keys(frame.node.edges),\n          len = edges.length\n  \n      if (frame.node.final) {\n        /* In Safari, at this point the prefix is sometimes corrupted, see:\n         * https://github.com/olivernn/lunr.js/issues/279 Calling any\n         * String.prototype method forces Safari to \"cast\" this string to what\n         * it's supposed to be, fixing the bug. */\n        frame.prefix.charAt(0)\n        words.push(frame.prefix)\n      }\n  \n      for (var i = 0; i < len; i++) {\n        var edge = edges[i]\n  \n        stack.push({\n          prefix: frame.prefix.concat(edge),\n          node: frame.node.edges[edge]\n        })\n      }\n    }\n  \n    return words\n  }\n  \n  /**\n   * Generates a string representation of a TokenSet.\n   *\n   * This is intended to allow TokenSets to be used as keys\n   * in objects, largely to aid the construction and minimisation\n   * of a TokenSet. As such it is not designed to be a human\n   * friendly representation of the TokenSet.\n   *\n   * @returns {string}\n   */\n  lunr.TokenSet.prototype.toString = function () {\n    // NOTE: Using Object.keys here as this.edges is very likely\n    // to enter 'hash-mode' with many keys being added\n    //\n    // avoiding a for-in loop here as it leads to the function\n    // being de-optimised (at least in V8). From some simple\n    // benchmarks the performance is comparable, but allowing\n    // V8 to optimize may mean easy performance wins in the future.\n  \n    if (this._str) {\n      return this._str\n    }\n  \n    var str = this.final ? '1' : '0',\n        labels = Object.keys(this.edges).sort(),\n        len = labels.length\n  \n    for (var i = 0; i < len; i++) {\n      var label = labels[i],\n          node = this.edges[label]\n  \n      str = str + label + node.id\n    }\n  \n    return str\n  }\n  \n  /**\n   * Returns a new TokenSet that is the intersection of\n   * this TokenSet and the passed TokenSet.\n   *\n   * This intersection will take into account any wildcards\n   * contained within the TokenSet.\n   *\n   * @param {lunr.TokenSet} b - An other TokenSet to intersect with.\n   * @returns {lunr.TokenSet}\n   */\n  lunr.TokenSet.prototype.intersect = function (b) {\n    var output = new lunr.TokenSet,\n        frame = undefined\n  \n    var stack = [{\n      qNode: b,\n      output: output,\n      node: this\n    }]\n  \n    while (stack.length) {\n      frame = stack.pop()\n  \n      // NOTE: As with the #toString method, we are using\n      // Object.keys and a for loop instead of a for-in loop\n      // as both of these objects enter 'hash' mode, causing\n      // the function to be de-optimised in V8\n      var qEdges = Object.keys(frame.qNode.edges),\n          qLen = qEdges.length,\n          nEdges = Object.keys(frame.node.edges),\n          nLen = nEdges.length\n  \n      for (var q = 0; q < qLen; q++) {\n        var qEdge = qEdges[q]\n  \n        for (var n = 0; n < nLen; n++) {\n          var nEdge = nEdges[n]\n  \n          if (nEdge == qEdge || qEdge == '*') {\n            var node = frame.node.edges[nEdge],\n                qNode = frame.qNode.edges[qEdge],\n                final = node.final && qNode.final,\n                next = undefined\n  \n            if (nEdge in frame.output.edges) {\n              // an edge already exists for this character\n              // no need to create a new node, just set the finality\n              // bit unless this node is already final\n              next = frame.output.edges[nEdge]\n              next.final = next.final || final\n  \n            } else {\n              // no edge exists yet, must create one\n              // set the finality bit and insert it\n              // into the output\n              next = new lunr.TokenSet\n              next.final = final\n              frame.output.edges[nEdge] = next\n            }\n  \n            stack.push({\n              qNode: qNode,\n              output: next,\n              node: node\n            })\n          }\n        }\n      }\n    }\n  \n    return output\n  }\n  lunr.TokenSet.Builder = function () {\n    this.previousWord = \"\"\n    this.root = new lunr.TokenSet\n    this.uncheckedNodes = []\n    this.minimizedNodes = {}\n  }\n  \n  lunr.TokenSet.Builder.prototype.insert = function (word) {\n    var node,\n        commonPrefix = 0\n  \n    if (word < this.previousWord) {\n      throw new Error (\"Out of order word insertion\")\n    }\n  \n    for (var i = 0; i < word.length && i < this.previousWord.length; i++) {\n      if (word[i] != this.previousWord[i]) break\n      commonPrefix++\n    }\n  \n    this.minimize(commonPrefix)\n  \n    if (this.uncheckedNodes.length == 0) {\n      node = this.root\n    } else {\n      node = this.uncheckedNodes[this.uncheckedNodes.length - 1].child\n    }\n  \n    for (var i = commonPrefix; i < word.length; i++) {\n      var nextNode = new lunr.TokenSet,\n          char = word[i]\n  \n      node.edges[char] = nextNode\n  \n      this.uncheckedNodes.push({\n        parent: node,\n        char: char,\n        child: nextNode\n      })\n  \n      node = nextNode\n    }\n  \n    node.final = true\n    this.previousWord = word\n  }\n  \n  lunr.TokenSet.Builder.prototype.finish = function () {\n    this.minimize(0)\n  }\n  \n  lunr.TokenSet.Builder.prototype.minimize = function (downTo) {\n    for (var i = this.uncheckedNodes.length - 1; i >= downTo; i--) {\n      var node = this.uncheckedNodes[i],\n          childKey = node.child.toString()\n  \n      if (childKey in this.minimizedNodes) {\n        node.parent.edges[node.char] = this.minimizedNodes[childKey]\n      } else {\n        // Cache the key for this node since\n        // we know it can't change anymore\n        node.child._str = childKey\n  \n        this.minimizedNodes[childKey] = node.child\n      }\n  \n      this.uncheckedNodes.pop()\n    }\n  }\n  /*!\n   * lunr.Index\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * An index contains the built index of all documents and provides a query interface\n   * to the index.\n   *\n   * Usually instances of lunr.Index will not be created using this constructor, instead\n   * lunr.Builder should be used to construct new indexes, or lunr.Index.load should be\n   * used to load previously built and serialized indexes.\n   *\n   * @constructor\n   * @param {Object} attrs - The attributes of the built search index.\n   * @param {Object} attrs.invertedIndex - An index of term/field to document reference.\n   * @param {Object<string, lunr.Vector>} attrs.fieldVectors - Field vectors\n   * @param {lunr.TokenSet} attrs.tokenSet - An set of all corpus tokens.\n   * @param {string[]} attrs.fields - The names of indexed document fields.\n   * @param {lunr.Pipeline} attrs.pipeline - The pipeline to use for search terms.\n   */\n  lunr.Index = function (attrs) {\n    this.invertedIndex = attrs.invertedIndex\n    this.fieldVectors = attrs.fieldVectors\n    this.tokenSet = attrs.tokenSet\n    this.fields = attrs.fields\n    this.pipeline = attrs.pipeline\n  }\n  \n  /**\n   * A result contains details of a document matching a search query.\n   * @typedef {Object} lunr.Index~Result\n   * @property {string} ref - The reference of the document this result represents.\n   * @property {number} score - A number between 0 and 1 representing how similar this document is to the query.\n   * @property {lunr.MatchData} matchData - Contains metadata about this match including which term(s) caused the match.\n   */\n  \n  /**\n   * Although lunr provides the ability to create queries using lunr.Query, it also provides a simple\n   * query language which itself is parsed into an instance of lunr.Query.\n   *\n   * For programmatically building queries it is advised to directly use lunr.Query, the query language\n   * is best used for human entered text rather than program generated text.\n   *\n   * At its simplest queries can just be a single term, e.g. `hello`, multiple terms are also supported\n   * and will be combined with OR, e.g `hello world` will match documents that contain either 'hello'\n   * or 'world', though those that contain both will rank higher in the results.\n   *\n   * Wildcards can be included in terms to match one or more unspecified characters, these wildcards can\n   * be inserted anywhere within the term, and more than one wildcard can exist in a single term. Adding\n   * wildcards will increase the number of documents that will be found but can also have a negative\n   * impact on query performance, especially with wildcards at the beginning of a term.\n   *\n   * Terms can be restricted to specific fields, e.g. `title:hello`, only documents with the term\n   * hello in the title field will match this query. Using a field not present in the index will lead\n   * to an error being thrown.\n   *\n   * Modifiers can also be added to terms, lunr supports edit distance and boost modifiers on terms. A term\n   * boost will make documents matching that term score higher, e.g. `foo^5`. Edit distance is also supported\n   * to provide fuzzy matching, e.g. 'hello~2' will match documents with hello with an edit distance of 2.\n   * Avoid large values for edit distance to improve query performance.\n   *\n   * Each term also supports a presence modifier. By default a term's presence in document is optional, however\n   * this can be changed to either required or prohibited. For a term's presence to be required in a document the\n   * term should be prefixed with a '+', e.g. `+foo bar` is a search for documents that must contain 'foo' and\n   * optionally contain 'bar'. Conversely a leading '-' sets the terms presence to prohibited, i.e. it must not\n   * appear in a document, e.g. `-foo bar` is a search for documents that do not contain 'foo' but may contain 'bar'.\n   *\n   * To escape special characters the backslash character '\\' can be used, this allows searches to include\n   * characters that would normally be considered modifiers, e.g. `foo\\~2` will search for a term \"foo~2\" instead\n   * of attempting to apply a boost of 2 to the search term \"foo\".\n   *\n   * @typedef {string} lunr.Index~QueryString\n   * @example <caption>Simple single term query</caption>\n   * hello\n   * @example <caption>Multiple term query</caption>\n   * hello world\n   * @example <caption>term scoped to a field</caption>\n   * title:hello\n   * @example <caption>term with a boost of 10</caption>\n   * hello^10\n   * @example <caption>term with an edit distance of 2</caption>\n   * hello~2\n   * @example <caption>terms with presence modifiers</caption>\n   * -foo +bar baz\n   */\n  \n  /**\n   * Performs a search against the index using lunr query syntax.\n   *\n   * Results will be returned sorted by their score, the most relevant results\n   * will be returned first.  For details on how the score is calculated, please see\n   * the {@link https://lunrjs.com/guides/searching.html#scoring|guide}.\n   *\n   * For more programmatic querying use lunr.Index#query.\n   *\n   * @param {lunr.Index~QueryString} queryString - A string containing a lunr query.\n   * @throws {lunr.QueryParseError} If the passed query string cannot be parsed.\n   * @returns {lunr.Index~Result[]}\n   */\n  lunr.Index.prototype.search = function (queryString) {\n    return this.query(function (query) {\n      var parser = new lunr.QueryParser(queryString, query)\n      parser.parse()\n    })\n  }\n  \n  /**\n   * A query builder callback provides a query object to be used to express\n   * the query to perform on the index.\n   *\n   * @callback lunr.Index~queryBuilder\n   * @param {lunr.Query} query - The query object to build up.\n   * @this lunr.Query\n   */\n  \n  /**\n   * Performs a query against the index using the yielded lunr.Query object.\n   *\n   * If performing programmatic queries against the index, this method is preferred\n   * over lunr.Index#search so as to avoid the additional query parsing overhead.\n   *\n   * A query object is yielded to the supplied function which should be used to\n   * express the query to be run against the index.\n   *\n   * Note that although this function takes a callback parameter it is _not_ an\n   * asynchronous operation, the callback is just yielded a query object to be\n   * customized.\n   *\n   * @param {lunr.Index~queryBuilder} fn - A function that is used to build the query.\n   * @returns {lunr.Index~Result[]}\n   */\n  lunr.Index.prototype.query = function (fn) {\n    // for each query clause\n    // * process terms\n    // * expand terms from token set\n    // * find matching documents and metadata\n    // * get document vectors\n    // * score documents\n  \n    var query = new lunr.Query(this.fields),\n        matchingFields = Object.create(null),\n        queryVectors = Object.create(null),\n        termFieldCache = Object.create(null),\n        requiredMatches = Object.create(null),\n        prohibitedMatches = Object.create(null)\n  \n    /*\n     * To support field level boosts a query vector is created per\n     * field. An empty vector is eagerly created to support negated\n     * queries.\n     */\n    for (var i = 0; i < this.fields.length; i++) {\n      queryVectors[this.fields[i]] = new lunr.Vector\n    }\n  \n    fn.call(query, query)\n  \n    for (var i = 0; i < query.clauses.length; i++) {\n      /*\n       * Unless the pipeline has been disabled for this term, which is\n       * the case for terms with wildcards, we need to pass the clause\n       * term through the search pipeline. A pipeline returns an array\n       * of processed terms. Pipeline functions may expand the passed\n       * term, which means we may end up performing multiple index lookups\n       * for a single query term.\n       */\n      var clause = query.clauses[i],\n          terms = null,\n          clauseMatches = lunr.Set.complete\n  \n      if (clause.usePipeline) {\n        terms = this.pipeline.runString(clause.term, {\n          fields: clause.fields\n        })\n      } else {\n        terms = [clause.term]\n      }\n  \n      for (var m = 0; m < terms.length; m++) {\n        var term = terms[m]\n  \n        /*\n         * Each term returned from the pipeline needs to use the same query\n         * clause object, e.g. the same boost and or edit distance. The\n         * simplest way to do this is to re-use the clause object but mutate\n         * its term property.\n         */\n        clause.term = term\n  \n        /*\n         * From the term in the clause we create a token set which will then\n         * be used to intersect the indexes token set to get a list of terms\n         * to lookup in the inverted index\n         */\n        var termTokenSet = lunr.TokenSet.fromClause(clause),\n            expandedTerms = this.tokenSet.intersect(termTokenSet).toArray()\n  \n        /*\n         * If a term marked as required does not exist in the tokenSet it is\n         * impossible for the search to return any matches. We set all the field\n         * scoped required matches set to empty and stop examining any further\n         * clauses.\n         */\n        if (expandedTerms.length === 0 && clause.presence === lunr.Query.presence.REQUIRED) {\n          for (var k = 0; k < clause.fields.length; k++) {\n            var field = clause.fields[k]\n            requiredMatches[field] = lunr.Set.empty\n          }\n  \n          break\n        }\n  \n        for (var j = 0; j < expandedTerms.length; j++) {\n          /*\n           * For each term get the posting and termIndex, this is required for\n           * building the query vector.\n           */\n          var expandedTerm = expandedTerms[j],\n              posting = this.invertedIndex[expandedTerm],\n              termIndex = posting._index\n  \n          for (var k = 0; k < clause.fields.length; k++) {\n            /*\n             * For each field that this query term is scoped by (by default\n             * all fields are in scope) we need to get all the document refs\n             * that have this term in that field.\n             *\n             * The posting is the entry in the invertedIndex for the matching\n             * term from above.\n             */\n            var field = clause.fields[k],\n                fieldPosting = posting[field],\n                matchingDocumentRefs = Object.keys(fieldPosting),\n                termField = expandedTerm + \"/\" + field,\n                matchingDocumentsSet = new lunr.Set(matchingDocumentRefs)\n  \n            /*\n             * if the presence of this term is required ensure that the matching\n             * documents are added to the set of required matches for this clause.\n             *\n             */\n            if (clause.presence == lunr.Query.presence.REQUIRED) {\n              clauseMatches = clauseMatches.union(matchingDocumentsSet)\n  \n              if (requiredMatches[field] === undefined) {\n                requiredMatches[field] = lunr.Set.complete\n              }\n            }\n  \n            /*\n             * if the presence of this term is prohibited ensure that the matching\n             * documents are added to the set of prohibited matches for this field,\n             * creating that set if it does not yet exist.\n             */\n            if (clause.presence == lunr.Query.presence.PROHIBITED) {\n              if (prohibitedMatches[field] === undefined) {\n                prohibitedMatches[field] = lunr.Set.empty\n              }\n  \n              prohibitedMatches[field] = prohibitedMatches[field].union(matchingDocumentsSet)\n  \n              /*\n               * Prohibited matches should not be part of the query vector used for\n               * similarity scoring and no metadata should be extracted so we continue\n               * to the next field\n               */\n              continue\n            }\n  \n            /*\n             * The query field vector is populated using the termIndex found for\n             * the term and a unit value with the appropriate boost applied.\n             * Using upsert because there could already be an entry in the vector\n             * for the term we are working with. In that case we just add the scores\n             * together.\n             */\n            queryVectors[field].upsert(termIndex, clause.boost, function (a, b) { return a + b })\n  \n            /**\n             * If we've already seen this term, field combo then we've already collected\n             * the matching documents and metadata, no need to go through all that again\n             */\n            if (termFieldCache[termField]) {\n              continue\n            }\n  \n            for (var l = 0; l < matchingDocumentRefs.length; l++) {\n              /*\n               * All metadata for this term/field/document triple\n               * are then extracted and collected into an instance\n               * of lunr.MatchData ready to be returned in the query\n               * results\n               */\n              var matchingDocumentRef = matchingDocumentRefs[l],\n                  matchingFieldRef = new lunr.FieldRef (matchingDocumentRef, field),\n                  metadata = fieldPosting[matchingDocumentRef],\n                  fieldMatch\n  \n              if ((fieldMatch = matchingFields[matchingFieldRef]) === undefined) {\n                matchingFields[matchingFieldRef] = new lunr.MatchData (expandedTerm, field, metadata)\n              } else {\n                fieldMatch.add(expandedTerm, field, metadata)\n              }\n  \n            }\n  \n            termFieldCache[termField] = true\n          }\n        }\n      }\n  \n      /**\n       * If the presence was required we need to update the requiredMatches field sets.\n       * We do this after all fields for the term have collected their matches because\n       * the clause terms presence is required in _any_ of the fields not _all_ of the\n       * fields.\n       */\n      if (clause.presence === lunr.Query.presence.REQUIRED) {\n        for (var k = 0; k < clause.fields.length; k++) {\n          var field = clause.fields[k]\n          requiredMatches[field] = requiredMatches[field].intersect(clauseMatches)\n        }\n      }\n    }\n  \n    /**\n     * Need to combine the field scoped required and prohibited\n     * matching documents into a global set of required and prohibited\n     * matches\n     */\n    var allRequiredMatches = lunr.Set.complete,\n        allProhibitedMatches = lunr.Set.empty\n  \n    for (var i = 0; i < this.fields.length; i++) {\n      var field = this.fields[i]\n  \n      if (requiredMatches[field]) {\n        allRequiredMatches = allRequiredMatches.intersect(requiredMatches[field])\n      }\n  \n      if (prohibitedMatches[field]) {\n        allProhibitedMatches = allProhibitedMatches.union(prohibitedMatches[field])\n      }\n    }\n  \n    var matchingFieldRefs = Object.keys(matchingFields),\n        results = [],\n        matches = Object.create(null)\n  \n    /*\n     * If the query is negated (contains only prohibited terms)\n     * we need to get _all_ fieldRefs currently existing in the\n     * index. This is only done when we know that the query is\n     * entirely prohibited terms to avoid any cost of getting all\n     * fieldRefs unnecessarily.\n     *\n     * Additionally, blank MatchData must be created to correctly\n     * populate the results.\n     */\n    if (query.isNegated()) {\n      matchingFieldRefs = Object.keys(this.fieldVectors)\n  \n      for (var i = 0; i < matchingFieldRefs.length; i++) {\n        var matchingFieldRef = matchingFieldRefs[i]\n        var fieldRef = lunr.FieldRef.fromString(matchingFieldRef)\n        matchingFields[matchingFieldRef] = new lunr.MatchData\n      }\n    }\n  \n    for (var i = 0; i < matchingFieldRefs.length; i++) {\n      /*\n       * Currently we have document fields that match the query, but we\n       * need to return documents. The matchData and scores are combined\n       * from multiple fields belonging to the same document.\n       *\n       * Scores are calculated by field, using the query vectors created\n       * above, and combined into a final document score using addition.\n       */\n      var fieldRef = lunr.FieldRef.fromString(matchingFieldRefs[i]),\n          docRef = fieldRef.docRef\n  \n      if (!allRequiredMatches.contains(docRef)) {\n        continue\n      }\n  \n      if (allProhibitedMatches.contains(docRef)) {\n        continue\n      }\n  \n      var fieldVector = this.fieldVectors[fieldRef],\n          score = queryVectors[fieldRef.fieldName].similarity(fieldVector),\n          docMatch\n  \n      if ((docMatch = matches[docRef]) !== undefined) {\n        docMatch.score += score\n        docMatch.matchData.combine(matchingFields[fieldRef])\n      } else {\n        var match = {\n          ref: docRef,\n          score: score,\n          matchData: matchingFields[fieldRef]\n        }\n        matches[docRef] = match\n        results.push(match)\n      }\n    }\n  \n    /*\n     * Sort the results objects by score, highest first.\n     */\n    return results.sort(function (a, b) {\n      return b.score - a.score\n    })\n  }\n  \n  /**\n   * Prepares the index for JSON serialization.\n   *\n   * The schema for this JSON blob will be described in a\n   * separate JSON schema file.\n   *\n   * @returns {Object}\n   */\n  lunr.Index.prototype.toJSON = function () {\n    var invertedIndex = Object.keys(this.invertedIndex)\n      .sort()\n      .map(function (term) {\n        return [term, this.invertedIndex[term]]\n      }, this)\n  \n    var fieldVectors = Object.keys(this.fieldVectors)\n      .map(function (ref) {\n        return [ref, this.fieldVectors[ref].toJSON()]\n      }, this)\n  \n    return {\n      version: lunr.version,\n      fields: this.fields,\n      fieldVectors: fieldVectors,\n      invertedIndex: invertedIndex,\n      pipeline: this.pipeline.toJSON()\n    }\n  }\n  \n  /**\n   * Loads a previously serialized lunr.Index\n   *\n   * @param {Object} serializedIndex - A previously serialized lunr.Index\n   * @returns {lunr.Index}\n   */\n  lunr.Index.load = function (serializedIndex) {\n    var attrs = {},\n        fieldVectors = {},\n        serializedVectors = serializedIndex.fieldVectors,\n        invertedIndex = {},\n        serializedInvertedIndex = serializedIndex.invertedIndex,\n        tokenSetBuilder = new lunr.TokenSet.Builder,\n        pipeline = lunr.Pipeline.load(serializedIndex.pipeline)\n  \n    if (serializedIndex.version != lunr.version) {\n      lunr.utils.warn(\"Version mismatch when loading serialised index. Current version of lunr '\" + lunr.version + \"' does not match serialized index '\" + serializedIndex.version + \"'\")\n    }\n  \n    for (var i = 0; i < serializedVectors.length; i++) {\n      var tuple = serializedVectors[i],\n          ref = tuple[0],\n          elements = tuple[1]\n  \n      fieldVectors[ref] = new lunr.Vector(elements)\n    }\n  \n    for (var i = 0; i < serializedInvertedIndex.length; i++) {\n      var tuple = serializedInvertedIndex[i],\n          term = tuple[0],\n          posting = tuple[1]\n  \n      tokenSetBuilder.insert(term)\n      invertedIndex[term] = posting\n    }\n  \n    tokenSetBuilder.finish()\n  \n    attrs.fields = serializedIndex.fields\n  \n    attrs.fieldVectors = fieldVectors\n    attrs.invertedIndex = invertedIndex\n    attrs.tokenSet = tokenSetBuilder.root\n    attrs.pipeline = pipeline\n  \n    return new lunr.Index(attrs)\n  }\n  /*!\n   * lunr.Builder\n   * Copyright (C) 2018 Oliver Nightingale\n   */\n  \n  /**\n   * lunr.Builder performs indexing on a set of documents and\n   * returns instances of lunr.Index ready for querying.\n   *\n   * All configuration of the index is done via the builder, the\n   * fields to index, the document reference, the text processing\n   * pipeline and document scoring parameters are all set on the\n   * builder before indexing.\n   *\n   * @constructor\n   * @property {string} _ref - Internal reference to the document reference field.\n   * @property {string[]} _fields - Internal reference to the document fields to index.\n   * @property {object} invertedIndex - The inverted index maps terms to document fields.\n   * @property {object} documentTermFrequencies - Keeps track of document term frequencies.\n   * @property {object} documentLengths - Keeps track of the length of documents added to the index.\n   * @property {lunr.tokenizer} tokenizer - Function for splitting strings into tokens for indexing.\n   * @property {lunr.Pipeline} pipeline - The pipeline performs text processing on tokens before indexing.\n   * @property {lunr.Pipeline} searchPipeline - A pipeline for processing search terms before querying the index.\n   * @property {number} documentCount - Keeps track of the total number of documents indexed.\n   * @property {number} _b - A parameter to control field length normalization, setting this to 0 disabled normalization, 1 fully normalizes field lengths, the default value is 0.75.\n   * @property {number} _k1 - A parameter to control how quickly an increase in term frequency results in term frequency saturation, the default value is 1.2.\n   * @property {number} termIndex - A counter incremented for each unique term, used to identify a terms position in the vector space.\n   * @property {array} metadataWhitelist - A list of metadata keys that have been whitelisted for entry in the index.\n   */\n  lunr.Builder = function () {\n    this._ref = \"id\"\n    this._fields = Object.create(null)\n    this._documents = Object.create(null)\n    this.invertedIndex = Object.create(null)\n    this.fieldTermFrequencies = {}\n    this.fieldLengths = {}\n    this.tokenizer = lunr.tokenizer\n    this.pipeline = new lunr.Pipeline\n    this.searchPipeline = new lunr.Pipeline\n    this.documentCount = 0\n    this._b = 0.75\n    this._k1 = 1.2\n    this.termIndex = 0\n    this.metadataWhitelist = []\n  }\n  \n  /**\n   * Sets the document field used as the document reference. Every document must have this field.\n   * The type of this field in the document should be a string, if it is not a string it will be\n   * coerced into a string by calling toString.\n   *\n   * The default ref is 'id'.\n   *\n   * The ref should _not_ be changed during indexing, it should be set before any documents are\n   * added to the index. Changing it during indexing can lead to inconsistent results.\n   *\n   * @param {string} ref - The name of the reference field in the document.\n   */\n  lunr.Builder.prototype.ref = function (ref) {\n    this._ref = ref\n  }\n  \n  /**\n   * A function that is used to extract a field from a document.\n   *\n   * Lunr expects a field to be at the top level of a document, if however the field\n   * is deeply nested within a document an extractor function can be used to extract\n   * the right field for indexing.\n   *\n   * @callback fieldExtractor\n   * @param {object} doc - The document being added to the index.\n   * @returns {?(string|object|object[])} obj - The object that will be indexed for this field.\n   * @example <caption>Extracting a nested field</caption>\n   * function (doc) { return doc.nested.field }\n   */\n  \n  /**\n   * Adds a field to the list of document fields that will be indexed. Every document being\n   * indexed should have this field. Null values for this field in indexed documents will\n   * not cause errors but will limit the chance of that document being retrieved by searches.\n   *\n   * All fields should be added before adding documents to the index. Adding fields after\n   * a document has been indexed will have no effect on already indexed documents.\n   *\n   * Fields can be boosted at build time. This allows terms within that field to have more\n   * importance when ranking search results. Use a field boost to specify that matches within\n   * one field are more important than other fields.\n   *\n   * @param {string} fieldName - The name of a field to index in all documents.\n   * @param {object} attributes - Optional attributes associated with this field.\n   * @param {number} [attributes.boost=1] - Boost applied to all terms within this field.\n   * @param {fieldExtractor} [attributes.extractor] - Function to extract a field from a document.\n   * @throws {RangeError} fieldName cannot contain unsupported characters '/'\n   */\n  lunr.Builder.prototype.field = function (fieldName, attributes) {\n    if (/\\//.test(fieldName)) {\n      throw new RangeError (\"Field '\" + fieldName + \"' contains illegal character '/'\")\n    }\n  \n    this._fields[fieldName] = attributes || {}\n  }\n  \n  /**\n   * A parameter to tune the amount of field length normalisation that is applied when\n   * calculating relevance scores. A value of 0 will completely disable any normalisation\n   * and a value of 1 will fully normalise field lengths. The default is 0.75. Values of b\n   * will be clamped to the range 0 - 1.\n   *\n   * @param {number} number - The value to set for this tuning parameter.\n   */\n  lunr.Builder.prototype.b = function (number) {\n    if (number < 0) {\n      this._b = 0\n    } else if (number > 1) {\n      this._b = 1\n    } else {\n      this._b = number\n    }\n  }\n  \n  /**\n   * A parameter that controls the speed at which a rise in term frequency results in term\n   * frequency saturation. The default value is 1.2. Setting this to a higher value will give\n   * slower saturation levels, a lower value will result in quicker saturation.\n   *\n   * @param {number} number - The value to set for this tuning parameter.\n   */\n  lunr.Builder.prototype.k1 = function (number) {\n    this._k1 = number\n  }\n  \n  /**\n   * Adds a document to the index.\n   *\n   * Before adding fields to the index the index should have been fully setup, with the document\n   * ref and all fields to index already having been specified.\n   *\n   * The document must have a field name as specified by the ref (by default this is 'id') and\n   * it should have all fields defined for indexing, though null or undefined values will not\n   * cause errors.\n   *\n   * Entire documents can be boosted at build time. Applying a boost to a document indicates that\n   * this document should rank higher in search results than other documents.\n   *\n   * @param {object} doc - The document to add to the index.\n   * @param {object} attributes - Optional attributes associated with this document.\n   * @param {number} [attributes.boost=1] - Boost applied to all terms within this document.\n   */\n  lunr.Builder.prototype.add = function (doc, attributes) {\n    var docRef = doc[this._ref],\n        fields = Object.keys(this._fields)\n  \n    this._documents[docRef] = attributes || {}\n    this.documentCount += 1\n  \n    for (var i = 0; i < fields.length; i++) {\n      var fieldName = fields[i],\n          extractor = this._fields[fieldName].extractor,\n          field = extractor ? extractor(doc) : doc[fieldName],\n          tokens = this.tokenizer(field, {\n            fields: [fieldName]\n          }),\n          terms = this.pipeline.run(tokens),\n          fieldRef = new lunr.FieldRef (docRef, fieldName),\n          fieldTerms = Object.create(null)\n  \n      this.fieldTermFrequencies[fieldRef] = fieldTerms\n      this.fieldLengths[fieldRef] = 0\n  \n      // store the length of this field for this document\n      this.fieldLengths[fieldRef] += terms.length\n  \n      // calculate term frequencies for this field\n      for (var j = 0; j < terms.length; j++) {\n        var term = terms[j]\n  \n        if (fieldTerms[term] == undefined) {\n          fieldTerms[term] = 0\n        }\n  \n        fieldTerms[term] += 1\n  \n        // add to inverted index\n        // create an initial posting if one doesn't exist\n        if (this.invertedIndex[term] == undefined) {\n          var posting = Object.create(null)\n          posting[\"_index\"] = this.termIndex\n          this.termIndex += 1\n  \n          for (var k = 0; k < fields.length; k++) {\n            posting[fields[k]] = Object.create(null)\n          }\n  \n          this.invertedIndex[term] = posting\n        }\n  \n        // add an entry for this term/fieldName/docRef to the invertedIndex\n        if (this.invertedIndex[term][fieldName][docRef] == undefined) {\n          this.invertedIndex[term][fieldName][docRef] = Object.create(null)\n        }\n  \n        // store all whitelisted metadata about this token in the\n        // inverted index\n        for (var l = 0; l < this.metadataWhitelist.length; l++) {\n          var metadataKey = this.metadataWhitelist[l],\n              metadata = term.metadata[metadataKey]\n  \n          if (this.invertedIndex[term][fieldName][docRef][metadataKey] == undefined) {\n            this.invertedIndex[term][fieldName][docRef][metadataKey] = []\n          }\n  \n          this.invertedIndex[term][fieldName][docRef][metadataKey].push(metadata)\n        }\n      }\n  \n    }\n  }\n  \n  /**\n   * Calculates the average document length for this index\n   *\n   * @private\n   */\n  lunr.Builder.prototype.calculateAverageFieldLengths = function () {\n  \n    var fieldRefs = Object.keys(this.fieldLengths),\n        numberOfFields = fieldRefs.length,\n        accumulator = {},\n        documentsWithField = {}\n  \n    for (var i = 0; i < numberOfFields; i++) {\n      var fieldRef = lunr.FieldRef.fromString(fieldRefs[i]),\n          field = fieldRef.fieldName\n  \n      documentsWithField[field] || (documentsWithField[field] = 0)\n      documentsWithField[field] += 1\n  \n      accumulator[field] || (accumulator[field] = 0)\n      accumulator[field] += this.fieldLengths[fieldRef]\n    }\n  \n    var fields = Object.keys(this._fields)\n  \n    for (var i = 0; i < fields.length; i++) {\n      var fieldName = fields[i]\n      accumulator[fieldName] = accumulator[fieldName] / documentsWithField[fieldName]\n    }\n  \n    this.averageFieldLength = accumulator\n  }\n  \n  /**\n   * Builds a vector space model of every document using lunr.Vector\n   *\n   * @private\n   */\n  lunr.Builder.prototype.createFieldVectors = function () {\n    var fieldVectors = {},\n        fieldRefs = Object.keys(this.fieldTermFrequencies),\n        fieldRefsLength = fieldRefs.length,\n        termIdfCache = Object.create(null)\n  \n    for (var i = 0; i < fieldRefsLength; i++) {\n      var fieldRef = lunr.FieldRef.fromString(fieldRefs[i]),\n          fieldName = fieldRef.fieldName,\n          fieldLength = this.fieldLengths[fieldRef],\n          fieldVector = new lunr.Vector,\n          termFrequencies = this.fieldTermFrequencies[fieldRef],\n          terms = Object.keys(termFrequencies),\n          termsLength = terms.length\n  \n  \n      var fieldBoost = this._fields[fieldName].boost || 1,\n          docBoost = this._documents[fieldRef.docRef].boost || 1\n  \n      for (var j = 0; j < termsLength; j++) {\n        var term = terms[j],\n            tf = termFrequencies[term],\n            termIndex = this.invertedIndex[term]._index,\n            idf, score, scoreWithPrecision\n  \n        if (termIdfCache[term] === undefined) {\n          idf = lunr.idf(this.invertedIndex[term], this.documentCount)\n          termIdfCache[term] = idf\n        } else {\n          idf = termIdfCache[term]\n        }\n  \n        score = idf * ((this._k1 + 1) * tf) / (this._k1 * (1 - this._b + this._b * (fieldLength / this.averageFieldLength[fieldName])) + tf)\n        score *= fieldBoost\n        score *= docBoost\n        scoreWithPrecision = Math.round(score * 1000) / 1000\n        // Converts 1.23456789 to 1.234.\n        // Reducing the precision so that the vectors take up less\n        // space when serialised. Doing it now so that they behave\n        // the same before and after serialisation. Also, this is\n        // the fastest approach to reducing a number's precision in\n        // JavaScript.\n  \n        fieldVector.insert(termIndex, scoreWithPrecision)\n      }\n  \n      fieldVectors[fieldRef] = fieldVector\n    }\n  \n    this.fieldVectors = fieldVectors\n  }\n  \n  /**\n   * Creates a token set of all tokens in the index using lunr.TokenSet\n   *\n   * @private\n   */\n  lunr.Builder.prototype.createTokenSet = function () {\n    this.tokenSet = lunr.TokenSet.fromArray(\n      Object.keys(this.invertedIndex).sort()\n    )\n  }\n  \n  /**\n   * Builds the index, creating an instance of lunr.Index.\n   *\n   * This completes the indexing process and should only be called\n   * once all documents have been added to the index.\n   *\n   * @returns {lunr.Index}\n   */\n  lunr.Builder.prototype.build = function () {\n    this.calculateAverageFieldLengths()\n    this.createFieldVectors()\n    this.createTokenSet()\n  \n    return new lunr.Index({\n      invertedIndex: this.invertedIndex,\n      fieldVectors: this.fieldVectors,\n      tokenSet: this.tokenSet,\n      fields: Object.keys(this._fields),\n      pipeline: this.searchPipeline\n    })\n  }\n  \n  /**\n   * Applies a plugin to the index builder.\n   *\n   * A plugin is a function that is called with the index builder as its context.\n   * Plugins can be used to customise or extend the behaviour of the index\n   * in some way. A plugin is just a function, that encapsulated the custom\n   * behaviour that should be applied when building the index.\n   *\n   * The plugin function will be called with the index builder as its argument, additional\n   * arguments can also be passed when calling use. The function will be called\n   * with the index builder as its context.\n   *\n   * @param {Function} plugin The plugin to apply.\n   */\n  lunr.Builder.prototype.use = function (fn) {\n    var args = Array.prototype.slice.call(arguments, 1)\n    args.unshift(this)\n    fn.apply(this, args)\n  }\n  /**\n   * Contains and collects metadata about a matching document.\n   * A single instance of lunr.MatchData is returned as part of every\n   * lunr.Index~Result.\n   *\n   * @constructor\n   * @param {string} term - The term this match data is associated with\n   * @param {string} field - The field in which the term was found\n   * @param {object} metadata - The metadata recorded about this term in this field\n   * @property {object} metadata - A cloned collection of metadata associated with this document.\n   * @see {@link lunr.Index~Result}\n   */\n  lunr.MatchData = function (term, field, metadata) {\n    var clonedMetadata = Object.create(null),\n        metadataKeys = Object.keys(metadata || {})\n  \n    // Cloning the metadata to prevent the original\n    // being mutated during match data combination.\n    // Metadata is kept in an array within the inverted\n    // index so cloning the data can be done with\n    // Array#slice\n    for (var i = 0; i < metadataKeys.length; i++) {\n      var key = metadataKeys[i]\n      clonedMetadata[key] = metadata[key].slice()\n    }\n  \n    this.metadata = Object.create(null)\n  \n    if (term !== undefined) {\n      this.metadata[term] = Object.create(null)\n      this.metadata[term][field] = clonedMetadata\n    }\n  }\n  \n  /**\n   * An instance of lunr.MatchData will be created for every term that matches a\n   * document. However only one instance is required in a lunr.Index~Result. This\n   * method combines metadata from another instance of lunr.MatchData with this\n   * objects metadata.\n   *\n   * @param {lunr.MatchData} otherMatchData - Another instance of match data to merge with this one.\n   * @see {@link lunr.Index~Result}\n   */\n  lunr.MatchData.prototype.combine = function (otherMatchData) {\n    var terms = Object.keys(otherMatchData.metadata)\n  \n    for (var i = 0; i < terms.length; i++) {\n      var term = terms[i],\n          fields = Object.keys(otherMatchData.metadata[term])\n  \n      if (this.metadata[term] == undefined) {\n        this.metadata[term] = Object.create(null)\n      }\n  \n      for (var j = 0; j < fields.length; j++) {\n        var field = fields[j],\n            keys = Object.keys(otherMatchData.metadata[term][field])\n  \n        if (this.metadata[term][field] == undefined) {\n          this.metadata[term][field] = Object.create(null)\n        }\n  \n        for (var k = 0; k < keys.length; k++) {\n          var key = keys[k]\n  \n          if (this.metadata[term][field][key] == undefined) {\n            this.metadata[term][field][key] = otherMatchData.metadata[term][field][key]\n          } else {\n            this.metadata[term][field][key] = this.metadata[term][field][key].concat(otherMatchData.metadata[term][field][key])\n          }\n  \n        }\n      }\n    }\n  }\n  \n  /**\n   * Add metadata for a term/field pair to this instance of match data.\n   *\n   * @param {string} term - The term this match data is associated with\n   * @param {string} field - The field in which the term was found\n   * @param {object} metadata - The metadata recorded about this term in this field\n   */\n  lunr.MatchData.prototype.add = function (term, field, metadata) {\n    if (!(term in this.metadata)) {\n      this.metadata[term] = Object.create(null)\n      this.metadata[term][field] = metadata\n      return\n    }\n  \n    if (!(field in this.metadata[term])) {\n      this.metadata[term][field] = metadata\n      return\n    }\n  \n    var metadataKeys = Object.keys(metadata)\n  \n    for (var i = 0; i < metadataKeys.length; i++) {\n      var key = metadataKeys[i]\n  \n      if (key in this.metadata[term][field]) {\n        this.metadata[term][field][key] = this.metadata[term][field][key].concat(metadata[key])\n      } else {\n        this.metadata[term][field][key] = metadata[key]\n      }\n    }\n  }\n  /**\n   * A lunr.Query provides a programmatic way of defining queries to be performed\n   * against a {@link lunr.Index}.\n   *\n   * Prefer constructing a lunr.Query using the {@link lunr.Index#query} method\n   * so the query object is pre-initialized with the right index fields.\n   *\n   * @constructor\n   * @property {lunr.Query~Clause[]} clauses - An array of query clauses.\n   * @property {string[]} allFields - An array of all available fields in a lunr.Index.\n   */\n  lunr.Query = function (allFields) {\n    this.clauses = []\n    this.allFields = allFields\n  }\n  \n  /**\n   * Constants for indicating what kind of automatic wildcard insertion will be used when constructing a query clause.\n   *\n   * This allows wildcards to be added to the beginning and end of a term without having to manually do any string\n   * concatenation.\n   *\n   * The wildcard constants can be bitwise combined to select both leading and trailing wildcards.\n   *\n   * @constant\n   * @default\n   * @property {number} wildcard.NONE - The term will have no wildcards inserted, this is the default behaviour\n   * @property {number} wildcard.LEADING - Prepend the term with a wildcard, unless a leading wildcard already exists\n   * @property {number} wildcard.TRAILING - Append a wildcard to the term, unless a trailing wildcard already exists\n   * @see lunr.Query~Clause\n   * @see lunr.Query#clause\n   * @see lunr.Query#term\n   * @example <caption>query term with trailing wildcard</caption>\n   * query.term('foo', { wildcard: lunr.Query.wildcard.TRAILING })\n   * @example <caption>query term with leading and trailing wildcard</caption>\n   * query.term('foo', {\n   *   wildcard: lunr.Query.wildcard.LEADING | lunr.Query.wildcard.TRAILING\n   * })\n   */\n  \n  lunr.Query.wildcard = new String (\"*\")\n  lunr.Query.wildcard.NONE = 0\n  lunr.Query.wildcard.LEADING = 1\n  lunr.Query.wildcard.TRAILING = 2\n  \n  /**\n   * Constants for indicating what kind of presence a term must have in matching documents.\n   *\n   * @constant\n   * @enum {number}\n   * @see lunr.Query~Clause\n   * @see lunr.Query#clause\n   * @see lunr.Query#term\n   * @example <caption>query term with required presence</caption>\n   * query.term('foo', { presence: lunr.Query.presence.REQUIRED })\n   */\n  lunr.Query.presence = {\n    /**\n     * Term's presence in a document is optional, this is the default value.\n     */\n    OPTIONAL: 1,\n  \n    /**\n     * Term's presence in a document is required, documents that do not contain\n     * this term will not be returned.\n     */\n    REQUIRED: 2,\n  \n    /**\n     * Term's presence in a document is prohibited, documents that do contain\n     * this term will not be returned.\n     */\n    PROHIBITED: 3\n  }\n  \n  /**\n   * A single clause in a {@link lunr.Query} contains a term and details on how to\n   * match that term against a {@link lunr.Index}.\n   *\n   * @typedef {Object} lunr.Query~Clause\n   * @property {string[]} fields - The fields in an index this clause should be matched against.\n   * @property {number} [boost=1] - Any boost that should be applied when matching this clause.\n   * @property {number} [editDistance] - Whether the term should have fuzzy matching applied, and how fuzzy the match should be.\n   * @property {boolean} [usePipeline] - Whether the term should be passed through the search pipeline.\n   * @property {number} [wildcard=lunr.Query.wildcard.NONE] - Whether the term should have wildcards appended or prepended.\n   * @property {number} [presence=lunr.Query.presence.OPTIONAL] - The terms presence in any matching documents.\n   */\n  \n  /**\n   * Adds a {@link lunr.Query~Clause} to this query.\n   *\n   * Unless the clause contains the fields to be matched all fields will be matched. In addition\n   * a default boost of 1 is applied to the clause.\n   *\n   * @param {lunr.Query~Clause} clause - The clause to add to this query.\n   * @see lunr.Query~Clause\n   * @returns {lunr.Query}\n   */\n  lunr.Query.prototype.clause = function (clause) {\n    if (!('fields' in clause)) {\n      clause.fields = this.allFields\n    }\n  \n    if (!('boost' in clause)) {\n      clause.boost = 1\n    }\n  \n    if (!('usePipeline' in clause)) {\n      clause.usePipeline = true\n    }\n  \n    if (!('wildcard' in clause)) {\n      clause.wildcard = lunr.Query.wildcard.NONE\n    }\n  \n    if ((clause.wildcard & lunr.Query.wildcard.LEADING) && (clause.term.charAt(0) != lunr.Query.wildcard)) {\n      clause.term = \"*\" + clause.term\n    }\n  \n    if ((clause.wildcard & lunr.Query.wildcard.TRAILING) && (clause.term.slice(-1) != lunr.Query.wildcard)) {\n      clause.term = \"\" + clause.term + \"*\"\n    }\n  \n    if (!('presence' in clause)) {\n      clause.presence = lunr.Query.presence.OPTIONAL\n    }\n  \n    this.clauses.push(clause)\n  \n    return this\n  }\n  \n  /**\n   * A negated query is one in which every clause has a presence of\n   * prohibited. These queries require some special processing to return\n   * the expected results.\n   *\n   * @returns boolean\n   */\n  lunr.Query.prototype.isNegated = function () {\n    for (var i = 0; i < this.clauses.length; i++) {\n      if (this.clauses[i].presence != lunr.Query.presence.PROHIBITED) {\n        return false\n      }\n    }\n  \n    return true\n  }\n  \n  /**\n   * Adds a term to the current query, under the covers this will create a {@link lunr.Query~Clause}\n   * to the list of clauses that make up this query.\n   *\n   * The term is used as is, i.e. no tokenization will be performed by this method. Instead conversion\n   * to a token or token-like string should be done before calling this method.\n   *\n   * The term will be converted to a string by calling `toString`. Multiple terms can be passed as an\n   * array, each term in the array will share the same options.\n   *\n   * @param {object|object[]} term - The term(s) to add to the query.\n   * @param {object} [options] - Any additional properties to add to the query clause.\n   * @returns {lunr.Query}\n   * @see lunr.Query#clause\n   * @see lunr.Query~Clause\n   * @example <caption>adding a single term to a query</caption>\n   * query.term(\"foo\")\n   * @example <caption>adding a single term to a query and specifying search fields, term boost and automatic trailing wildcard</caption>\n   * query.term(\"foo\", {\n   *   fields: [\"title\"],\n   *   boost: 10,\n   *   wildcard: lunr.Query.wildcard.TRAILING\n   * })\n   * @example <caption>using lunr.tokenizer to convert a string to tokens before using them as terms</caption>\n   * query.term(lunr.tokenizer(\"foo bar\"))\n   */\n  lunr.Query.prototype.term = function (term, options) {\n    if (Array.isArray(term)) {\n      term.forEach(function (t) { this.term(t, lunr.utils.clone(options)) }, this)\n      return this\n    }\n  \n    var clause = options || {}\n    clause.term = term.toString()\n  \n    this.clause(clause)\n  \n    return this\n  }\n  lunr.QueryParseError = function (message, start, end) {\n    this.name = \"QueryParseError\"\n    this.message = message\n    this.start = start\n    this.end = end\n  }\n  \n  lunr.QueryParseError.prototype = new Error\n  lunr.QueryLexer = function (str) {\n    this.lexemes = []\n    this.str = str\n    this.length = str.length\n    this.pos = 0\n    this.start = 0\n    this.escapeCharPositions = []\n  }\n  \n  lunr.QueryLexer.prototype.run = function () {\n    var state = lunr.QueryLexer.lexText\n  \n    while (state) {\n      state = state(this)\n    }\n  }\n  \n  lunr.QueryLexer.prototype.sliceString = function () {\n    var subSlices = [],\n        sliceStart = this.start,\n        sliceEnd = this.pos\n  \n    for (var i = 0; i < this.escapeCharPositions.length; i++) {\n      sliceEnd = this.escapeCharPositions[i]\n      subSlices.push(this.str.slice(sliceStart, sliceEnd))\n      sliceStart = sliceEnd + 1\n    }\n  \n    subSlices.push(this.str.slice(sliceStart, this.pos))\n    this.escapeCharPositions.length = 0\n  \n    return subSlices.join('')\n  }\n  \n  lunr.QueryLexer.prototype.emit = function (type) {\n    this.lexemes.push({\n      type: type,\n      str: this.sliceString(),\n      start: this.start,\n      end: this.pos\n    })\n  \n    this.start = this.pos\n  }\n  \n  lunr.QueryLexer.prototype.escapeCharacter = function () {\n    this.escapeCharPositions.push(this.pos - 1)\n    this.pos += 1\n  }\n  \n  lunr.QueryLexer.prototype.next = function () {\n    if (this.pos >= this.length) {\n      return lunr.QueryLexer.EOS\n    }\n  \n    var char = this.str.charAt(this.pos)\n    this.pos += 1\n    return char\n  }\n  \n  lunr.QueryLexer.prototype.width = function () {\n    return this.pos - this.start\n  }\n  \n  lunr.QueryLexer.prototype.ignore = function () {\n    if (this.start == this.pos) {\n      this.pos += 1\n    }\n  \n    this.start = this.pos\n  }\n  \n  lunr.QueryLexer.prototype.backup = function () {\n    this.pos -= 1\n  }\n  \n  lunr.QueryLexer.prototype.acceptDigitRun = function () {\n    var char, charCode\n  \n    do {\n      char = this.next()\n      charCode = char.charCodeAt(0)\n    } while (charCode > 47 && charCode < 58)\n  \n    if (char != lunr.QueryLexer.EOS) {\n      this.backup()\n    }\n  }\n  \n  lunr.QueryLexer.prototype.more = function () {\n    return this.pos < this.length\n  }\n  \n  lunr.QueryLexer.EOS = 'EOS'\n  lunr.QueryLexer.FIELD = 'FIELD'\n  lunr.QueryLexer.TERM = 'TERM'\n  lunr.QueryLexer.EDIT_DISTANCE = 'EDIT_DISTANCE'\n  lunr.QueryLexer.BOOST = 'BOOST'\n  lunr.QueryLexer.PRESENCE = 'PRESENCE'\n  \n  lunr.QueryLexer.lexField = function (lexer) {\n    lexer.backup()\n    lexer.emit(lunr.QueryLexer.FIELD)\n    lexer.ignore()\n    return lunr.QueryLexer.lexText\n  }\n  \n  lunr.QueryLexer.lexTerm = function (lexer) {\n    if (lexer.width() > 1) {\n      lexer.backup()\n      lexer.emit(lunr.QueryLexer.TERM)\n    }\n  \n    lexer.ignore()\n  \n    if (lexer.more()) {\n      return lunr.QueryLexer.lexText\n    }\n  }\n  \n  lunr.QueryLexer.lexEditDistance = function (lexer) {\n    lexer.ignore()\n    lexer.acceptDigitRun()\n    lexer.emit(lunr.QueryLexer.EDIT_DISTANCE)\n    return lunr.QueryLexer.lexText\n  }\n  \n  lunr.QueryLexer.lexBoost = function (lexer) {\n    lexer.ignore()\n    lexer.acceptDigitRun()\n    lexer.emit(lunr.QueryLexer.BOOST)\n    return lunr.QueryLexer.lexText\n  }\n  \n  lunr.QueryLexer.lexEOS = function (lexer) {\n    if (lexer.width() > 0) {\n      lexer.emit(lunr.QueryLexer.TERM)\n    }\n  }\n  \n  // This matches the separator used when tokenising fields\n  // within a document. These should match otherwise it is\n  // not possible to search for some tokens within a document.\n  //\n  // It is possible for the user to change the separator on the\n  // tokenizer so it _might_ clash with any other of the special\n  // characters already used within the search string, e.g. :.\n  //\n  // This means that it is possible to change the separator in\n  // such a way that makes some words unsearchable using a search\n  // string.\n  lunr.QueryLexer.termSeparator = lunr.tokenizer.separator\n  \n  lunr.QueryLexer.lexText = function (lexer) {\n    while (true) {\n      var char = lexer.next()\n  \n      if (char == lunr.QueryLexer.EOS) {\n        return lunr.QueryLexer.lexEOS\n      }\n  \n      // Escape character is '\\'\n      if (char.charCodeAt(0) == 92) {\n        lexer.escapeCharacter()\n        continue\n      }\n  \n      if (char == \":\") {\n        return lunr.QueryLexer.lexField\n      }\n  \n      if (char == \"~\") {\n        lexer.backup()\n        if (lexer.width() > 0) {\n          lexer.emit(lunr.QueryLexer.TERM)\n        }\n        return lunr.QueryLexer.lexEditDistance\n      }\n  \n      if (char == \"^\") {\n        lexer.backup()\n        if (lexer.width() > 0) {\n          lexer.emit(lunr.QueryLexer.TERM)\n        }\n        return lunr.QueryLexer.lexBoost\n      }\n  \n      // \"+\" indicates term presence is required\n      // checking for length to ensure that only\n      // leading \"+\" are considered\n      if (char == \"+\" && lexer.width() === 1) {\n        lexer.emit(lunr.QueryLexer.PRESENCE)\n        return lunr.QueryLexer.lexText\n      }\n  \n      // \"-\" indicates term presence is prohibited\n      // checking for length to ensure that only\n      // leading \"-\" are considered\n      if (char == \"-\" && lexer.width() === 1) {\n        lexer.emit(lunr.QueryLexer.PRESENCE)\n        return lunr.QueryLexer.lexText\n      }\n  \n      if (char.match(lunr.QueryLexer.termSeparator)) {\n        return lunr.QueryLexer.lexTerm\n      }\n    }\n  }\n  \n  lunr.QueryParser = function (str, query) {\n    this.lexer = new lunr.QueryLexer (str)\n    this.query = query\n    this.currentClause = {}\n    this.lexemeIdx = 0\n  }\n  \n  lunr.QueryParser.prototype.parse = function () {\n    this.lexer.run()\n    this.lexemes = this.lexer.lexemes\n  \n    var state = lunr.QueryParser.parseClause\n  \n    while (state) {\n      state = state(this)\n    }\n  \n    return this.query\n  }\n  \n  lunr.QueryParser.prototype.peekLexeme = function () {\n    return this.lexemes[this.lexemeIdx]\n  }\n  \n  lunr.QueryParser.prototype.consumeLexeme = function () {\n    var lexeme = this.peekLexeme()\n    this.lexemeIdx += 1\n    return lexeme\n  }\n  \n  lunr.QueryParser.prototype.nextClause = function () {\n    var completedClause = this.currentClause\n    this.query.clause(completedClause)\n    this.currentClause = {}\n  }\n  \n  lunr.QueryParser.parseClause = function (parser) {\n    var lexeme = parser.peekLexeme()\n  \n    if (lexeme == undefined) {\n      return\n    }\n  \n    switch (lexeme.type) {\n      case lunr.QueryLexer.PRESENCE:\n        return lunr.QueryParser.parsePresence\n      case lunr.QueryLexer.FIELD:\n        return lunr.QueryParser.parseField\n      case lunr.QueryLexer.TERM:\n        return lunr.QueryParser.parseTerm\n      default:\n        var errorMessage = \"expected either a field or a term, found \" + lexeme.type\n  \n        if (lexeme.str.length >= 1) {\n          errorMessage += \" with value '\" + lexeme.str + \"'\"\n        }\n  \n        throw new lunr.QueryParseError (errorMessage, lexeme.start, lexeme.end)\n    }\n  }\n  \n  lunr.QueryParser.parsePresence = function (parser) {\n    var lexeme = parser.consumeLexeme()\n  \n    if (lexeme == undefined) {\n      return\n    }\n  \n    switch (lexeme.str) {\n      case \"-\":\n        parser.currentClause.presence = lunr.Query.presence.PROHIBITED\n        break\n      case \"+\":\n        parser.currentClause.presence = lunr.Query.presence.REQUIRED\n        break\n      default:\n        var errorMessage = \"unrecognised presence operator'\" + lexeme.str + \"'\"\n        throw new lunr.QueryParseError (errorMessage, lexeme.start, lexeme.end)\n    }\n  \n    var nextLexeme = parser.peekLexeme()\n  \n    if (nextLexeme == undefined) {\n      var errorMessage = \"expecting term or field, found nothing\"\n      throw new lunr.QueryParseError (errorMessage, lexeme.start, lexeme.end)\n    }\n  \n    switch (nextLexeme.type) {\n      case lunr.QueryLexer.FIELD:\n        return lunr.QueryParser.parseField\n      case lunr.QueryLexer.TERM:\n        return lunr.QueryParser.parseTerm\n      default:\n        var errorMessage = \"expecting term or field, found '\" + nextLexeme.type + \"'\"\n        throw new lunr.QueryParseError (errorMessage, nextLexeme.start, nextLexeme.end)\n    }\n  }\n  \n  lunr.QueryParser.parseField = function (parser) {\n    var lexeme = parser.consumeLexeme()\n  \n    if (lexeme == undefined) {\n      return\n    }\n  \n    if (parser.query.allFields.indexOf(lexeme.str) == -1) {\n      var possibleFields = parser.query.allFields.map(function (f) { return \"'\" + f + \"'\" }).join(', '),\n          errorMessage = \"unrecognised field '\" + lexeme.str + \"', possible fields: \" + possibleFields\n  \n      throw new lunr.QueryParseError (errorMessage, lexeme.start, lexeme.end)\n    }\n  \n    parser.currentClause.fields = [lexeme.str]\n  \n    var nextLexeme = parser.peekLexeme()\n  \n    if (nextLexeme == undefined) {\n      var errorMessage = \"expecting term, found nothing\"\n      throw new lunr.QueryParseError (errorMessage, lexeme.start, lexeme.end)\n    }\n  \n    switch (nextLexeme.type) {\n      case lunr.QueryLexer.TERM:\n        return lunr.QueryParser.parseTerm\n      default:\n        var errorMessage = \"expecting term, found '\" + nextLexeme.type + \"'\"\n        throw new lunr.QueryParseError (errorMessage, nextLexeme.start, nextLexeme.end)\n    }\n  }\n  \n  lunr.QueryParser.parseTerm = function (parser) {\n    var lexeme = parser.consumeLexeme()\n  \n    if (lexeme == undefined) {\n      return\n    }\n  \n    parser.currentClause.term = lexeme.str.toLowerCase()\n  \n    if (lexeme.str.indexOf(\"*\") != -1) {\n      parser.currentClause.usePipeline = false\n    }\n  \n    var nextLexeme = parser.peekLexeme()\n  \n    if (nextLexeme == undefined) {\n      parser.nextClause()\n      return\n    }\n  \n    switch (nextLexeme.type) {\n      case lunr.QueryLexer.TERM:\n        parser.nextClause()\n        return lunr.QueryParser.parseTerm\n      case lunr.QueryLexer.FIELD:\n        parser.nextClause()\n        return lunr.QueryParser.parseField\n      case lunr.QueryLexer.EDIT_DISTANCE:\n        return lunr.QueryParser.parseEditDistance\n      case lunr.QueryLexer.BOOST:\n        return lunr.QueryParser.parseBoost\n      case lunr.QueryLexer.PRESENCE:\n        parser.nextClause()\n        return lunr.QueryParser.parsePresence\n      default:\n        var errorMessage = \"Unexpected lexeme type '\" + nextLexeme.type + \"'\"\n        throw new lunr.QueryParseError (errorMessage, nextLexeme.start, nextLexeme.end)\n    }\n  }\n  \n  lunr.QueryParser.parseEditDistance = function (parser) {\n    var lexeme = parser.consumeLexeme()\n  \n    if (lexeme == undefined) {\n      return\n    }\n  \n    var editDistance = parseInt(lexeme.str, 10)\n  \n    if (isNaN(editDistance)) {\n      var errorMessage = \"edit distance must be numeric\"\n      throw new lunr.QueryParseError (errorMessage, lexeme.start, lexeme.end)\n    }\n  \n    parser.currentClause.editDistance = editDistance\n  \n    var nextLexeme = parser.peekLexeme()\n  \n    if (nextLexeme == undefined) {\n      parser.nextClause()\n      return\n    }\n  \n    switch (nextLexeme.type) {\n      case lunr.QueryLexer.TERM:\n        parser.nextClause()\n        return lunr.QueryParser.parseTerm\n      case lunr.QueryLexer.FIELD:\n        parser.nextClause()\n        return lunr.QueryParser.parseField\n      case lunr.QueryLexer.EDIT_DISTANCE:\n        return lunr.QueryParser.parseEditDistance\n      case lunr.QueryLexer.BOOST:\n        return lunr.QueryParser.parseBoost\n      case lunr.QueryLexer.PRESENCE:\n        parser.nextClause()\n        return lunr.QueryParser.parsePresence\n      default:\n        var errorMessage = \"Unexpected lexeme type '\" + nextLexeme.type + \"'\"\n        throw new lunr.QueryParseError (errorMessage, nextLexeme.start, nextLexeme.end)\n    }\n  }\n  \n  lunr.QueryParser.parseBoost = function (parser) {\n    var lexeme = parser.consumeLexeme()\n  \n    if (lexeme == undefined) {\n      return\n    }\n  \n    var boost = parseInt(lexeme.str, 10)\n  \n    if (isNaN(boost)) {\n      var errorMessage = \"boost must be numeric\"\n      throw new lunr.QueryParseError (errorMessage, lexeme.start, lexeme.end)\n    }\n  \n    parser.currentClause.boost = boost\n  \n    var nextLexeme = parser.peekLexeme()\n  \n    if (nextLexeme == undefined) {\n      parser.nextClause()\n      return\n    }\n  \n    switch (nextLexeme.type) {\n      case lunr.QueryLexer.TERM:\n        parser.nextClause()\n        return lunr.QueryParser.parseTerm\n      case lunr.QueryLexer.FIELD:\n        parser.nextClause()\n        return lunr.QueryParser.parseField\n      case lunr.QueryLexer.EDIT_DISTANCE:\n        return lunr.QueryParser.parseEditDistance\n      case lunr.QueryLexer.BOOST:\n        return lunr.QueryParser.parseBoost\n      case lunr.QueryLexer.PRESENCE:\n        parser.nextClause()\n        return lunr.QueryParser.parsePresence\n      default:\n        var errorMessage = \"Unexpected lexeme type '\" + nextLexeme.type + \"'\"\n        throw new lunr.QueryParseError (errorMessage, nextLexeme.start, nextLexeme.end)\n    }\n  }\n  \n    /**\n     * export the module via AMD, CommonJS or as a browser global\n     * Export code from https://github.com/umdjs/umd/blob/master/returnExports.js\n     */\n    ;(function (root, factory) {\n      if (typeof define === 'function' && define.amd) {\n        // AMD. Register as an anonymous module.\n        define(factory)\n      } else if (typeof exports === 'object') {\n        /**\n         * Node. Does not work with strict CommonJS, but\n         * only CommonJS-like enviroments that support module.exports,\n         * like Node.\n         */\n        module.exports = factory()\n      } else {\n        // Browser globals (root is window)\n        root.lunr = factory()\n      }\n    }(this, function () {\n      /**\n       * Just return a value to define the module export.\n       * This example returns an object, but the module\n       * can return a function as the exported value.\n       */\n      return lunr\n    }))\n  })();","document.addEventListener('DOMContentLoaded', function () {\n  var searchButton = document.querySelector('.mdc-icon-button.search')\n  var searchCancelButton = document.querySelector('.mdc-icon-button.search-arrow-back')\n  var searchClearButton = document.querySelector('.mdc-icon-button.search-clear-query')\n\n  function clearSearch() {\n    var searchInput = document.getElementById('search-input')\n    searchInput.value = '';\n    searchInput.dispatchEvent(new KeyboardEvent('keydown')); // trigger rerender of results\n  }\n\n  function enterOrSpacebarPressed(e) {\n    // Need both, 'keyCode' and 'which' to work in all browsers.\n    var code = e.keyCode || e.which;\n    var enterKey = 13;\n    var spaceKey = 32;\n\n    return (code === enterKey || code === spaceKey);\n  }\n\n  function toggleSearch() {\n    var mainContainer = document.querySelector('main')\n    var navContainer = document.querySelector('div.nav-container')\n    var searchResult = document.querySelector('.search-result-dropdown-menu')\n    var toolbarContainer = document.querySelector('.toolbar')\n    if(searchResult.classList.contains('hide')){\n      searchResult.classList.remove('hide')\n      mainContainer.classList.add('hide')\n      navContainer.classList.add('hide')\n      toolbarContainer.classList.add('hide')\n    } else{\n      searchResult.classList.add('hide')\n      mainContainer.classList.remove('hide')\n\n      // Toolbar should stay hidden on desktop and navbar should stay hidden on mobile.\n      if (window.innerWidth > 1024) {\n        navContainer.classList.remove('hide')\n      } else {\n        toolbarContainer.classList.remove('hide')\n      }\n    }\n\n    var regularTopBar = document.querySelector('.mdc-top-app-bar__row')\n    var searchTopBar = document.querySelector('.mdc-top-app-bar__row.sdp-top-app-bar__search')\n    if(regularTopBar.classList.contains('hide')){\n      regularTopBar.classList.remove('hide')\n      searchTopBar.classList.add('hide')\n    } else{\n      regularTopBar.classList.add('hide')\n      searchTopBar.classList.remove('hide')\n    }\n  }\n\n  // Add event listeners to search icon\n  searchButton.addEventListener('click', toggleSearch)\n  searchButton.addEventListener('keypress', function (e) {\n    if (enterOrSpacebarPressed(e)) {\n      toggleSearch()\n    }\n  })\n  if ('ontouchstart' in window) {\n    searchButton.addEventListener('ontouchstart', toggleSearch)\n  }\n\n  // Add event listeners to search back/cancel icon\n  searchCancelButton.addEventListener('click', toggleSearch)\n  searchCancelButton.addEventListener('keypress', function (e) {\n    if (enterOrSpacebarPressed(e)) {\n      toggleSearch()\n    }\n  })\n  if ('ontouchstart' in window) {\n    searchCancelButton.addEventListener('ontouchstart', toggleSearch)\n  }\n\n  // Add event listeners to clear search button\n  searchClearButton.addEventListener('click', clearSearch)\n  searchClearButton.addEventListener('keypress', function (e) {\n    if (enterOrSpacebarPressed(e)) {\n      clearSearch()\n    }\n  })\n  if ('ontouchstart' in window) {\n    searchClearButton.addEventListener('ontouchstart', clearSearch)\n  }\n});\n\n\n/* eslint-env browser */\nwindow.antoraLunr = (function (lunr) {\n  var searchInput = document.getElementById('search-input')\n  var searchResult = document.createElement('div')\n  var body = document.querySelector('.body')\n  searchResult.classList.add('search-result-dropdown-menu')\n  searchResult.classList.add('hide')\n  body.insertBefore(searchResult, body.firstChild)\n\n  function highlightText (doc, position) {\n    var hits = []\n    var start = position[0]\n    var length = position[1]\n\n    var text = doc.text\n    var highlightSpan = document.createElement('span')\n    highlightSpan.classList.add('search-result-highlight')\n    highlightSpan.innerText = text.substr(start, length)\n\n    var end = start + length\n    var textEnd = text.length - 1\n    var contextOffset = 15\n    var contextAfter = end + contextOffset > textEnd ? textEnd : end + contextOffset\n    var contextBefore = start - contextOffset < 0 ? 0 : start - contextOffset\n    if (start === 0 && end === textEnd) {\n      hits.push(highlightSpan)\n    } else if (start === 0) {\n      hits.push(highlightSpan)\n      hits.push(document.createTextNode(text.substr(end, contextAfter)))\n    } else if (end === textEnd) {\n      hits.push(document.createTextNode(text.substr(0, start)))\n      hits.push(highlightSpan)\n    } else {\n      hits.push(document.createTextNode('...' + text.substr(contextBefore, start - contextBefore)))\n      hits.push(highlightSpan)\n      hits.push(document.createTextNode(text.substr(end, contextAfter - end) + '...'))\n    }\n    return hits\n  }\n\n  function highlightTitle (hash, doc, position) {\n    var hits = []\n    var start = position[0]\n    var length = position[1]\n\n    var highlightSpan = document.createElement('span')\n    highlightSpan.classList.add('search-result-highlight')\n    var title\n    if (hash) {\n      title = doc.titles.filter(function (item) {\n        return item.id === hash\n      })[0].text\n    } else {\n      title = doc.title\n    }\n    highlightSpan.innerText = title.substr(start, length)\n\n    var end = start + length\n    var titleEnd = title.length - 1\n    if (start === 0 && end === titleEnd) {\n      hits.push(highlightSpan)\n    } else if (start === 0) {\n      hits.push(highlightSpan)\n      hits.push(document.createTextNode(title.substr(length, titleEnd)))\n    } else if (end === titleEnd) {\n      hits.push(document.createTextNode(title.substr(0, start)))\n      hits.push(highlightSpan)\n    } else {\n      hits.push(document.createTextNode(title.substr(0, start)))\n      hits.push(highlightSpan)\n      hits.push(document.createTextNode(title.substr(end, titleEnd)))\n    }\n    return hits\n  }\n\n  function highlightHit (metadata, hash, doc) {\n    var hits = []\n    for (var token in metadata) {\n      var fields = metadata[token]\n      for (var field in fields) {\n        var positions = fields[field]\n        if (positions.position) {\n          var position = positions.position[0] // only higlight the first match\n          if (field === 'title') {\n            hits = highlightTitle(hash, doc, position)\n          } else if (field === 'text') {\n            hits = highlightText(doc, position)\n          }\n        }\n      }\n    }\n    return hits\n  }\n\n  function createSearchResult(result, store, searchResultDataset) {\n    result.forEach(function (item) {\n      var url = item.ref\n      var hash\n      if (url.includes('#')) {\n        hash = url.substring(url.indexOf('#') + 1)\n        url = url.replace('#' + hash, '')\n      }\n      var doc = store[url]\n      var metadata = item.matchData.metadata\n      var hits = highlightHit(metadata, hash, doc)\n      searchResultDataset.appendChild(createSearchResultItem(doc, item, hits))\n    })\n  }\n\n  function createSearchResultItem (doc, item, hits) {\n    var documentTitle = document.createElement('div')\n    documentTitle.classList.add('search-result-document-title')\n    documentTitle.innerText = doc.title\n    var documentHit = document.createElement('div')\n    documentHit.classList.add('search-result-document-hit')\n    var documentHitLink = document.createElement('a')\n    documentHitLink.href = item.ref\n    documentHit.appendChild(documentHitLink)\n    hits.forEach(function (hit) {\n      documentHitLink.appendChild(hit)\n    })\n    var searchResultItem = document.createElement('div')\n    searchResultItem.classList.add('search-result-item')\n    searchResultItem.appendChild(documentTitle)\n    searchResultItem.appendChild(documentHit)\n    return searchResultItem\n  }\n\n  function createNoResult (text) {\n    var searchResultItem = document.createElement('div')\n    searchResultItem.classList.add('search-result-item')\n    var documentHit = document.createElement('div')\n    documentHit.classList.add('search-result-document-hit')\n    var message = document.createElement('strong')\n    message.innerText = 'No results found for query \"' + text + '\"'\n    documentHit.appendChild(message)\n    searchResultItem.appendChild(documentHit)\n    return searchResultItem\n  }\n\n  function search (index, text) {\n    // execute an exact match search\n    var result = index.search(text)\n    if (result.length > 0) {\n      return result\n    }\n    // no result, use a begins with search\n    result = index.search(text + '*')\n    if (result.length > 0) {\n      return result\n    }\n    // no result, use a contains search\n    result = index.search('*' + text + '*')\n    return result\n  }\n\n  function searchIndex (index, store, text) {\n    // reset search result\n    while (searchResult.firstChild) {\n      searchResult.removeChild(searchResult.firstChild)\n    }\n    if (text.trim() === '') {\n      return\n    }\n    var result = search(index, text)\n    var searchResultDataset = document.createElement('div')\n    searchResultDataset.classList.add('search-result-dataset')\n    searchResult.appendChild(searchResultDataset)\n    if (result.length > 0) {\n      createSearchResult(result, store, searchResultDataset)\n    } else {\n      searchResultDataset.appendChild(createNoResult(text))\n    }\n  }\n\n  function debounce (func, wait, immediate) {\n    var timeout\n    return function () {\n      var context = this\n      var args = arguments\n      var later = function () {\n        timeout = null\n        if (!immediate) func.apply(context, args)\n      }\n      var callNow = immediate && !timeout\n      clearTimeout(timeout)\n      timeout = setTimeout(later, wait)\n      if (callNow) func.apply(context, args)\n    }\n  }\n\n  function init (data) {\n    var index = Object.assign({index: lunr.Index.load(data.index), store: data.store})\n    var search = debounce(function () {\n      searchIndex(index.index, index.store, searchInput.value)\n    }, 100)\n    // TODO listen to blur, focus and input events\n    searchInput.addEventListener('keydown', search)\n  }\n\n  return {\n    init: init,\n  }\n})(window.lunr)\n"]}